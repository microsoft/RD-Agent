exp_feedback:
  system: |-
    You are an advanced assistant analyzing results in data-driven R&D.

    Below is a detailed description of the current Kaggle competition scenario:
    {{ scenario }}

    Your task is to analyze the current experiment's hypothesis, implementation (code), and results, explicitly comparing them with previous experiments and the best previous result (SOTA).

    Step-by-step Analysis Process:

    Step 1: Verify Submission Format
    - If the submission format check fails:
      - Identify and clearly specify code or workflow issues.
      - Recommend corrective actions explicitly.
      - Set `"Replace Best Result": "no"`.
      - Begin your `reasoning` with `[Submission format error]`, clearly stating the issues causing experiment failure.
    - If submission passes, proceed to Step 2.

    Step 2: Evaluate Alignment with Competition Requirements (if format correct)
    - GOAL: TRY YOUR BEST TO AVOID SIGNIFICANT DISCREPANCIES BETWEEN VALIDATION AND TEST SET SCORES.
    - Confirm strict adherence to the competition's evaluation rules listed in `scenario`:
      - Exact match between validation metric and official Kaggle metric.
      - Consistent prediction methodologies between validation and test datasets.
      - No shortcuts or fold-specific strategies applied inconsistently.
      - Rigorous checks for corner-case consistency.
    - If discrepancies are identified:
      - Clearly document these issues in `Reasoning`.
      - Set `"Evaluation Aligned With Task": "no"` and `"Replace Best Result": "no"`.
      - Begin your `reasoning` with `[Evaluation error]`, explicitly stating the evaluation alignment issues causing experiment failure.
    - If evaluation alignment passes, set `"Evaluation Aligned With Task": "yes"`, and then proceed to Step 3.

    Step 3: Analyze Experimental Results (if format and evaluation alignment correct)
    - Explicitly confirm or refute the hypothesis with precise data points or performance trends.
    - Directly compare the current `ensemble` validation score to the SOTA `ensemble` validation score. Do not focus on individual models unless anomalies are significant.
    - If current `ensemble` validation score surpasses SOTA `ensemble` validation score, set `"Replace Best Result": "yes"`; otherwise, set as "no".
    - Begin your `reasoning` with `[Experiment Analysis]`, clearly stating why the current experiment's result surpasses or falls short compared to the SOTA.
    - NOTES:
      - The experiments focus on the final ensemble results (Don't reject the results because they are still not perfect; e.g., the ensemble does not improve performance due to having only one model. We can improve that later. We are comparing the current ensemble validation score to the SOTA ensemble validation score to check if the overall performance is improved.)
 
    Provide detailed and constructive feedback structured as follows:
    Example JSON Structure for Result Analysis:
    {
      "Observations": "Clearly summarize current and SOTA ensemble results with exact scores and notable patterns. Limit to no more than three concise, data-focused sentences.",
      "Feedback for Hypothesis": Explicitly confirm or refute the hypothesis based on specific data points or performance trends. Limit to two sentences.",
      "Evaluation Aligned With Task": "yes or no",
      "Replace Best Result": "yes or no",
      "Reasoning": "Clearly explain the reason for success or failure of the experiment. Begin explicitly with [Submission format error], [Evaluation error], or [Experiment Analysis] depending on the step at which issues arose. Reference specific scores and methodological differences with SOTA. Limit to three sentences."
    }

  user: |-
    We are currently in a process of validating hypotheses to iteratively improve our models for Kaggle competitions. Each round aims explicitly to confirm or reject hypotheses based on experiment results.

    ## SOTA Solution
    {{ sota_desc }}

    ## Current Solution
    ### Task of Current Solution
    {{ cur_exp.pending_tasks_list[0][0].get_task_information() }}

    {% if cur_exp.hypothesis %}
    The experiment was designed based on the following hypothesis:
    {{ cur_exp.hypothesis }}
    
    Modified code according to hypothesis:
    {% else %}
    Modified code:
    {% endif %}

    {% for de in diff_edition %}
    {{ de }}
    {% endfor %}

    ### Final Results of the Current Solution
    1. Pay close attention to the `ensemble` score, as it represents the final evaluation metric for this iteration.
    2. If any individual model significantly outperforms the ensemble, this may indicate an issue in the ensemble method. But if the final `ensemble` score surpasses the current SOTA, you should update the SOTA record. However, it seems that there are noticeable issues in the ensemble component, be sure to highlight them explicitly.

    Below are the results for this experiment:
    {{ cur_exp.result }}

    {% if cur_vs_sota_score is not none %}
    Below is the comparison of the current `ensemble` performance with the SOTA results:
    {{ cur_vs_sota_score }}
    {% endif %}
    
    {% if cur_exp.format_check_result is not none %}
    ### Submission format check to current solution:
    {{ cur_exp.format_check_result }}
    {% endif %}
    
    ### Complete Code of Current Solution
    {{ cur_exp.experiment_workspace.all_codes }}

    ## Feedback of past experiments
    {{ feedback_desc }}
    Please refer to these hypotheses and feedback to help you recommend new experiment and hypothesis

    Tips:
    - Step 1: If submission format has issues, prioritize fixing them before proceeding.
    - Step 2: If evaluation alignment issues are identified (validation approach does not follow competition requirements), address these methodological discrepancies immediately.
    - Step 3: If new results significantly worse than SOTA, or repeated hyperparameter adjustments yield no improvement, it might be time to rethink or shift focus.
