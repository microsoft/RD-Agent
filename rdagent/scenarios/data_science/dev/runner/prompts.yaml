DSCoSTEER_eval:
  system: |-
    {% include "scenarios.data_science.share:scen.role" %}
    {% if is_sub_enabled %}
    Your task is to perform the following evaluation(s):
    # Evalution 1: Code Correctness
    ## Scenario
    The code is focusing on the following scenario:
    {{ scenario }}

    ## Task Description
    The code is focusing on the following task:
    {{ task_desc }}

    You have following environment to run the code:
    {{ runtime_environment }}

    ## Evaluation Guidelines
    1. Verify that the workflow executes successfully.
    2. Ensure the code does not contain any incorrect, fabricated, or deceptive operations, such as mocking data, scores, or results.
    3. Confirm that the prediction file (`submission.csv`) is generated using only the test dataset, and its format matches the sample submission.
    If the code does not satisfy the requirements:
    - Set "final_decision" to false.
    {% if enable_runner_iteration %}
    - set "hyperparameter_tuning_decision" to false.
    - Set "hyperparameter_tuning_suggestion" to an empty string.
    {% endif %}

    {% if enable_runner_iteration %}
    # Evaluation 2: Hyperparameters
    ## Task Description
    The user will provide you the time spent on the whole code execution and the timeout of the code execution. You should decide whether the hyperparameters are reasonable based on the time.
    For example, if the code only spent ten percent of the timeout and the hyperparameter like `n_estimators` or 'epochs' is very small or batch size is small you should suggest to increase these hyperparameter.

    ## Evaluation Guidelines
    1. The code execution time and timeout suggest that there is room for improvement in the hyperparameters.
    2. The code applied early stopping strategy already (in order to prevent overfitting)
    If the code satisfy the requirements:
    - Set "hyperparameter_tuning_decision" to true.
    - Set "final_decision" to false.
    - Provide a reasonable suggestion in "hyperparameter_tuning_suggestion", such as increasing `n_estimators` to 1000, increasing `epochs` to 100, or increasing `batch_size` to 64.
    If the code does not satisfy the requirements:
    - set "hyperparameter_tuning_decision" to false.
    - Set "hyperparameter_tuning_suggestion" to an empty string.
    {% endif %}

    # Output format
    Please respond with your feedback in the following JSON format and order without anything else:
    ```json
    {
        "execution": "Describe whether the whole code base executed successfully and generating the final submission. Include any errors or issues encountered, and retain all error messages and traceback details.",
        "return_checking": "Verify the generated files, particularly the submission file. Ensure that its format matches the sample submission",
        "code": "Provide feedback on code quality, readability, and adherence to the given specifications.",
        "final_decision": <true/false>,
        {% if enable_runner_iteration %}
        "hyperparameter_tuning_decision": <true/false>,
        "hyperparameter_tuning_suggestion": <suggestion in plain text for hyperparameter tuning>,
        {% endif %}
    }
    ```
    {% else %}
    The user will provide you the whole code base, some logs generated during the execution of the whole workflow. Your evaluation scope includes whether the workflow code executes successfully.
    No need to check the detail of submission file.

    Please respond with your feedback in the following JSON format and order
    ```json
    {
        "execution": "Describe whether the code executed successfully. Include any errors or issues encountered, and append all error messages and full traceback details without summarizing or omitting any information.",
        "return_checking": "Describe the expected file to be generated.",
        "code": "Provide feedback on code quality, readability, and adherence to the given specifications.",
        "final_decision": <true/false>,
        {% if enable_runner_iteration %}
        "hyperparameter_tuning_decision": <true/false>,
        "hyperparameter_tuning_suggestion": <suggestion in plain text for hyperparameter tuning>,
        {% endif %}
    }
    ```
    {% endif %}

  user: |-
    # Code
    {{ code }}

    ## Stdout of code execution and testing
    {{ stdout }}

    # The time spend on code execution and timeout
    {{ time_spent }}

    ## The timeout of code execution
    {{ timeout }}

    ## The percent of timeout used
    {{ percent_of_timeout_used }}

DSCoSTEER:
  system_debugger: |-
    {% include "scenarios.data_science.share:scen.role" %}
    You have finished the implementation of the whole workflow which has executed well on a sampled dataset. Now we are working on the full dataset.
    The user has reported that the workflow failed to execute on the full dataset.
    Your will be provided with:
    1. Code base.
    2. Task description, which is the task the code is trying to solve.
    3. Feedback generated during the execution of the whole workflow.
    4. Suggestions for hyperparameter tuning.
    Your job is to debug the whole code base, try to correct the errors, and ensure that the workflow can execute successfully on the full dataset.

    ## Task description
    {{ task_desc }}

    ## Instructions
    1. Minimal changes principle: only modify the code that is necessary to fix the issues but not affect any other parts of the code. Try to correct as less files as possible since files are interdependent.
    {% if enable_runner_code_diff %}
    2. You must output in Code Diff format. The detailed format specification is as follows.
    {% else %}
    2. You must output the COMPLETE and FULL code. Do not truncate, summarize, or omit any parts of the code. Include all imports, functions, classes, and the entire workflow from start to finish.
    {% endif %}

    ## Output Format
    {% if out_spec %}
    {{ out_spec }}
    {% else %}
    Please response the code in the following JSON format without anything else.
    {
        "code": "The Python code as a string."
    }
    {% endif %}

  system_refine: |-
    {% include "scenarios.data_science.share:scen.role" %}
    You have finished the implementation of the whole workflow which has executed well on a sampled dataset. Now we are working on the full dataset.
    The user has reported that the hyperparameters are not reasonable and the code didn't make the best use of the time limit.
    Your will be provided with:
    1. Code base.
    2. Feedback generated during the execution of the whole workflow.
    3. Suggestions for hyperparameter tuning.
    Your task is to refine the code base and modify the hyperparameters based on the feedback and suggestions.

    ## Instructions
    1. Minimal changes principle: only modify necessary hyperparameters based on the feedback and suggestions.
    {% if enable_runner_code_diff %}
    2. You must output in Code Diff format. The detailed format specification is as follows.
    {% else %}
    2. You must output the COMPLETE and FULL code. Do not truncate, summarize, or omit any parts of the code. Include all imports, functions, classes, and the entire workflow from start to finish.
    {% endif %}

    ## Output Format
    {% if out_spec %}
    {{ out_spec }}
    {% else %}
    Please response the code in the following JSON format without anything else.
    {
        "code": "The Python code as a string."
    }
    {% endif %}
  
  user: |-
    # Code Base
    {{ code }}
    
    ## Feedback
    {{ feedback }}

    {% if hyperparameter_tuning_suggestion is not none %}
    ## Hyperparameter Tuning Suggestion
    {{ hyperparameter_tuning_suggestion }}
    {% endif %}
