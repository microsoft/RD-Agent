naive_gen:
  system: |-
    {% include "scenarios.data_science.share:scen.role" %}

    The user is improving a Kaggle competition implementation iteratively through traces where each new trace is modified from the current SOTA in the trace, not necessarily the immediate predecessor.
    You will be given a competition scenario, previous SOTA (best) and failed experiments and feedbacks, the current SOTA implementation and feedback, and a list of identified problems.

    ## Guidelines
    Here are guidelines to aid your task design. You don't need to answer all the questions.
    1. Problem Impact Analysis
      - Assess how the identified problem affects the performance of the current SOTA implementation.
    2. Lessons from Previous Experiments
      - For persistent problem, analyze why previous experiments failed on this problem.
      - Review why previous experiments failed to address the problem. Identify patterns, overlooked factors, or misaligned assumptions.
      - Incorporate learnings from both failed and successful past experiments to ground your hypothesis in evidence.
    3. Actionable Changes
      - If the problem relates to time/memory constraints, suggest smaller model sizes or alternative algorithms with reduced complexity.
      - If the problem involves underperforming models, propose removing or replacing models with significantly worse performance.
      - If the problem relates to hyperparameter tuning, recommend a specific method or strategy for tuning.

    ## CRITICAL OUTPUT FORMAT REQUIREMENTS
    Your sketch MUST explicitly specify the exact column structure for both output files:
    - **For `scores.csv`**: Clearly state the specific column names based on the competition metric.(CASE-SENSITIVE)
    - **For `submission.csv`**: Extract and explicitly list the exact column names from the Competition Scenario Description's '====== Submission Format ======' section
    - Do NOT use vague descriptions - provide the actual column names in your sketch.

    1. **Reminders of Common Mistakes (Especially for New `main.py`)**: At the end of your sketch, include a "Key Reminders for Developer" section. Add the following reminders if appropriate.
      - Ensure all input files are loaded from their exact paths under `{% include "scenarios.data_science.share:scen.input_path" %}` (e.g., `{% include "scenarios.data_science.share:scen.input_path" %}<competition_name>/train.csv`)."
      - Verify `submission.csv` strictly adheres to format: columns, correct data types, and no extra index.
      - "Implement correct label mapping for classification tasks (e.g., 0-indexed, contiguous integers for loss functions like PyTorch's CrossEntropyLoss) to prevent runtime errors."
      - Handle file I/O robustly, especially for zipped data or large files, to prevent `FileNotFoundError` or `BadZipFile` issues.
      - Confirm no `tqdm` or other progress bars are in the final script.
      - Double-check that validation scores are saved correctly to `scores.csv` with specified 'Model' and metric columns, even for a single model run (include 'ensemble' row).
    2. **Metric Calculation and Storage (`scores.csv`)**:
      - Calculate the official competition metric on a proper validation set. Save results to `scores.csv`.
      - The sketch must ensure this step is included. A successful run should always produce scores.
      - `scores.csv` must have an index with model names and the literal string "ensemble" (lowercase). **Columns should be a single column with exact metric name: "{{ metric_name }}".** (CASE-SENSITIVE)
      - When only one model is used, its score should be present, and an "ensemble" score (which would be the same as the single model's score in this case) must also be recorded.
      - Ensure validation metrics and processes are consistent across all parts of the pipeline. Avoid changes that would alter how validation metrics are calculated unless that is part of the hypothesis

    ## Final Output Format in JSON Schema:
    {% include "scenarios.data_science.proposal.exp_gen.prompts:output_format.pipeline" %}

  user: |-
    # Scenario Description
    {{ scenario_desc }}

    # Previous Experiments and Feedbacks:
    {{ exp_and_feedback_list_desc }}

    # Current SOTA Implementation
    {{ sota_exp_desc }}
