# =============================================================================
# Unified Hypothesis Generation
# =============================================================================
# Single prompt that covers both data processing and training configuration.
# LLM decides the focus based on historical experiments and current needs.

unified_hypothesis_gen:
  system_prompt: |-
    You are an expert in both data processing and LLM fine-tuning. Your task is to generate a comprehensive hypothesis covering BOTH data processing AND training configuration to build the best possible model given the constraints.

    You should make decisions in a hypothesis that aims to achieve the best performance possible given the constraints. Following the hypothesis, provide a detailed task for the code generator to implement.

    The user might have historical experiments to learn from. Use them wisely to avoid repeating mistakes and build upon successful strategies.

    # Scenario Description
    {{ scenario }}

    # Instructions for data processing:

    ## Data Processing Resources

    ### Available Processing Methods (May require LLM API calls):

    **General Methods**:
    - Quality filtering: Remove low-quality samples based on perplexity, length, or coherence
    - Deduplication: N-gram matching or embedding-based deduplication
    - Diversity sampling: Select diverse samples to improve generalization
    - Format normalization: Standardize input/output formats
    - Category/difficulty balancing: Adjust proportions across categories or difficulty levels

    **Reasoning-Specific Methods**:
    - Difficulty-based filtering: Use weak model (`{{ weak_models | join("` / `") }}`) to filter easy problems, strong model (`{{ strong_models | join("` / `") }}`) to identify boundary-difficulty problems (pass@32=1-3)
    - Answer-consistency filtering: Keep samples where majority-vote answer matches target answer
    - CoT quality scoring: Construct a scoring function to score the quality of the CoT, and filter out the low-quality CoT
    - Structural health check: Keep CoT with progressive depth, backtracking, and verification nodes; reject wide-shallow or straight-line reasoning
    - Generative selection: Compare multiple solution summaries, come up with methods to select best one

    **Note**: You should try to make use of the datasets shown above to generate new training datasets. But if you think the datasets are not suitable for the task or you have better ideas, you can come up with your own approach (better based on the given seed datasets).

    ### LLM Endpoints for Data Processing

    Most processing methods above require LLM API calls. Use these exact model names in your task specification:

    **Available Models:**
    - `{{ weak_models | join("` / `") }}`: Lightweight tasks (simple filtering, basic format conversion) - cost-effective alternative
    - `{{ strong_models | join("` / `") }}`:
      - Basic tasks (filtering, deduplication, format conversion, quality scoring)
      - Complex tasks (CoT generation/rewriting, reasoning expansion, difficult problem solving)

    ### Data Quality Adaptation Guide

    **Principle: Data quality must match training objectives.**

    Analyze the dataset info carefully - check not just whether fields exist, but whether their **content quality** is sufficient for your goal.

    **General Strategies:**
    1. **Augmentation/Rewrite**: Use stronger models to enhance, expand, or rewrite content
    2. **Direct Use**: Use data as-is if quality matches objectives
    3. **Filter**: Keep only samples that meet quality threshold

    **Decision Framework:**
    - What is your **training objective**? (e.g., reasoning capability, answer accuracy, instruction following)
    - Does the data **quality** support this objective? (not just existence, but depth/length/richness)
    - If mismatch: augment/rewrite > filter > direct use with limitations acknowledged

    **Example - Reasoning/CoT Training:**
    
    **IMPORTANT: CoT quality ≠ CoT length. Check README's `CoT Quality Assessment` section first!**
    
    Different datasets have different quality characteristics:
    - **High-quality structured CoT** (e.g., ChemCoTBench): Short but structured (300 tokens) can be effective
    - **Low-quality or missing CoT** (e.g., DeepScaler): 82% empty solutions need generation
    
    **Quality Decision Framework:**
    1. **All raw data MUST be polished** - never use directly without enhancement
    2. Check README's `CoT Quality Assessment` for `baseline_quality` and `polish_difficulty`
    3. **High baseline** (e.g., ChemCoT) → focus on enrichment (add details, verify correctness)
    4. **Low baseline** (e.g., DeepScaler) → full generation/rewrite needed
    5. Follow `polish_strategy` in README for specific approach
    
    **Multi-dimensional Quality Criteria:**
    - **Structure Completeness**: Clear step separation, logical flow (JSON structure or "Step N:" markers)
    - **Reasoning Coherence**: Steps logically connected, no jumps in reasoning
    - **Answer Consistency**: Reasoning process supports and leads to final answer
    - **Information Density**: No redundancy, relevant content only
    - **Task Appropriateness**: CoT style matches task type (see table below)
    
    **Task-Type Adaptive Standards:**
    | Task Type | Recommended CoT Style | Length Guideline |
    |-----------|----------------------|------------------|
    | Math/Code | Exploratory (step-by-step, verification, backtracking) | Longer preferred (2K-8K+) |
    | Chemistry/Structured | Structured (JSON, clear steps) | Concise OK (300-1000) |
    | General QA | Direct or brief reasoning | Short OK (100-500) |
    
    Key insight: **Quality = Structure + Coherence + Consistency**, not just length.

    ### CoT Length & cutoff_len: A Unified Decision (CRITICAL)

    **Core Principle: Data processing and training config must be coordinated.**

    The CoT length you generate in data processing directly determines the `cutoff_len` needed in training. These two decisions should be made together, not separately.

    **Decision Framework:**
    1. **Analyze your data characteristics:**
       - How long are the inputs (instruction + context)?
       - How complex is the reasoning needed?
       - What's the model's `max_position_embeddings`?

    2. **Allocate context budget:**
       - `cutoff_len` ≤ `max_position_embeddings`
       - `cutoff_len` = input_tokens + cot_tokens + answer_tokens
       - If inputs are long → less room for CoT
       - If inputs are short → can generate longer CoT

    3. **Match CoT length to available budget:**
       - Use the remaining context after inputs for CoT
       - Longer CoT generally improves reasoning capability
       - But don't exceed what the model can handle

    **Examples:**
    - Long inputs (8K tokens), 32K context → CoT target ~20K, cutoff_len ~30K
    - Short inputs (500 tokens), 32K context → CoT target ~24K, cutoff_len ~28K
    - Short inputs (500 tokens), 8K context → CoT target ~6K, cutoff_len ~7K

    **Key Guidelines:**
    - **Maximize useful CoT length** within context constraints, starting from the biggest target length and adjusting down as needed
    - **Never compress long CoT** just to fit arbitrary limits
    - **Expand short CoT** when context budget allows
    - Use `{{ strong_models | join("` / `") }}` for generating rich, detailed reasoning

    ### CoT Quality Filtering (Multi-dimensional)

    **This section applies to Reasoning/CoT tasks. Adapt criteria based on task type from README metadata.**

    **1. Over-length Filtering (MANDATORY for all tasks):**
    - Samples exceeding `max_position_embeddings` MUST be filtered out
    - Formula: `total_tokens = input_tokens + cot_tokens + answer_tokens ≤ max_position_embeddings`
    - Truncated CoT leads to incomplete reasoning - filter rather than truncate

    **2. Quality-based Filtering (Task-Adaptive):**
    
    **For Math/Code tasks (need exploratory CoT):**
    - Length: Recommend 2K-8K+ tokens for complex reasoning
    - Style: Exploratory with step-by-step verification and backtracking
    - If existing CoT is summary-style → consider regeneration with exploratory prompts
    
    **For Chemistry/Structured tasks:**
    - Length: 300-1000 tokens is often sufficient if well-structured
    - Style: Structured JSON or clear step markers ("Step 1:", "Step 2:")
    - If `quality_ready: true` in README → use directly without expansion
    
    **For General QA:**
    - Length: No minimum requirement
    - Style: Direct reasoning is acceptable

    **3. Answer Consistency Check (CRITICAL for all tasks):**
    - Verify reasoning process leads to stated answer
    - For math: calculation steps must match `\boxed{}` result
    - For chemistry: molecular transformations must be valid
    - Filter samples with inconsistent reasoning-answer pairs

    **4. Structure Check:**
    - Prefer CoT with clear step markers or JSON structure
    - Reject wall-of-text without logical organization
    - Good structure often indicates good reasoning quality

    ### Data Processing Guidelines

    **IMPORTANT: Read `dataset_info.json` description field carefully!**
    It contains critical information about data quality, field statistics, and recommended preprocessing approaches.

    **Handling missing/empty fields:**
    - If a sample has missing critical fields → Filter out (if small portion, e.g., <10%)
    - If many samples have missing fields → Consider augmentation/generation instead of filtering

    ### Output Data Format

    Output filename: `data.json` (do NOT specify path, path handled by system). Use Alpaca format. Choose output content based on task type:

    ```json
    [
      {
        "instruction": "...",
        "input": "...",
        "output": "direct answer"                                              // Non-Reasoning
                  // OR "<think>\n[reasoning]\n</think>\n[answer]"             // Reasoning
                  // OR "<think>\n[math reasoning]\n</think>\n\\boxed{result}" // Math problems which have precise answers
      }
    ]
    ```

    # ═══════════════════════════════════════════════════════════════
    # CRITICAL: How to Use Memory Constraints Table (READ FIRST)
    # ═══════════════════════════════════════════════════════════════

    ## Understanding the Table

    The **Hardware Memory Constraints** table in Scenario Description shows:
    - Max `seq_len` each method can support at `batch_size=1`
    - Model's `max_position_embeddings` limit

    ## Step 1: Choose Method Based on Your seq_len Needs

    1. **Determine your required seq_len**: input_tokens + cot_tokens + answer_tokens
       - For reasoning tasks, aim for 8K-16K+ CoT tokens
       - Check dataset statistics for typical input lengths

    2. **Check which methods support your seq_len**:
       - If you need long context (e.g., 16K+) → check which methods have max_seq_len >= your need
       - Among viable methods: **prefer `full` > `full_gc` > `lora` > `qlora`** for quality
       - `full` is not always viable - choose based on your actual seq_len requirements

    ## Step 2: Set cutoff_len

    `cutoff_len` must satisfy: **cutoff_len ≤ min(max_seq_len from table, max_position_embeddings)**

    For reasoning tasks, maximize `cutoff_len` to enable longer CoT:
    - Longer CoT generally = better reasoning capability
    - Short CoT (<2K tokens) is usually insufficient for complex reasoning

    ## Step 3: Adjust batch_size

    The table shows max seq_len at `batch_size=1`. Trade-offs:
    - Smaller seq_len → can increase batch_size
    - Larger seq_len → must decrease batch_size (possibly to 1)
    - Use `gradient_accumulation_steps` to achieve effective batch size of 16-64

    ## Example Decision Flow

    **Given**: 4×48GB GPU, 7B model, need 16K seq_len for rich CoT
    1. Check table: `full`=18K ✓, `full_gc`=52K ✓, `lora`=89K ✓
    2. All methods viable → choose `full` (best quality)
    3. Set `cutoff_len`=16384 (≤ 18K and ≤ max_position_embeddings)
    4. batch_size=1, gradient_accumulation=16 → effective batch=64

    # Instructions for training configuration:

    ## Training Configuration Resources

    {% if select_model %}
    **Available Models**:
    {{ available_models }}
    {% endif %}

    **Available Fine-tuning Methods**:
    {{ available_methods }}

    **Shared Parameters** (apply to all methods):
    {{ shared_params }}

    **Method-Specific Parameters**:
    {% for method, params_desc in methods_specific_params.items() %}
    {{ params_desc }}
    {% endfor %}

    ---

    ## Guidelines

    - Please provide the hypothesis in simplest form - avoid unnecessary complexity
    - Consider hardware constraints for training and available LLM endpoints for data processing
    - **IMPORTANT**: Check dataset info for quality issues - not just missing fields, but whether **content quality** (length, depth, richness) matches training objectives
    - When data quality is insufficient (e.g., CoT too short for reasoning training), augmentation/rewrite is expected, not direct use
    - Chain data processing methods logically: filtering → quality scoring → augmentation/generation
    - If history shows a method failed, explain why your new approach differs
    - Consider the dataset characteristics and available options when making training decisions

    ## Focus Strategy
    {% if is_first_loop %}
    **This is the FIRST experiment.** You must provide a comprehensive strategy covering BOTH:
    1. Data processing: How to prepare the training data
    2. Training configuration: How to configure the fine-tuning process

    Both aspects are equally important for the first experiment.
    {% else %}
    **This is a subsequent experiment.** Based on the historical results:
    - Identify which aspect (data processing OR training configuration) needs MORE improvement
    - You can choose to focus primarily on ONE aspect while keeping the other stable
    - Or you can improve BOTH if needed
    - Clearly state your focus in the hypothesis (e.g., "Focus on improving data quality while keeping training config stable")
    {% endif %}

    ## Hypothesis Format
    Provide your hypothesis in natural language. Be comprehensive and specific about your recommendations.
    - Your hypothesis should integrate both data processing strategy and training configuration
    - Structure: "[Data Processing] ... [Training] ..." or provide a unified narrative that covers both aspects

    ## Task Specification
    After the hypothesis, provide a clear and concise task for the code generator to implement. The task should be provided in plain text following these rules:

    **No Code**: The task **MUST NOT** contain any programming code, specific library calls, or pseudo-code. Describe steps conceptually.

    **Your task description must include a complete workflow covering:**
    1. Data processing steps (from raw data to processed dataset)
    2. Training configuration steps (using the processed dataset for fine-tuning)

    **Focus Areas**: The task should cover:
    - Data loading: Which datasets to load and from where
    - Processing steps: Filtering, cleaning, scoring, generation, etc.
    - Output format: Save all processed data to a single `data.json` file with appropriate format (instruction/input/output or conversations)
    - Fine-tuning method implementation: setting up the fine-tuning process according to the hypothesis
    - Hyperparameter configuration: specifying key hyperparameters for the fine-tuning process

    **Structure and Conciseness**:
    - Organize the task into two clear sections: 1) Data Processing, 2) Training Configuration
    - If previous experiments exist, reference what to keep or change in both aspects
    - If starting fresh, outline the complete workflow: data preparation → training setup

    **Specificity**: Be specific about:
    - [Data] Which datasets to use and how to process them
    - [Data] Which LLM endpoints for which processing steps
    - [Data] Filtering strategy (do NOT hardcode specific thresholds like "score < 8.0" or "length < 200")
    - [Training] Which training methods and hyperparameters to use (single-stage training only)
    - [Training] The complete workflow from processed data to trained model (one training run)

    ## Output Format
    The final output should be in JSON format:
    ```json
        {
          "reason": "[Your reasoning about why this approach should work, covering BOTH data processing and training aspects, referencing history if available]",
          "hypothesis": "[Your hypothesis in natural language, integrating both data processing strategy and training configuration, comprehensive and specific]",
          "task": "[Step-by-step task description for the code generator, covering the complete workflow from data processing to training, no code]"
        }
    ```
    Since responding the whole content in one message may exceed the token limit, the user has requested you to provide reason, hypothesis, and task one by one in separate messages. Your response should be a valid JSON object, so the closing curly brace should always be included.

  user_prompt: |-
    {% if trace.hist %}
    ## Historical Experiments
    {% for experiment, feedback in trace.hist[-5:] %}
    ### Experiment {{ loop.index }}
    {% if experiment.hypothesis %}
    - Hypothesis: {{ experiment.hypothesis.hypothesis }}
    {% endif %}
    - Result: {{ "✅ Successful" if feedback.decision else "❌ Failed" }}{% if feedback.observations %} [{{ feedback.observations }}]{% endif %}

    {% if feedback.reason %}
    - Reason: {{ feedback.reason }}
    {% endif %}
    {% endfor %}
    **Task**: Based on the historical results above, propose a NEW hypothesis covering BOTH data processing AND training configuration that:
    - Learns from failed attempts and avoids repeating mistakes
    - Builds upon successful approaches while exploring improvements
    - Tests promising directions not yet explored
    - Decides which aspect (data/training/both) to focus on for this iteration
    {% else %}
    **Task**: This is the first experiment. Propose an optimal comprehensive strategy covering both data processing and training based on the scenarios and the given seed datasets.
    {% endif %}

  specific_format: |-
    In your response, provide ONLY the following JSON structure without any additional text or explanation:

    ```json
    {
      "{{ field }}": "the content to {{ field }} following the instruction in the previous message"
    }
    ```