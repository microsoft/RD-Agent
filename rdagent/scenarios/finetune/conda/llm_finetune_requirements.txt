# LLaMA Factory Environment Requirements
# Equivalent to: rdagent/scenarios/finetune/docker/llm_finetune_docker/Dockerfile
# Docker base: hiyouga/llamafactory:0.9.4 uses PyTorch 2.6.0 + CUDA 12.4 + flash-attn 2.7.4

# PyTorch 2.6.0 with CUDA 12.8 (for B200 GPUs)
# Note: Change to cu124 for CUDA 12.4 machines
--index-url https://download.pytorch.org/whl/cu128
torch==2.6.0
torchvision==0.21.0

# Reset to default index for other packages
--index-url https://pypi.org/simple

# Core LlamaFactory package (PyPI latest is 0.9.3, Docker uses 0.9.4 from GitHub)
llamafactory==0.9.3

# FlashAttention-2: installed separately via llm_finetune_flash_attn.txt
# (requires torch installed first, and --no-build-isolation flag)

# Additional dependencies (matches Dockerfile line 17)
bitsandbytes>=0.39.0
mixture-of-depth>=1.1.6
openai

# Common utilities for data processing scripts
requests

# DeepSpeed for memory optimization (version constraint from LlamaFactory)
deepspeed>=0.10.0,<=0.16.9
