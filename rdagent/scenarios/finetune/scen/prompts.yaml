task_description: |-
  Fine-tune {{ model_name }} on {{ dataset_name }} dataset.

  **Dataset Details:**
  {{ dataset_description if dataset_description else "Dataset for fine-tuning the model." }}

  {% if dataset_samples %}
  **Sample Data:**
  {% for sample in dataset_samples[:3] %}
  - {{ sample }}
  {% endfor %}
  {% endif %}

  **Model:** {{ model_name }}
  {% if model_description %}
  {{ model_description }}
  {% endif %}

scenario_description: |-
  # LLM Fine-tuning Scenario

  ## Background
  {{ background }}

  ## Dataset Information
  - Name: {{ dataset_info.name }}
  - Samples: {{ dataset_info.sample_count }} samples
  - Size: {{ dataset_info.total_size_mb }} MB
  - First Sample: {{ dataset_info.samples[0] if dataset_info.samples else "N/A" }}

  ## Model Information
  - Base Model: {{ model_info.name if model_info else "TBD" }}
  {% if model_info and model_info.specs %}
  - Model Specs: {{ model_info.specs }}
  {% endif %}

  ## Timeout Constraints
  - Debug Timeout: {{ debug_timeout }}
  - Full Training Timeout: {{ full_timeout }}

dataset_info_generation:
  system: |-
    Generate a LLaMA-Factory dataset_info.json configuration entry.

    ## Format Types:
    **Alpaca**: instruction-response data with "instruction", "input", "output" fields
    **ShareGPT**: conversational data with message arrays

    ## Required Fields:
    - `file_name`: relative path from datasets directory
    - `formatting`: "alpaca" (default) or "sharegpt"  
    - `columns`: map data fields to LLaMA-Factory expected names

    ## Common Column Mappings:
    - Alpaca: "instruction"→prompt, "input"→query, "output"→response
    - ShareGPT: "conversations"→messages, plus role/content tags
    
    ## Dataset Selection
    {% if target_dataset_list|length == 0 %}
    Generate configuration for all datasets found in the datasets directory.
    {% else %}
    The user only focus on certain dataset(s).
    Generate configuration for the following dataset(s): {{ target_dataset_list }}.
    {% endif %}

    ## Specific Instructions
    - The outer key must be the dataset name (preserve "/" exactly). For example, "my_dataset" or "chat/dataset".
    - For `file_name`, provide the relative path from the original directory to the data files. Mostly it is under "dataset_name/...".
    - Choose "alpaca" or "sharegpt" based on data structure.
    - Map column names from sample data to LLaMA-Factory format.
    - For datasets not following Alpaca or ShareGPT format. Please choose the closest one and adjust column mappings accordingly.
    - The folder might contain multiple files also multiple datasets. Please include all valid datasets you can find in your response.

    ## Examples:
    ```json
    {
      "my_dataset": {
        "file_name": "my_dataset/train.json",
        "columns": {"prompt": "instruction", "response": "output"}
      },
      "chat_dataset": {
        "file_name": "chat_dataset/data.json",
        "formatting": "sharegpt",
        "columns": {"messages": "conversations"}
      }
    }
    ```

    **Output Format**:
    ```json
    {
      "dataset_name": {
        "file_name": "dataset_name/...",
        "formatting": "alpaca|sharegpt",
        "columns": {...}
      }
    }
    ```

  user: |-
    **Dataset folder description**:
    ```
    {{ dataset_info }}
    ```
