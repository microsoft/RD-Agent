scenario_description: |-
  The user is targeting a fine-tuned model best for specific scenarios based on the provided dataset.
  The user has decided to fine-tune the model using LLaMA-Factory framework. Make sure your hypothesis and task align with LLaMA-Factory's capabilities and best practices.

  # User objectives
  By Fine-tuning the model, the user aims to achieve the following objectives:
  {% if user_target_scenario is not none %}
  The user described their target scenario as: {{ user_target_scenario }}
  {% endif %}
  {% if target_benchmark is not none and benchmark_description is not none %}
  The user aims to excel in the following benchmark(s): {{ target_benchmark }}.
  The benchmark can be described as: {{ benchmark_description }}.
  {% endif %}

  # Device Information
  The device available for fine-tuning has the following specifications:
  {{ device_info }}
  The hardware constraints might limit certain choices, so consider them carefully.

  {% if chosen_model %}
  # Base Model Details
  The user has decided the base model to fine-tune: {{ base_model }}.
  ## Model Details
  {{ model_info }}
  {% else %}
  The user has not yet decided the base model to fine-tune.
  {% endif %}

  {% if enable_dataset_description %}
  # Provided Dataset Details
  {{ dataset_info }}
  The dataset characteristics is provided for you to understand the data better. Each dataset is a folder of files. Some in good format for fine-tuning while some are not.

  ## Dataset info json
  {{ data_info_json }}

  ## Timeout Constraints
  - Debug Timeout: {{ debug_timeout }}
  - Full Training Timeout: {{ full_timeout }}
  {% endif %}
  
dataset_info_generation:
  system: |-
    Generate a LLaMA-Factory dataset_info.json configuration entry with category classification and insightful descriptions.

    ## Format Types:
    **Alpaca**: instruction-response data with "instruction", "input", "output" fields
    **ShareGPT**: conversational data with message arrays

    ## Required Fields:
    - `file_name`: relative path from datasets directory
    - `formatting`: "alpaca" (default) or "sharegpt"  
    - `columns`: map data fields to LLaMA-Factory expected names
    - `category`: list of domain categories (can be one or multiple, see below)
    - `description`: AI-generated concise summary (200-300 words) highlighting key insights for fine-tuning

    ## Category Classification
    Analyze each dataset and classify it by domain. A dataset can belong to **one or multiple** domains. Use one or more of these category names:
    
    - **physics**: Physics, mechanics, thermodynamics, quantum mechanics, electromagnetism, optics
    - **math**: Mathematics, algebra, geometry, calculus, statistics, logic, mathematical reasoning
    - **chemistry**: Chemistry, organic chemistry, inorganic chemistry, biochemistry, molecular structures
    - **biology**: Biology, genetics, ecology, microbiology, cell biology, evolution, physiology
    - **legal**: Law, regulations, contracts, legal documents, court cases, legislation
    - **medicine**: Medicine, clinical data, medical diagnosis, pharmaceuticals, healthcare, patient care
    - **finance**: Finance, trading, economics, investment, markets, banking, financial analysis
    
    **Classification Guidelines:**
    - Examine column names, sample content, and README files carefully
    - Look for domain-specific keywords in dataset names and descriptions
    - **A dataset can have multiple categories** if it covers multiple domains (e.g., biophysics → ["biology", "physics"])
    - List ALL applicable domains, ordered by relevance (most relevant first)

    ## Description Generation Guidelines
    For each dataset, analyze the README content (if available) and data samples to generate **practical insights about the data itself**. Focus ONLY on data characteristics and preprocessing - do NOT include training strategies, hyperparameters, or model selection advice.
    
    **Focus Areas (data-centric insights only):**
    
    1. **Data Structure Insights** (MOST IMPORTANT):
       - Are there columns that need to be combined? (e.g., "The 'answer' field only contains final results; concatenate with 'solution' field to get complete reasoning chains")
       - Missing or incomplete fields that need handling?
       - Relationships between columns that matter?
    
    2. **Data Quality & Format Issues**:
       - Any format inconsistencies or cleaning needed?
       - Edge cases or problematic samples to filter?
       - Encoding or special character issues?
    
    3. **Content Characteristics**:
       - Token length distribution (if highly variable)
       - Domain and difficulty level of the content
       - What makes this dataset unique or valuable as training data?
    
    4. **Data Curation Background** (if available from README):
       - How was the data collected/curated?
       - Any known limitations or biases?
       - Benchmark results that validate data quality?
    
    **Description Requirements:**
    - Write 150-250 words, focused on data itself
    - **DO NOT include**: training strategies, hyperparameters, model recommendations, batch sizes, learning rates
    - **DO include**: field transformations, data preprocessing steps, quality issues, content characteristics
    - Be specific about what preprocessing the data needs before it can be used
    - Use concrete examples when explaining data operations

    ## Common Column Mappings:
    - Alpaca: "instruction"→prompt, "input"→query, "output"→response
    - ShareGPT: "conversations"→messages, plus role/content tags
    
    ## Dataset Selection
    {% if target_dataset_list|length == 0 %}
    Generate configuration for all datasets found in the datasets directory.
    {% else %}
    The user only focus on certain dataset(s).
    Generate configuration for the following dataset(s): {{ target_dataset_list }}.
    {% endif %}

    ## Specific Instructions
    - The outer key must be the dataset name (preserve "/" exactly). For example, "my_dataset" or "chat/dataset".
    - For `file_name`, provide the relative path from the original directory to the data files. Mostly it is under "dataset_name/...".
    - Choose "alpaca" or "sharegpt" based on data structure.
    - Map column names from sample data to LLaMA-Factory format.
    - Assign appropriate `category` array based on dataset content analysis:
      - Single domain: `["math"]`
      - Multiple domains: `["physics", "math"]` (ordered by relevance)
    - Generate insightful `description` based on README and sample analysis
    - For datasets not following Alpaca or ShareGPT format. Please choose the closest one and adjust column mappings accordingly.
    - The folder might contain multiple files also multiple datasets. Please include all valid datasets you can find in your response.

    ## Examples:
    ```json
    {
      "reasoning_problems": {
        "file_name": "reasoning_problems/train.jsonl",
        "columns": {"question": "instruction", "thinking": "output"},
        "category": ["math"],
        "description": "DATA STRUCTURE NOTE: The 'thinking' field contains step-by-step reasoning process but does NOT include the final answer; the 'final_answer' field contains only the boxed result. For complete reasoning chains, concatenate 'final_answer' to the end of 'thinking' field. Response lengths are highly variable (500-30000 tokens). Some samples have empty 'final_answer' field - these should be filtered or the answer extracted from the last line of 'thinking'."
      },
      "conversation_corpus": {
        "file_name": "conversation_corpus/data.json",
        "formatting": "sharegpt",
        "columns": {"messages": "conversations"},
        "category": ["general"],
        "description": "DATA QUALITY NOTES: Some conversation entries have broken context - the assistant response references content not present in user query. Code blocks may contain escaped characters (\\n shown as literal instead of newline). The 'system' role is inconsistently present across samples. Recommend filtering conversations where turn count < 2."
      }
    }
    ```

    **Output Format**:
    ```json
    {
      "dataset_name": {
        "file_name": "dataset_name/...",
        "formatting": "alpaca|sharegpt",
        "columns": {...},
        "category": ["domain1", "domain2", ...],
        "description": "Data-centric insights: structure notes, preprocessing needs, quality issues (150-250 words)"
      }
    }
    ```

  user: |-
    **Dataset folder description**:
    ```
    {{ dataset_info }}
    ```
