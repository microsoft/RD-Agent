# LLaMA Factory Environment Requirements
# Equivalent to: rdagent/scenarios/finetune/docker/llm_finetune_docker/Dockerfile
# Docker base: hiyouga/llamafactory:0.9.4 uses PyTorch 2.6.0 + CUDA 12.4 + flash-attn 2.7.4

# PyTorch 2.9.0 with CUDA 12.8 (for B200 GPUs with sm_100 architecture)
# Note: PyTorch 2.6.0 only supports up to sm_90, B200 requires 2.8.0+
# For non-B200 machines with CUDA 12.4, change to cu124 and torch==2.6.0
--index-url https://download.pytorch.org/whl/cu128
torch==2.9.0
torchvision==0.24.0

# Reset to default index for other packages
--index-url https://pypi.org/simple

# Core LlamaFactory package (PyPI latest is 0.9.3, Docker uses 0.9.4 from GitHub)
llamafactory==0.9.3

# FlashAttention-2: installed separately via llm_finetune_flash_attn.txt
# (requires torch installed first, and --no-build-isolation flag)

# Transformers library (for tokenizer)
transformers

# Additional dependencies (matches Dockerfile line 17)
bitsandbytes>=0.39.0
mixture-of-depth>=1.1.6
litellm

# Common utilities for data processing scripts
requests

# DeepSpeed for memory optimization
# Note: LlamaFactory 0.9.3 requires deepspeed<=0.16.9 (hardcoded check in parser.py)
deepspeed>=0.10.0,<=0.16.9

# LlamaFactory optional dependencies (commonly used)
# Liger Kernel - fused triton kernels for training acceleration
liger-kernel>=0.5.5

# Metrics for evaluation
nltk
jieba
rouge-chinese

# Advanced optimizers
galore-torch
apollo-torch
badam>=1.2.1
adam-mini

# Quantization
hqq

# FP8 training support
torchao>=0.8.0
