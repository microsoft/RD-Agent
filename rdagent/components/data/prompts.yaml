search_params:
  system: |-
    You are a HuggingFace dataset search expert. Generate search parameters based on 3 core dimensions.

    ## Core Parameters (Only These 3)

    ### 1. Domain/Topic (é¢†åŸŸ/ä¸»é¢˜)
    - **What it is**: The subject area or application domain
    - **How to specify**: Use a SINGLE English keyword (not phrases)
    - **Standard domains** (USE THESE EXACT TERMS when applicable):
      * art - Art, design, creative works
      * code - Programming, software, coding
      * medical - Healthcare, clinical, medicine
      * biology - Life sciences, genomics, biological
      * finance - Financial, economy, banking
      * legal - Law, legislation, judicial
      * chemistry - Chemical, molecular
      * climate - Weather, environmental, climate science
      * math - Mathematics, mathematical, arithmetic
      * education - Learning, academic, teaching
      * agriculture - Farming, crops, agricultural
      * law - Legal systems, regulations (use "legal" if more appropriate)
      * music - Audio, songs, musical
      * business - Commerce, trade, corporate
      * sports - Athletics, games, physical activities
    - **Technical note**: Maps to HuggingFace `search` parameter (fuzzy matching across dataset names, descriptions, README)
    - **Guidelines**:
      * **ALWAYS use the standard domain term from the list above** when user's request matches
      * Use ONE broad term (e.g., "finance" not "financial services")
      * Avoid multi-word phrases (e.g., ~~"medical question answering"~~ â†’ "medical")
      * **DO NOT include language names** (e.g., ~~"chinese finance"~~ â†’ "finance" + language filter)
      * Keep it to a single keyword for best results

    ### 2. Size Categories (æ•°æ®é›†å¤§å°) - OPTIONAL
    - **What it is**: Dataset size range for filtering by scale
    - **How to specify**: Exact size category identifier from HuggingFace
    - **Common values**:
      * `n<1K` - Very small (< 1,000 samples)
      * `1K<n<10K` - Small (1,000 to 10,000 samples)
      * `10K<n<100K` - Medium (10K to 100K samples)
      * `100K<n<1M` - Large (100K to 1M samples)
      * `1M<n<10M` - Very large (1M to 10M samples)
      * `10M<n<100M` - Extremely large (10M to 100M samples)
      * `100M<n<1B` - Massive (100M to 1B samples)
      * `n>1B` - Ultra-massive (> 1 billion samples)
    - **Technical note**: Maps to HuggingFace filter `size_categories:xxx`
    - **Guidelines**:
      * Set to `null` if user doesn't specify size requirements
      * Use this to avoid extremely large datasets that are hard to download
      * Consider user's use case: small for prototyping, large for production

    ### 3. Language (è¯­è¨€) - OPTIONAL
    - **What it is**: Natural language(s) contained in the dataset
    - **How to specify**: ISO 639-1 language code (2 letters lowercase)
    - **Common values**:
      * `zh` - Chinese (Mandarin)
      * `en` - English
      * `es` - Spanish
      * `fr` - French
      * `de` - German
      * `ja` - Japanese
      * `ko` - Korean
      * `ar` - Arabic
      * `multilingual` - Multiple languages or cross-lingual
    - **Technical note**: Maps to HuggingFace filter `language:xxx`
    - **Guidelines**:
      * **CRITICAL**: Set to `null` if language is NOT EXPLICITLY mentioned by user
      * **DO NOT infer** language from user's query language (user writes in Chinese â‰  dataset must be Chinese)
      * Only set if user clearly states language requirement: "ä¸­æ–‡æ•°æ®é›†", "English dataset", "éœ€è¦è‹±æ–‡çš„", etc.
      * Use `multilingual` for translation datasets or cross-lingual tasks
      * Use 2-letter ISO code, not full language name

    ## Output Format

    Return JSON with ONLY these 3 fields plus reasoning:
    ```json
    {
      "domain": "single-keyword",
      "size_categories": "size-range or null",
      "language": "language-code or null",
      "reasoning": "1-2 sentences explaining your choices"
    }
    ```

    ## Examples

    <example>
    User: "éœ€è¦ä¸­æ–‡å®¢æœå¯¹è¯æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå¯¹è¯åˆ†ç±»æ¨¡å‹"

    Output:
    {
      "domain": "customer",
      "size_categories": null,
      "language": "zh",
      "reasoning": "Customer service domain, Chinese language explicitly required, no size constraint specified."
    }
    </example>

    <example>
    User: "æƒ³æ‰¾åŒ»ç–—é¢†åŸŸçš„é—®ç­”æ•°æ®ï¼Œæœ€å¥½æ˜¯ä¸­ç­‰è§„æ¨¡çš„ï¼Œä¸é™è¯­è¨€"

    Output:
    {
      "domain": "medical",
      "size_categories": "10K<n<100K",
      "language": null,
      "reasoning": "Medical domain, medium-sized dataset (10K-100K) as requested, no language constraint."
    }
    </example>

    <example>
    User: "éœ€è¦ç”¨äºæƒ…æ„Ÿåˆ†æçš„è¯„è®ºæ•°æ®ï¼Œè¦å¤§è§„æ¨¡çš„"

    Output:
    {
      "domain": "sentiment",
      "size_categories": "1M<n<10M",
      "language": null,
      "reasoning": "Sentiment analysis domain, large-scale dataset (1M-10M samples), no specific language requirement."
    }
    </example>

    <example>
    User: "æ‰¾ä¸ªè‹±æ–‡ç¿»è¯‘æ•°æ®é›†ï¼Œä¸­è¯‘è‹±çš„ï¼Œå°è§„æ¨¡å°±è¡Œ"

    Output:
    {
      "domain": "translation",
      "size_categories": "1K<n<10K",
      "language": "multilingual",
      "reasoning": "Translation domain, small-scale dataset (1K-10K), multilingual since it involves Chinese-to-English translation."
    }
    </example>


  user: |-
    ## Task Requirement

    {{ task_description }}

    ## Your Task

    Generate HuggingFace search parameters using ONLY the 3 core dimensions:
    1. **Domain/Topic** - What subject area? Use a SINGLE keyword from common domains (e.g., "math", "code", "medical", "finance", "biology")
    2. **Size Categories** - Dataset size requirement? (e.g., "10K<n<100K", "1M<n<10M", or null if not specified)
    3. **Language** - What language(s)? **ONLY set if user EXPLICITLY mentions language** (e.g., "ä¸­æ–‡", "English", "multilingual"). Otherwise set to null. DO NOT infer from user's query language.

    Output JSON with domain, size_categories, language, and reasoning.


dataset_selection:
  system: |-
    You are an expert at selecting the most suitable dataset for machine learning tasks, especially for fine-tuning scenarios.

    ## ğŸ“¦ Dataset Information Format

    Each candidate includes:
    - `id`: HuggingFace dataset identifier
    - `description`: Dataset description
    - `downloads`, `likes`: Community metrics
    - `tags`: Complete list of ALL HuggingFace tags (e.g., "task_categories:question-answering", "size_categories:10K<n<100K", "language:en")
    - `structured_info`: Pre-extracted common fields for quick reference
      - `task_ids`: Extracted task categories from tags
      - `languages`: Extracted language codes
      - `licenses`: Extracted license types
      - `modalities`: Extracted data modalities

    **Note**: Use `tags` for comprehensive information (including all task categories), and `structured_info` for quick reference.

    ## ğŸ¯ Selection Criteria (Weighted Evaluation)

    ### 1. Task Relevance (Weight: 40%) - MOST IMPORTANT
    - Does the description semantically match the user's task?
    - Check `tags` for task_categories (e.g., "task_categories:question-answering", "task_categories:text-classification")
    - You can also check `task_ids` in `structured_info` for quick reference
    - Related tasks are acceptable (e.g., "dialogue" for "conversation", "text-generation" for "summarization")
    - Domain alignment: Dataset's domain should match user's requirements
    - **Since we don't filter by task_type in search, YOU must judge task relevance carefully**

    ### 2. License Compatibility (Weight: 30%) - CRITICAL FOR FINE-TUNING
    - Check `tags` for license info (e.g., "license:mit", "license:apache-2.0")
    - You can also check `licenses` in `structured_info`
    - **AVOID**: Any license containing:
      - NC (Non-Commercial): cc-by-nc, cc-by-nc-sa, cc-by-nc-nd
      - ND (No Derivatives): cc-by-nd
      - Copyleft (requires open-sourcing): gpl, agpl
    - **PREFER**: mit, apache-2.0, cc-by, cc-by-sa, cc0, or no explicit license
    - If license is unclear/missing, mention in reasoning but don't disqualify

    ### 3. Language Match (Weight: 20%) - IF USER SPECIFIED
    - User's language requirement: {{ user_language or "Not specified (any language acceptable)" }}
    - Check `tags` for language info (e.g., "language:zh", "language:en")
    - You can also check `languages` in `structured_info`
    - Multi-lingual datasets are OK if they include the target language
    - If user didn't specify language, ignore this criterion

    ### 4. Modality & Quality (Weight: 10%)
    - **Modality**: 
      - **STRONGLY PREFER** datasets with ONLY "modality:text" (pure text datasets)
      - Check `tags` for modality info (e.g., "modality:text")
      - You can also check `modalities` in `structured_info`
      - Multi-modal datasets (text+image, text+audio) should be **deprioritized**
      - If modality tag is missing, infer from description (prefer text-only)
    - **Quality indicators**:
      - Downloads: Community trust and battle-tested reliability
      - Likes: User satisfaction and quality endorsement
      - Use as secondary factor when task relevance is similar

    ## ğŸ“Š Scoring Guidance (for reference)

    - Task relevance: 0-40 points (semantic match to user's requirements)
    - License compatibility: 0-30 points (commercial-friendly for fine-tuning)
    - Language match: 0-20 points (if user specified language)
    - Modality & quality: 0-10 points (text-based + community validation)

    **Total score â†’ Confidence: 85-100pts = 0.85-1.0, 65-84pts = 0.65-0.84, <65pts = 0.0-0.64**

    ## ğŸ“‹ Output Requirements

    Return JSON:
    ```json
    {
      "dataset_id": "owner/dataset-name",
      "reason": "Detailed explanation covering: (1) task relevance, (2) license status, (3) language match (if applicable), (4) quality indicators",
      "confidence": 0.85,
      "alternatives": ["alt1", "alt2"],
      "warnings": ["Optional: concerns about license/language/modality"]
    }
    ```

    - `reason`: 2-3 sentences explaining why this is the best choice
    - `confidence`: 0.0-1.0 based on weighted score above
    - `alternatives`: Up to 2 runner-up options (or empty list)
    - `warnings`: Optional list of potential concerns (e.g., "License is not explicitly stated", "Multi-modal dataset may include non-text data")

  user: |-
    ## User's Task Requirement

    {{ task_description }}

    ## User's Search Parameters

    - Language preference: {{ user_language or "Not specified (any language OK)" }}

    ## Candidate Datasets

    {{ candidates_json }}

    ## Your Task

    Analyze each candidate using the 4-dimension weighted criteria:
    1. Task relevance (40%): Does it match the user's requirements?
    2. License compatibility (30%): Is it fine-tuning-friendly?
    3. Language match (20%): Does it match user's language (if specified)?
    4. Modality & quality (10%): Is it text-based with good community validation?

    Select the SINGLE best dataset and provide detailed reasoning. Prioritize task relevance and license compatibility.


retry_search_params:
  system: |-
    You are a HuggingFace dataset search expert. The previous search returned 0 results.
    Your task is to generate NEW search parameters that are more likely to find datasets.

    ## Retry Strategy Guidelines

    ### ğŸ”„ Recommended Adjustments (in order of preference)

    1. **Relax size_categories** (least critical)
       - If original had size constraint, set to `null`
       - Example: "10K<n<100K" â†’ null

    2. **Try broader/related domain terms** (expand search scope)
       - **IMPORTANT**: If the domain is already one of the standard terms (art, code, medical, biology, finance, legal, chemistry, climate, math, education, agriculture, law, music, business, sports), **DO NOT change it**
       - Only modify domain if it's NOT a standard term or if using a related broader term might help
       - Examples of acceptable changes:
         * "financial" â†’ "finance" (standardize to canonical term)
         * "healthcare" â†’ "medical" (standardize to canonical term)
         * "programming" â†’ "code" (standardize to canonical term)
       - Keep it to single keyword

    3. **Relax language** (if not explicitly required by user)
       - If original had language, set to `null`
       - Cross-lingual datasets might be useful

    ### âŒ What NOT to do
    - Don't make parameters MORE restrictive
    - Don't change domain to completely unrelated topics
    - Don't add new constraints that weren't in original request

    ## Output Format

    Return JSON with ONLY these 3 fields plus reasoning:
    ```json
    {
      "domain": "single-keyword",
      "size_categories": "size-range or null",
      "language": "language-code or null",
      "reasoning": "Explain what you changed and why it might help find results"
    }
    ```

    ## Examples

    <example>
    Failed params: {"domain": "finance", "size_categories": "10K<n<100K", "language": "zh"}

    Output:
    {
      "domain": "financial",
      "size_categories": null,
      "language": "zh",
      "reasoning": "Removed size constraint (least critical) and tried domain synonym 'financial' to expand search scope while preserving language requirement."
    }
    </example>

    <example>
    Failed params: {"domain": "medical", "size_categories": null, "language": "en"}

    Output:
    {
      "domain": "healthcare",
      "size_categories": null,
      "language": null,
      "reasoning": "Tried domain synonym 'healthcare' and removed language constraint to expand search scope."
    }
    </example>

  user: |-
    ## Original User Request
    {{ task_description }}

    ## Failed Search Parameters (returned 0 results)
    {{ failed_params_json }}

    ## Your Task
    Generate NEW search parameters that are more likely to succeed.
    Follow the retry strategy guidelines above.

    Output JSON with domain, size_categories, language, and reasoning.
    In the reasoning, explain what you changed and why.

analyze_files_for_sft:
  system: |-
    You are a dataset cleanup expert. The user is preparing datasets for SFT (Supervised Fine-Tuning) text training.

    Your task: Analyze files in a dataset directory and determine which are useful for SFT training, which are junk.

    ## SFT Training Needs

    **Useful files (KEEP):**
    - Text data files: .csv, .json, .jsonl, .parquet, .arrow
    - Contains Q&A pairs, dialogues, instructions, or text data
    - Annotation files (e.g., annot.json, labels.json)
    - Data files in subdirectories (e.g., data/train.parquet)

    **Junk files (REMOVE):**
    - Images, audio, video: .zip, .tar.gz, .png, .jpg, .mp3, .wav
    - Git files: .git/, .gitattributes, .gitignore
    - Cache directories: .cache/, __pycache__/
    - Documentation: README.md, LICENSE, .md files
    - Configuration: .yaml, .yml (unless clearly data-related)

    ## Special Cases

    - **Compressed files (.zip, .tar.gz)**: Usually contain images/audio â†’ REMOVE
    - **JSON files**: Could be data OR metadata â†’ Check name (annot.json â†’ KEEP, config.json â†’ REMOVE)
    - **data/ subdirectories**: Usually contain actual data â†’ KEEP
    - **Unknown extensions**: Default to KEEP (safer than removing)

    ## Output Format

    Return JSON:
    ```json
    {
      "file_analysis": {
        "images.zip": {
          "useful": false,
          "reason": "Compressed file likely containing images, not needed for text SFT"
        },
        "annot_testmini.json": {
          "useful": true,
          "reason": "Annotation file, may contain important question labels"
        },
        "data/train.parquet": {
          "useful": true,
          "reason": "Training data in parquet format"
        },
        "README.md": {
          "useful": false,
          "reason": "Documentation file, not training data"
        }
      }
    }
    ```

    **IMPORTANT**:
    - Analyze EVERY file provided
    - Be conservative: if unsure, mark as useful=true
    - Focus on file size: prioritize removing large junk files (images.zip)

  user: |-
    ## Task Description
    {{ task_description }}

    ## Files in Dataset
    {{ files_json }}

    {{ file_contents }}

    ## Your Task
    Analyze each file and determine if it's useful for SFT text training.
    Return JSON with file_analysis for each file.

schema_analysis_for_sft:
  system: |-
    You are a dataset schema analysis expert. Your task is to analyze a dataset's structure and determine:
    1. Data type (single-turn Q&A vs multi-turn dialogue)
    2. Column name mapping for Alpaca format conversion

    ## Alpaca Format Requirements

    Target format:
    ```json
    {
      "instruction": "The question or instruction",
      "input": "Optional additional context (can be empty string)",
      "output": "The expected answer or response"
    }
    ```

    ## Data Type Detection

    ### Single-Turn Q&A
    - Each row is an independent question-answer pair
    - No conversation history or context dependencies
    - Examples: Q&A datasets, instruction-following, text classification

    ### Multi-Turn Dialogue
    - Contains conversation history with multiple rounds
    - Typical structures:
      * OpenAI format: `[{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]`
      * ShareGPT format: `{"conversations": [{"from": "human", "value": "..."}, {"from": "gpt", "value": "..."}]}`
      * Custom formats with conversation/dialogue/messages fields
    - Look for: "role", "from", "conversation", "dialogue", "messages", "history" in column names or data structure

    ## Column Name Mapping

    Common patterns to recognize:

    **Instruction column** (question/task):
    - question, query, prompt, instruction, task, input, text, user, human

    **Input column** (optional context):
    - context, background, history, previous_conversation, input_context
    - Set to `null` if no additional context field exists

    **Output column** (answer/response):
    - answer, response, output, completion, assistant, gpt, target, label

    ## Output Format

    Return JSON:
    ```json
    {
      "data_type": "single_turn" | "multi_turn",
      "instruction_col": "column_name_for_instruction",
      "input_col": "column_name_for_input" | null,
      "output_col": "column_name_for_output",
      "reasoning": "Explain your analysis: (1) why single/multi-turn, (2) why these columns map to instruction/input/output"
    }
    ```

    **Important**:
    - If `input_col` doesn't exist (no additional context), set to `null`
    - For multi-turn dialogues, identify the column that contains the conversation array
    - Be conservative: if unsure about data type, default to "single_turn"

  user: |-
    ## Task Description
    {{ task_description }}

    ## Dataset Sample (first 10 rows)

    Column names: {{ column_names }}

    Sample data:
    {{ sample_data_json }}

    ## Your Task

    Analyze the dataset structure and determine:
    1. Is this single-turn Q&A or multi-turn dialogue?
    2. Which columns map to instruction, input (optional), and output for Alpaca format?

    Return JSON with data_type, instruction_col, input_col, output_col, and reasoning.

quality_scoring_batch:
  system: |-
    You are a dataset quality evaluation expert for SFT (Supervised Fine-Tuning) training.

    Your task: Score multiple samples (0-10) based on their suitability for fine-tuning language models.

    ## Scoring Dimensions (Total: 10 points)

    ### 1. Task Relevance (4 points) - MOST IMPORTANT
    - Does the sample match the user's task requirements?
    - Is the question/instruction meaningful and on-topic?
    - Is the answer/output directly addressing the question?
    - **Deduct points for**: Off-topic content, irrelevant Q&A pairs, spam

    ### 2. Clarity & Quality (3 points)
    - Is the instruction clear and unambiguous?
    - Is the output well-written and coherent?
    - Proper grammar, spelling, punctuation?
    - **Deduct points for**: Unclear questions, gibberish, poor formatting, excessive typos

    ### 3. Completeness (3 points)
    - Is the answer complete and informative?
    - Does it provide sufficient detail?
    - Are there missing fields or empty responses?
    - **Deduct points for**: Incomplete answers, truncated text, missing critical information

    ## Scoring Guidelines

    - **9-10**: Excellent - Perfect for training, high-quality, relevant, complete
    - **7-8**: Good - Acceptable for training, minor issues
    - **5-6**: Mediocre - Borderline, may need cleanup
    - **3-4**: Poor - Significant issues, likely should be filtered out
    - **0-2**: Very poor - Unusable, spam, or completely irrelevant

    ## Output Format

    Return JSON:
    ```json
    {
      "samples": [
        {
          "index": 0,
          "score": 8.5,
          "reason": "Relevant math problem, clear question, complete step-by-step solution. Minor formatting issue in output."
        },
        {
          "index": 1,
          "score": 4.0,
          "reason": "Question is unclear, answer is incomplete and lacks detail. Not suitable for training."
        }
      ]
    }
    ```

    **Important**:
    - Score EVERY sample provided (use index to match)
    - Be consistent in scoring standards
    - Provide brief reason (1 sentence) for each score
    - Consider the user's specific task when evaluating relevance

  user: |-
    ## Task Description
    {{ task_description }}

    ## Samples to Score ({{ num_samples }} samples)

    {{ samples_json }}

    ## Your Task

    Score each sample (0-10) based on:
    1. Task relevance (4 pts): Does it match the task requirements?
    2. Clarity & quality (3 pts): Is it well-written and clear?
    3. Completeness (3 pts): Is the answer complete and informative?

    Return JSON with scores and reasons for all {{ num_samples }} samples.

sentiment_analysis:
  system: |-
    ä½ åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–æ–‡å­—æˆ–ä»£ç å—æ ‡è®°ã€‚

  user: |-
    ä½ æ˜¯ä¸€ä½ä¸“æ³¨äºè´¢åŠ¡æŠ¥å‘Šåˆ†æçš„ä¸­å›½è´¢åŠ¡ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æè´¢ç»ç±»ç¤¾äº¤åª’ä½“æ–‡ç« ä¸­æ‰€è¡¨è¾¾çš„**å¯¹æœªæ¥çš„é¢„æœŸæƒ…ç»ª**ï¼Œ**è€Œéå½“å‰æˆ–å·²å‘ç”Ÿäº‹ä»¶çš„æƒ…ç»ªè¯„ä»·**ã€‚è¯·åŠ¡å¿…**é¿å…å‰ç»åå·®**ï¼Œä»…æ ¹æ®æ–‡æœ¬ä¸­å‡ºç°çš„ä¿¡æ¯ã€è¯­æ°”åŠæªè¾ï¼Œæ¨æ–­ä½œè€…å½“ä¸‹æ‰€è¡¨è¾¾çš„å¯¹æœªæ¥çš„çœ‹æ³•ã€‚

    ä»¥ä¸‹æ˜¯éœ€è¦åˆ†æçš„æ–‡æœ¬å†…å®¹:
    {{ text }}

    è¯·åˆ†åˆ«è¯„ä¼°ä»¥ä¸‹9ä¸ªç»´åº¦çš„**æœªæ¥é¢„æœŸæƒ…ç»ªå€¾å‘**ï¼š
    1. é¢„æœŸå›½å†…å®è§‚ç»æµåŸºæœ¬é¢
    2. é¢„æœŸå…¨çƒå®è§‚ç»æµåŸºæœ¬é¢  
    3. é¢„æœŸå›½å†…è‚¡ç¥¨å¸‚åœº
    4. é¢„æœŸå›½å†…å€ºåˆ¸å¸‚åœº
    5. é¢„æœŸå›½å†…è´¢æ”¿æ”¿ç­–
    6. é¢„æœŸå›½å†…è´§å¸æ”¿ç­–
    7. é¢„æœŸå›½å†…ç›‘ç®¡æ”¿ç­–
    8. é¢„æœŸè¡Œä¸šåŸºæœ¬é¢
    9. é¢„æœŸè‚¡ç¥¨åŸºæœ¬é¢

    æ¯ä¸ªç»´åº¦ä»…åœ¨æ–‡ç« ä¸­**æ˜ç¡®æ¶‰åŠå¹¶è¡¨è¾¾æœªæ¥é¢„æœŸæ—¶**è¿›è¡Œæƒ…ç»ªåˆ¤æ–­ã€‚æ¯ä¸ªç»´åº¦çš„è¾“å‡ºåˆ†ä¸ºä»¥ä¸‹å…­ç±»ä¹‹ä¸€ï¼š
    - "æ­£é¢"
    - "å¼±æ­£é¢"  
    - "ä¸­æ€§"
    - "å¼±è´Ÿé¢"
    - "è´Ÿé¢"
    - "æœªæåŠ"
    è¯·æŒ‰ç…§ä¸Šè¿°9ä¸ªç»´åº¦é€ä¸€åˆ†æå¹¶ç»™å‡ºåˆ¤æ–­ç»“æœã€‚

    ã€è¾“å‡ºæ ¼å¼ã€‘è¯·è¿”å›JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–æ–‡å­—æˆ–ä»£ç å—æ ‡è®°ã€‚
    {
        "results": {
                "å›½å†…å®è§‚ç»æµåŸºæœ¬é¢": "å…­ç±»æƒ…ç»ªä¹‹ä¸€",
                "å…¨çƒå®è§‚ç»æµåŸºæœ¬é¢": "å…­ç±»æƒ…ç»ªä¹‹ä¸€",
                "å›½å†…è‚¡ç¥¨å¸‚åœº": "å…­ç±»æƒ…ç»ªä¹‹ä¸€",
                "å›½å†…å€ºåˆ¸å¸‚åœº": "å…­ç±»æƒ…ç»ªä¹‹ä¸€",
                "å›½å†…è´¢æ”¿æ”¿ç­–": "å…­ç±»æƒ…ç»ªä¹‹ä¸€",
                "å›½å†…è´§å¸æ”¿ç­–": "å…­ç±»æƒ…ç»ªä¹‹ä¸€",
                "å›½å†…ç›‘ç®¡æ”¿ç­–": "å…­ç±»æƒ…ç»ªä¹‹ä¸€",
                "è¡Œä¸šåŸºæœ¬é¢": "å…­ç±»æƒ…ç»ªä¹‹ä¸€",
                "è‚¡ç¥¨åŸºæœ¬é¢": "å…­ç±»æƒ…ç»ªä¹‹ä¸€"
            }
    }

heavy_conversion:
  system: |-
    You are a data conversion expert specializing in creating high-quality instruction-output pairs for SFT (Supervised Fine-Tuning) training.

    Your task is to convert raw, unstructured, or poorly-formatted data into clean Alpaca format training samples.

    ## Target Alpaca Format
    ```json
    {
      "instruction": "A clear question, task, or instruction",
      "input": "Optional additional context or background (can be empty string)",
      "output": "A complete and appropriate response/answer"
    }
    ```

    ## Conversion Guidelines

    ### 1. Data Interpretation
    - Analyze the raw data to understand its content and structure
    - Identify potential question-answer relationships
    - Extract meaningful information that can be used for training
    - Handle various formats: tables, lists, narratives, dialogues, etc.

    ### 2. Quality Standards
    - **instruction**: Must be clear, specific, and answerable
    - **input**: Use for additional context only when necessary
    - **output**: Must be complete, accurate, and directly address the instruction

    ### 3. Common Conversion Patterns
    - **Factual data** â†’ "What is..." questions with informative answers
    - **Dialogues** â†’ Extract meaningful Q&A pairs or tasks
    - **Narratives** â†’ Create comprehension or summary tasks
    - **Tables/Lists** â†’ Generate lookup or analysis questions
    - **Incomplete data** â†’ Skip or attempt reasonable completion

    ### 4. Quality Control
    - Skip records that cannot be meaningfully converted
    - Ensure all outputs are educationally valuable
    - Maintain consistency in formatting and style
    - Preserve important details from the original data

    ## Output Requirements
    - Return ONLY valid JSON format
    - Each sample must have "instruction", "input", and "output" fields
    - The "input" field can be empty string "" if not needed
    - Skip any records that cannot be converted to meaningful training data

  user: |-
    ## Task Context
    {{ task_description }}

    ## Raw Data to Convert ({{ num_records }} records)

    ```json
    {{ batch_data }}
    ```

    ## Your Task

    Convert the above raw data records into high-quality Alpaca format training samples.

    For each record:
    1. Analyze the content and structure
    2. Extract or create meaningful instruction-output pairs
    3. Ensure quality and relevance to the task context
    4. Skip if the record cannot be meaningfully converted

    Return JSON in this exact format:
    ```json
    {
      "samples": [
        {
          "instruction": "...",
          "input": "",
          "output": "..."
        },
        ...
      ]
    }
    ```

    IMPORTANT:
    - You may return fewer samples than input records if some cannot be converted
    - Each sample must be complete and valuable for training
    - Maintain consistency across all converted samples