{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 2nd Place Solution - CV 741, Public 727, Private 740\nWe (Chun Ming Lee @leecming , Udbhav Bamba @ubamba98, and Chris Deotte @cdeotte ) are excited to present our 2nd place solution to Kaggle's \"Feedback Prize - Evaluating Student Writing\" Competition. Thank you Georgia State University, The Learning Agency Lab, and Kaggle for an awesome competition.\n\nOur full solution write up is [here][1]. The main ingredients to our solution are \n* powerful post process per model\n* huge variety of NLP models trained on NER task\n* ensemble with weighted box fusion (from ZFTurbo's GitHub [here][3]). \n\n[1]: https://www.kaggle.com/c/feedback-prize-2021/discussion/313389\n[3]: https://github.com/ZFTurbo/Weighted-Boxes-Fusion","metadata":{}},{"cell_type":"markdown","source":"# Inference Script with Post Process\nThe following Python script accepts a filename of a saved model, infers the test texts, applies post process, and writes a `submission.csv` file with an extra column of confidence scores per span (i.e. the probability that this span is correct). **Click \"show hidden cell\" to see the code.**","metadata":{}},{"cell_type":"code","source":"%%writefile generate_preds.py\n\nimport os\nimport argparse\n\nap = argparse.ArgumentParser()\nap.add_argument('--model_paths', nargs='+', required=True)\nap.add_argument(\"--save_name\", type=str, required=True)\nap.add_argument(\"--max_len\", type=int, required=True)\nargs = ap.parse_args()\n\nif args.save_name == \"yoso\":\n    os.system(\"cp -r ../input/hf-transformers/transformers-4.16.0 .\")\n    os.system(\"pip install -U --no-build-isolation --no-deps /kaggle/working/transformers-4.16.0\")\n    \nif (\"v3\" in args.save_name)|(\"v2\" in args.save_name):\n    # https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizer\n    # The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n    # This must be done before importing transformers\n    import shutil\n    from pathlib import Path\n\n    transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\n    input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\n    convert_file = input_dir / \"convert_slow_tokenizer.py\"\n    conversion_path = transformers_path/convert_file.name\n\n    if conversion_path.exists():\n        conversion_path.unlink()\n\n    shutil.copy(convert_file, transformers_path)\n    deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\n    for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n        filepath = deberta_v2_path/filename\n        if filepath.exists():\n            filepath.unlink()\n\n        shutil.copy(input_dir/filename, filepath)\n\nif args.save_name == \"longformerwithlstm\":\n    os.system(\"cp -r ../input/longformerwithbilstmhead/model.py .\")\n    from model import LongformerForTokenClassificationwithbiLSTM\n    \nif args.save_name == \"debertawithlstm\":\n    os.system(\"cp -r ../input/deberta-lstm/model.py .\")\n    from model import DebertaForTokenClassificationwithbiLSTM\n        \nimport gc\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport multiprocessing as mp\nfrom scipy.special import softmax\nfrom torch.utils.data import Dataset\nfrom transformers import (AutoModelForTokenClassification, \n                          AutoTokenizer, \n                          TrainingArguments, \n                          Trainer)\n\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\nNUM_CORES = 16\nBATCH_SIZE = 4\nMAX_SEQ_LENGTH = args.max_len\nPRETRAINED_MODEL_PATHS = args.model_paths\nif \"debertal_chris\" in args.save_name:\n    print('==> using -1 in offset mapping...')\nif (\"v3\" in args.save_name)|(\"v2\" in args.save_name):\n    print('==> using -1 in offset mapping...')\n    \nAGG_FUNC = np.mean\nprint('==> using span token mean...')\n\nTEST_DIR = '../input/feedback-prize-2021/test/'\n\nMIN_TOKENS = {\n    \"Lead\": 32,\n    \"Position\": 5,\n    \"Evidence\": 35,\n    \"Claim\": 7,\n    \"Concluding Statement\": 6,\n    \"Counterclaim\": 6,\n    \"Rebuttal\": 6\n}\n\nif \"chris\" not in args.save_name:\n    ner_labels = {'O': 0,\n                  'B-Lead': 1,\n                  'I-Lead': 2,\n                  'B-Position': 3,\n                  'I-Position': 4,\n                  'B-Evidence': 5,\n                  'I-Evidence': 6,\n                  'B-Claim': 7,\n                  'I-Claim': 8,\n                  'B-Concluding Statement': 9,\n                  'I-Concluding Statement': 10,\n                  'B-Counterclaim': 11,\n                  'I-Counterclaim': 12,\n                  'B-Rebuttal': 13,\n                  'I-Rebuttal': 14}\nelse:\n    print(\"==> Using Chris BIO\")\n    ner_labels = {'O': 14,\n                  'B-Lead': 0,\n                  'I-Lead': 1,\n                  'B-Position': 2,\n                  'I-Position': 3,\n                  'B-Evidence': 4,\n                  'I-Evidence': 5,\n                  'B-Claim': 6,\n                  'I-Claim': 7,\n                  'B-Concluding Statement': 8,\n                  'I-Concluding Statement': 9,\n                  'B-Counterclaim': 10,\n                  'I-Counterclaim': 11,\n                  'B-Rebuttal': 12,\n                  'I-Rebuttal': 13}\n\n\ninverted_ner_labels = dict((v,k) for k,v in ner_labels.items())\ninverted_ner_labels[-100] = 'Special Token'\n\ntest_files = os.listdir(TEST_DIR)\n\n# accepts file path, returns tuple of (file_ID, txt split, NER labels)\ndef generate_text_for_file(input_filename):\n    curr_id = input_filename.split('.')[0]\n    with open(os.path.join(TEST_DIR, input_filename)) as f:\n        curr_txt = f.read()\n\n    return curr_id, curr_txt\n\nwith mp.Pool(NUM_CORES) as p:\n    ner_test_rows = p.map(generate_text_for_file, test_files)\n    \nif (\"v3\" in args.save_name)|(\"v2\" in args.save_name):\n    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n    tokenizer = DebertaV2TokenizerFast.from_pretrained(PRETRAINED_MODEL_PATHS[0])\nelse:\n    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_PATHS[0])\n# Check is rust-based fast tokenizer\nassert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n\nner_test_rows = sorted(ner_test_rows, key=lambda x: len(tokenizer(x[1], max_length=MAX_SEQ_LENGTH, truncation=True)['input_ids']))\n\n# tokenize and store word ids\ndef tokenize_with_word_ids(ner_raw_data):\n    # ner_raw_data is shaped (num_examples, 3) where cols are (ID, words, word-level labels)\n    tokenized_inputs = tokenizer([x[1] for x in ner_raw_data], \n                                 max_length=MAX_SEQ_LENGTH,\n                                 return_offsets_mapping=True,\n                                 truncation=True)\n    \n    tokenized_inputs['id'] = [x[0] for x in ner_raw_data]\n    tokenized_inputs['offset_mapping'] = [tokenized_inputs['offset_mapping'][i] for i in range(len(ner_raw_data))]\n    \n    return tokenized_inputs\n\ntokenized_all = tokenize_with_word_ids(ner_test_rows)\n\nclass NERDataset(Dataset):\n    def __init__(self, input_dict):\n        self.input_dict = input_dict\n        \n    def __getitem__(self, index):\n        return {k:self.input_dict[k][index] for k in self.input_dict.keys() if k not in {'id', 'offset_mapping'}}\n    \n    def get_filename(self, index):\n        return self.input_dict['id'][index]\n    \n    def get_offset(self, index):\n        return self.input_dict['offset_mapping'][index]\n    \n    def __len__(self):\n        return len(self.input_dict['input_ids'])\n\ntest_dataset = NERDataset(tokenized_all)\n\nsoft_predictions = None\nhfargs = TrainingArguments(output_dir='None',\n                         log_level='warning',\n                         per_device_eval_batch_size=BATCH_SIZE)\n\nfor idx, curr_path in enumerate(PRETRAINED_MODEL_PATHS):\n\n    if args.save_name == \"longformerwithlstm\":\n        model = LongformerForTokenClassificationwithbiLSTM.from_pretrained(curr_path)\n    elif args.save_name == \"debertawithlstm\":\n        model = DebertaForTokenClassificationwithbiLSTM.from_pretrained(curr_path)\n    else:\n        model = AutoModelForTokenClassification.from_pretrained(curr_path, trust_remote_code=True)\n    trainer = Trainer(model,\n                      hfargs,\n                      tokenizer=tokenizer)\n    \n    curr_preds, _, _ = trainer.predict(test_dataset)\n    curr_preds = curr_preds.astype(np.float16)\n    curr_preds = softmax(curr_preds, -1)\n\n    if soft_predictions is not None:\n        soft_predictions = soft_predictions + curr_preds\n    else:\n        soft_predictions = curr_preds\n        \n    del model, trainer, curr_preds\n    gc.collect()\n\nsoft_predictions = soft_predictions / len(PRETRAINED_MODEL_PATHS)\n\nsoft_claim_predictions = soft_predictions[:, :, 8]\n\npredictions = np.argmax(soft_predictions, axis=2)\nsoft_predictions = np.max(soft_predictions, axis=2)\n\ndef generate_token_to_word_mapping(txt, offset):\n    # GET WORD POSITIONS IN CHARS\n    w = []\n    blank = True\n    for i in range(len(txt)):\n        if not txt[i].isspace() and blank==True:\n            w.append(i)\n            blank=False\n        elif txt[i].isspace():\n            blank=True\n    w.append(1e6)\n\n    # MAPPING FROM TOKENS TO WORDS\n    word_map = -1 * np.ones(len(offset),dtype='int32')\n    w_i = 0\n    for i in range(len(offset)):\n        if offset[i][1]==0: continue\n        while offset[i][0]>=(w[w_i+1]-(\"debertal_chris\" in args.save_name)-(\"v3\" in args.save_name)\\\n                             -(\"v2\" in args.save_name) ): w_i += 1\n        word_map[i] = int(w_i)\n\n    return word_map\n\nall_preds = []\n\n# Clumsy gathering of predictions at word lvl - only populate with 1st subword pred\nfor curr_sample_id in range(len(test_dataset)):\n    curr_preds = []\n    sample_preds = predictions[curr_sample_id]\n    sample_offset = test_dataset.get_offset(curr_sample_id)\n    sample_txt = ner_test_rows[curr_sample_id][1]\n    sample_word_map = generate_token_to_word_mapping(sample_txt, sample_offset)\n\n    word_preds = [''] * (max(sample_word_map) + 1)\n    word_probs = dict(zip(range((max(sample_word_map) + 1)),[0]*(max(sample_word_map) + 1)))\n    claim_probs = dict(zip(range((max(sample_word_map) + 1)),[0]*(max(sample_word_map) + 1)))\n\n    for i, curr_word_id in enumerate(sample_word_map):\n        if curr_word_id != -1:\n            if word_preds[curr_word_id] == '': # only use 1st subword\n                word_preds[curr_word_id] = inverted_ner_labels[sample_preds[i]]\n                word_probs[curr_word_id] = soft_predictions[curr_sample_id, i]\n                claim_probs[curr_word_id] = soft_claim_predictions[curr_sample_id, i]\n            elif 'B-' in inverted_ner_labels[sample_preds[i]]:\n                word_preds[curr_word_id] = inverted_ner_labels[sample_preds[i]]\n                word_probs[curr_word_id] = soft_predictions[curr_sample_id, i]\n                claim_probs[curr_word_id] = soft_claim_predictions[curr_sample_id, i]\n\n    # Dict to hold Lead, Position, Concluding Statement\n    let_one_dict = dict() # K = Type, V = (Prob of start token, start, end)\n\n    # If we see tokens I-X, I-Y, I-X -> change I-Y to I-X\n    for j in range(1, len(word_preds) - 1):\n        pred_trio = [word_preds[k] for k in [j - 1, j, j + 1]]\n        splitted_trio = [x.split('-')[0] for x in pred_trio]\n        if all([x == 'I' for x in splitted_trio]) and pred_trio[0] == pred_trio[2] and pred_trio[0] != pred_trio[1]:\n            word_preds[j] = word_preds[j-1]\n\n    # B-X, ? (not B), I-X -> change ? to I-X\n    for j in range(1, len(word_preds) - 1):\n        if 'B-' in word_preds[j-1] and word_preds[j+1] == f\"I-{word_preds[j-1].split('-')[-1]}\" and word_preds[j] != word_preds[j+1] and 'B-' not in word_preds[j]:\n            word_preds[j] = word_preds[j+1]\n\n     # If we see tokens I-X, O, I-X, change center token to the same for stated discourse types\n    for j in range(1, len(word_preds) - 1):\n        if word_preds[j - 1] in ['I-Lead', 'I-Position', 'I-Concluding Statement'] and word_preds[j-1] == word_preds[j+1] and word_preds[j] == 'O':\n            word_preds[j] = word_preds[j-1]\n\n    j = 0 # start of candidate discourse\n    while j < len(word_preds): \n        cls = word_preds[j] \n        cls_splitted = cls.split('-')[-1]\n        end = j + 1 # try to extend discourse as far as possible\n\n        if word_probs[j] > 0.54: \n            # Must match suffix i.e., I- to I- only; no B- to I-\n            while end < len(word_preds) and (word_preds[end].split('-')[-1] == cls_splitted if cls_splitted in ['Lead', 'Position', 'Concluding Statement'] else word_preds[end] == f'I-{cls_splitted}'):\n                end += 1\n            # if we're here, end is not the same pred as start\n            if cls != 'O' and (end - j > MIN_TOKENS[cls_splitted] or max(word_probs[l] for l in range(j, end)) > 0.73): # needs to be longer than class-specified min\n                if cls_splitted in ['Lead', 'Position', 'Concluding Statement']:\n                    lpc_max_prob = max(word_probs[c] for c in range(j, end))\n                    if cls_splitted in let_one_dict: # Already existing, check contiguous or higher prob\n                        prev_prob, prev_start, prev_end = let_one_dict[cls_splitted]\n                        if cls_splitted in ['Lead', 'Concluding Statement'] and j - prev_end < 49: # If close enough, combine\n                            let_one_dict[cls_splitted] = (max(prev_prob, lpc_max_prob), prev_start, end)\n                            \n                            # Delete other preds that lie inside the joined LC discourse\n                            for l in range(len(curr_preds) - 1, 0, -1):\n                                check_span = curr_preds[l][2]\n                                check_start, check_end = int(check_span[0]), int(check_span[-1])\n                                if check_start > prev_start and check_end < end:\n                                    del curr_preds[l]\n                            \n                        elif lpc_max_prob > prev_prob: # Overwrite if current candidate is more likely\n                            let_one_dict[cls_splitted] = (lpc_max_prob, j, end)\n                    else: # Add to it\n                        let_one_dict[cls_splitted] = (lpc_max_prob, j, end)\n                else:\n                    # Lookback and add preceding I- tokens\n                    while j - 1 > 0 and word_preds[j-1] == cls:\n                        j = j - 1\n                    # Try to add the matching B- tag if immediately precedes the current I- sequence\n                    if j - 1 > 0 and word_preds[j-1] == f'B-{cls_splitted}':\n                        j = j - 1\n\n\n                    #############################################################\n                    # Run a bunch of adjustments to discourse predictions based on CV \n                    adj_start, adj_end = j, end + 1\n\n                    # Run some heuristics against previous discourse\n                    if len(curr_preds) > 0:\n                        prev_span = list(map(int, curr_preds[-1][2].split()))\n                        prev_start, prev_end = prev_span[0], prev_span[-1]\n\n                        # Join adjacent rebuttals\n                        if cls_splitted in 'Rebuttal':                        \n                            if curr_preds[-1][1] == cls_splitted and adj_start - prev_end < 32:\n                                del curr_preds[-1]\n                                combined_list = prev_span + list(range(adj_start, adj_end))                                \n                                curr_preds.append((test_dataset.get_filename(curr_sample_id), \n                                                   cls_splitted, \n                                                   ' '.join(map(str, combined_list)),\n                                                   AGG_FUNC([word_probs[i] for i in combined_list if i in word_probs.keys()])))\n                                j = end\n                                continue\n                                \n                        elif cls_splitted in 'Counterclaim':                        \n                            if curr_preds[-1][1] == cls_splitted and adj_start - prev_end < 24:\n                                del curr_preds[-1]\n                                combined_list = prev_span + list(range(adj_start, adj_end))                                \n                                curr_preds.append((test_dataset.get_filename(curr_sample_id), \n                                                   cls_splitted, \n                                                   ' '.join(map(str, combined_list)),\n                                                  AGG_FUNC([word_probs[i] for i in combined_list if i in word_probs.keys()])))\n                                j = end\n                                continue\n\n                        elif cls_splitted in 'Evidence':                        \n                            if curr_preds[-1][1] == cls_splitted and 8 < adj_start - prev_end < 25:\n                                if max(claim_probs[l] for l in range(prev_end+1, adj_start)) > 0.35:\n                                    claim_tokens = [str(l) for l in range(prev_end+1, adj_start) if claim_probs[l] > 0.15]\n                                    if len(claim_tokens) > 2:\n                                        curr_preds.append((test_dataset.get_filename(curr_sample_id), \n                                                           'Claim', \n                                                           ' '.join(claim_tokens),\n                                                           AGG_FUNC([word_probs[int(i)] for i in claim_tokens if int(i) in word_probs.keys()])))\n                        # If gap with discourse of same type, extend to it \n                        elif curr_preds[-1][1] == cls_splitted and adj_start - prev_end > 2:\n                            adj_start -= 1\n\n                    # Adjust discourse lengths if too long or short\n                    if cls_splitted == 'Evidence':\n                        if adj_end - adj_start < 45:\n                            adj_start -= 9\n                        else:\n                            adj_end -= 1\n                    elif cls_splitted == 'Claim':\n                        if adj_end - adj_start > 24:\n                            adj_end -= 1\n                    elif cls_splitted == 'Counterclaim':\n                        if adj_end - adj_start > 24:\n                            adj_end -= 1\n                        else:\n                            adj_start -= 1\n                            adj_end += 1\n                    elif cls_splitted == 'Rebuttal':\n                        if adj_end - adj_start > 32:\n                            adj_end -= 1\n                        else:\n                            adj_start -= 1\n                            adj_end += 1\n                    adj_start = max(0, adj_start)\n                    adj_end = min(len(word_preds) - 1, adj_end)\n                    curr_preds.append((test_dataset.get_filename(curr_sample_id), \n                                       cls_splitted, \n                                       ' '.join(map(str, list(range(adj_start, adj_end)))),\n                                       AGG_FUNC([word_probs[i] for i in range(adj_start, adj_end) if i in word_probs.keys()])))\n\n        j = end \n\n    # Add the Lead, Position, Concluding Statement\n    for k, v in let_one_dict.items():\n        pred_start = v[1]\n        pred_end = v[2]\n\n        # Lookback and add preceding I- tokens\n        while pred_start - 1 > 0 and word_preds[pred_start-1] == f'I-{k}':\n            pred_start = pred_start - 1\n        # Try to add the matching B- tag if immediately precedes the current I- sequence\n        if pred_start - 1 > 0 and word_preds[pred_start - 1] == f'B-{k}':\n            pred_start = pred_start - 1\n\n        # Extend short Leads and Concluding Statements\n        if k == 'Lead':\n            if pred_end - pred_start < 33:\n                pred_end = min(len(word_preds), pred_end + 5)\n            else:\n                pred_end -= 5\n        elif k == 'Concluding Statement':\n            if pred_end - pred_start < 23:\n                pred_start = max(0, pred_start - 1)\n                pred_end = min(len(word_preds), pred_end + 10)\n        elif k == 'Position':\n            if pred_end - pred_start < 18:\n                pred_end = min(len(word_preds), pred_end + 3)\n\n        pred_start = max(0, pred_start)\n        if pred_end - pred_start > 6:\n            curr_preds.append((test_dataset.get_filename(curr_sample_id), \n                               k, \n                               ' '.join(map(str, list(range(pred_start, pred_end)))),\n                               AGG_FUNC([word_probs[i] for i in range(pred_start, pred_end) if i in word_probs.keys()])))\n\n    all_preds.extend(curr_preds)\n\noutput_df = pd.DataFrame(all_preds)\noutput_df.columns = ['id', 'class', 'predictionstring', 'scores']\noutput_df.to_csv(f'{args.save_name}.csv', index=False)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-13T07:39:00.274578Z","iopub.execute_input":"2022-03-13T07:39:00.274891Z","iopub.status.idle":"2022-03-13T07:39:00.313298Z","shell.execute_reply.started":"2022-03-13T07:39:00.274808Z","shell.execute_reply":"2022-03-13T07:39:00.312379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Weighted Box Fusion\nThe following code comes from ZFTurbo's GitHub [here][1]. First, the text `predictionstring` are converted to 1 dimensional boxes. Next they are ensembled with ZFTurbo's WBF code. **Click \"show hidden cell\" to see the code.**\n\n[1]: https://github.com/ZFTurbo/Weighted-Boxes-Fusion","metadata":{}},{"cell_type":"code","source":"'''\nCode taken and modified for 1D sequences from:\nhttps://github.com/ZFTurbo/Weighted-Boxes-Fusion/blob/master/ensemble_boxes/ensemble_boxes_wbf.py\n'''\nimport warnings\nimport numpy as np\n\ndef prefilter_boxes(boxes, scores, labels, weights, thr):\n    # Create dict with boxes stored by its label\n    new_boxes = dict()\n\n    for t in range(len(boxes)):\n\n        if len(boxes[t]) != len(scores[t]):\n            print('Error. Length of boxes arrays not equal to length of scores array: {} != {}'.format(len(boxes[t]), len(scores[t])))\n            exit()\n\n        for j in range(len(boxes[t])):\n            score = scores[t][j]\n            if score < thr:\n                continue\n            label = labels[t][j]\n            box_part = boxes[t][j]\n\n            x = float(box_part[0])\n            y = float(box_part[1])\n\n            # Box data checks\n            if y < x:\n                warnings.warn('Y < X value in box. Swap them.')\n                x, y = y, x\n\n            # [label, score, weight, model index, x, y]\n            b = [label, float(score) * weights[t], weights[t], t, x, y]\n            if label not in new_boxes:\n                new_boxes[label] = []\n            new_boxes[label].append(b)\n\n    # Sort each list in dict by score and transform it to numpy array\n    for k in new_boxes:\n        current_boxes = np.array(new_boxes[k])\n        new_boxes[k] = current_boxes[current_boxes[:, 1].argsort()[::-1]]\n\n    return new_boxes\n\n\ndef get_weighted_box(boxes, conf_type='avg'):\n    \"\"\"\n    Create weighted box for set of boxes\n    :param boxes: set of boxes to fuse\n    :param conf_type: type of confidence one of 'avg' or 'max'\n    :return: weighted box (label, score, weight, model index, x, y)\n    \"\"\"\n\n    box = np.zeros(6, dtype=np.float32)\n    conf = 0\n    conf_list = []\n    w = 0\n    for b in boxes:\n        box[4:] += (b[1] * b[4:])\n        conf += b[1]\n        conf_list.append(b[1])\n        w += b[2]\n    box[0] = boxes[0][0]\n    if conf_type == 'avg':\n        box[1] = conf / len(boxes)\n    elif conf_type == 'max':\n        box[1] = np.array(conf_list).max()\n    elif conf_type in ['box_and_model_avg', 'absent_model_aware_avg']:\n        box[1] = conf / len(boxes)\n    box[2] = w\n    box[3] = -1 # model index field is retained for consistensy but is not used.\n    box[4:] /= conf\n    return box\n\n\ndef find_matching_box_quickly(boxes_list, new_box, match_iou):\n    \"\"\" \n        Reimplementation of find_matching_box with numpy instead of loops. Gives significant speed up for larger arrays\n        (~100x). This was previously the bottleneck since the function is called for every entry in the array.\n\n        boxes_list: shape: (N, label, score, weight, model index, x, y)\n        new_box: shape: (label, score, weight, model index, x, y)\n    \"\"\"\n    def bb_iou_array(boxes, new_box):\n        '''\n        boxes: shape: (N, x, y)\n        new_box: shape: (x, y)\n        '''\n        # bb interesection over union\n        x_min = np.minimum(boxes[:, 0], new_box[0])\n        x_max = np.maximum(boxes[:, 0], new_box[0])\n        y_min = np.minimum(boxes[:, 1], new_box[1])+1\n        y_max = np.maximum(boxes[:, 1], new_box[1])+1\n\n        iou = np.maximum(0, (y_min-x_max)/(y_max-x_min))\n\n        return iou\n\n    if boxes_list.shape[0] == 0:\n        return -1, match_iou\n\n    # boxes = np.array(boxes_list)\n    boxes = boxes_list\n\n    ious = bb_iou_array(boxes[:, 4:], new_box[4:])\n\n    ious[boxes[:, 0] != new_box[0]] = -1\n\n    best_idx = np.argmax(ious)\n    best_iou = ious[best_idx]\n\n    if best_iou <= match_iou:\n        best_iou = match_iou\n        best_idx = -1\n\n    return best_idx, best_iou\n\n\ndef weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=None, iou_thr=0.55, skip_box_thr=0.0, conf_type='avg', allows_overflow=False):\n    '''\n    :param boxes_list: list of boxes predictions from each model, each box is 2 numbers.\n     It has 3 dimensions (models_number, model_preds, 2)\n     Order of boxes: x, y.\n    :param scores_list: list of scores for each model\n    :param labels_list: list of labels for each model\n    :param weights: list of weights for each model. Default: None, which means weight == 1 for each model\n    :param iou_thr: IoU value for boxes to be a match\n    :param skip_box_thr: exclude boxes with score lower than this variable\n    :param conf_type: how to calculate confidence in weighted boxes. 'avg': average value, 'max': maximum value, 'box_and_model_avg': box and model wise hybrid weighted average, 'absent_model_aware_avg': weighted average that takes into account the absent model.\n    :param allows_overflow: false if we want confidence score not exceed 1.0\n\n    :return: boxes: boxes coordinates (Order of boxes: x, y).\n    :return: scores: confidence scores\n    :return: labels: boxes labels\n    '''\n\n    if weights is None:\n        weights = np.ones(len(boxes_list))\n    if len(weights) != len(boxes_list):\n        print('Warning: incorrect number of weights {}. Must be: {}. Set weights equal to 1.'.format(len(weights), len(boxes_list)))\n        weights = np.ones(len(boxes_list))\n    weights = np.array(weights)\n\n    if conf_type not in ['avg', 'max', 'box_and_model_avg', 'absent_model_aware_avg']:\n        print('Unknown conf_type: {}. Must be \"avg\", \"max\" or \"box_and_model_avg\", or \"absent_model_aware_avg\"'.format(conf_type))\n        exit()\n\n    filtered_boxes = prefilter_boxes(boxes_list, scores_list, labels_list, weights, skip_box_thr)\n    if len(filtered_boxes) == 0:\n        return np.zeros((0, 2)), np.zeros((0,)), np.zeros((0,))\n\n    overall_boxes = []\n    for label in filtered_boxes:\n        boxes = filtered_boxes[label]\n        new_boxes = []\n        weighted_boxes = np.empty((0,6)) ## [label, score, weight, model index, x, y]\n        # Clusterize boxes\n        for j in range(0, len(boxes)):\n            index, best_iou = find_matching_box_quickly(weighted_boxes, boxes[j], iou_thr)\n\n            if index != -1:\n                new_boxes[index].append(boxes[j])\n                weighted_boxes[index] = get_weighted_box(new_boxes[index], conf_type)\n            else:\n                new_boxes.append([boxes[j].copy()])\n                weighted_boxes = np.vstack((weighted_boxes, boxes[j].copy()))\n\n        # Rescale confidence based on number of models and boxes\n        for i in range(len(new_boxes)):\n            clustered_boxes = np.array(new_boxes[i])\n            if conf_type == 'box_and_model_avg':\n                # weighted average for boxes\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weighted_boxes[i, 2]\n                # identify unique model index by model index column\n                _, idx = np.unique(clustered_boxes[:, 3], return_index=True)\n                # rescale by unique model weights\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] *  clustered_boxes[idx, 2].sum() / weights.sum()\n            elif conf_type == 'absent_model_aware_avg':\n                # get unique model index in the cluster\n                models = np.unique(clustered_boxes[:, 3]).astype(int)\n                # create a mask to get unused model weights\n                mask = np.ones(len(weights), dtype=bool)\n                mask[models] = False\n                # absent model aware weighted average\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / (weighted_boxes[i, 2] + weights[mask].sum())\n            elif conf_type == 'max':\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] / weights.max()\n            elif not allows_overflow:\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * min(len(weights), len(clustered_boxes)) / weights.sum()\n            else:\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weights.sum()\n        \n        # REQUIRE BBOX TO BE PREDICTED BY AT LEAST 2 MODELS\n        #for i in range(len(new_boxes)):\n        #    clustered_boxes = np.array(new_boxes[i])\n        #    if len(np.unique(clustered_boxes[:, 3])) > 1:\n        #        overall_boxes.append(weighted_boxes[i])\n                \n        overall_boxes.append(weighted_boxes) # NOT NEEDED FOR \"REQUIRE TWO MODELS\" ABOVE\n    overall_boxes = np.concatenate(overall_boxes, axis=0) # NOT NEEDED FOR \"REQUIRE TWO MODELS\" ABOVE\n    #overall_boxes = np.array(overall_boxes) # NEEDED FOR \"REQUIRE TWO MODELS\" ABOVE\n    overall_boxes = overall_boxes[overall_boxes[:, 1].argsort()[::-1]]\n    boxes = overall_boxes[:, 4:]\n    scores = overall_boxes[:, 1]\n    labels = overall_boxes[:, 0]\n    return boxes, scores, labels\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-13T07:39:00.315048Z","iopub.execute_input":"2022-03-13T07:39:00.315488Z","iopub.status.idle":"2022-03-13T07:39:00.434581Z","shell.execute_reply.started":"2022-03-13T07:39:00.315448Z","shell.execute_reply":"2022-03-13T07:39:00.433364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Our NLP models trained on NER task\nHere is a summary of our models and their individual performance:\n\n| Hugging Face Model | CV | Public LB | Private LB | special |\n| --- | --- | | | |\n| microsoft/deberta-large | 706 | 710 | 721 | trained with 100% train data|\n| microsoft/deberta-large | 699 | 706 | 719 | add lstm, add jaccard loss |\n| microsoft/deberta-v3-large | 705 | |  | [convert slow tokenizer to fast][9] |\n| microsoft/deberta-xlarge | 708 | 704 | 713 | |\n| microsoft/deberta-v2-xlarge | 702 |  |  | [convert slow tokenizer to fast][9] |\n| allenai/longformer-large-4096 | 702 | 705 | 716 | add lstm head|\n| [LSG converted roberta][1] | 703 | 702 | 714 | convert 512 roberta to 1536 |\n| funnel-transformer/large | 688 | 689 | 708 |\n| google/bigbird-roberta-base | 675 | 676 | 692 | train 1024 infer 1024 |\n| uw-madison/yoso-4096 | 652 | 655 | 668 | lsh_backward=False |\n\n[1]: https://github.com/ccdv-ai/convert_checkpoint_to_lsg\n[3]: https://github.com/ZFTurbo/Weighted-Boxes-Fusion\n[9]: https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizer","metadata":{}},{"cell_type":"code","source":"!python generate_preds.py --model_paths ../input/longformerwithbilstmhead/aug-longformer-large-4096-f0/checkpoint-5500 \\\n                                        ../input/longformerwithbilstmhead/aug-longformer-large-4096-f2/checkpoint-7500 \\\n                                        ../input/longformerwithbilstmhead/aug-longformer-large-4096-f5/checkpoint-6000 \\\n                            --save_name longformerwithlstm --max_len 1536\n\n!python generate_preds.py --model_paths ../input/deberta-large-100/fold0 \\\n                                        ../input/deberta-large-100/fold1 \\\n                                        ../input/deberta-large-100/fold2 \\\n                            --save_name debertal_chris --max_len 1536\n\n!python generate_preds.py --model_paths ../input/deberta-large-v2/deberta-large-v2100-f0/checkpoint-10500 \\\n                                        ../input/deberta-large-v2/deberta-large-v2101-f1/checkpoint-11500 \\\n                                        ../input/deberta-large-v2/deberta-large-v2102-f2/checkpoint-8500 \\\n                            --save_name debertal --max_len 1536\n\n!python generate_preds.py --model_paths ../input/deberta-xlarge-1536/deberta-xlarge-v8004-f4/checkpoint-14000 \\\n                                        ../input/deberta-xlarge-1536/deberta-xlarge-v4005-f5/checkpoint-13000 \\\n                            --save_name debertaxl --max_len 1536\n\n!python generate_preds.py --model_paths ../input/deberta-v2-xlarge/deberta-v2-xlarge-v6000-f0/checkpoint-7500 \\\n                                        ../input/deberta-v2-xlarge/deberta-v2-xlarge-v6003-f3/checkpoint-9000 \\\n                            --save_name deberta_v2 --max_len 1536\n\n!python generate_preds.py --model_paths ../input/deberta-lstm-jaccard/jcl-deberta-large-f1/checkpoint-4500 \\\n                                        ../input/deberta-lstm-jaccard/jcl-deberta-large-f2/checkpoint-5000 \\\n                                        ../input/deberta-lstm-jaccard/jcl-deberta-large-f3/checkpoint-3500 \\\n                            --save_name debertawithlstm --max_len 1536\n\n!python generate_preds.py --model_paths  ../input/funnel-large-6folds/large-v628-f1/checkpoint-11500 \\\n                                         ../input/funnel-large-6folds/large-v627-f3/checkpoint-11000 \\\n                                         ../input/funnel-large-6folds/large-v623-f4/checkpoint-10500 \\\n                            --save_name funnel --max_len 1536\n\n!python generate_preds.py --model_paths  ../input/auglsgrobertalarge/lsg-roberta-large-0/checkpoint-6750 \\\n                                         ../input/auglsgrobertalarge/lsg-roberta-large-2/checkpoint-7000 \\\n                                         ../input/auglsgrobertalarge/lsg-roberta-large-5/checkpoint-6500 \\\n                             --save_name lsg --max_len 1536\n\n!python generate_preds.py --model_paths  ../input/bird-base/fold1 \\\n                                         ../input/bird-base/fold3 \\\n                                         ../input/bird-base/fold5 \\\n                            --save_name bigbird_base_chris --max_len 1024\n\n!python generate_preds.py --model_paths  ../input/feedbackyoso/yoso-4096-0/checkpoint-12500 \\\n                                         ../input/feedbackyoso/yoso-4096-2/checkpoint-11000 \\\n                                         ../input/feedbackyoso/yoso-4096-4/checkpoint-12500 \\\n                            --save_name yoso --max_len 1536","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:39:00.721366Z","iopub.execute_input":"2022-03-13T07:39:00.721767Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np, os\n\nlongformer_csv = pd.read_csv(\"longformerwithlstm.csv\").dropna()\ndeberta_v3_csv = pd.read_csv(\"debertawithlstm.csv\").dropna()\ndeberta_v2_csv = pd.read_csv(\"deberta_v2.csv\").dropna()\ndebertaxl_csv = pd.read_csv(\"debertaxl.csv\").dropna()\ndebertal_chris_csv = pd.read_csv(\"debertal_chris.csv\").dropna()\ndebertal_csv = pd.read_csv(\"debertal.csv\").dropna()\nyoso_csv = pd.read_csv(\"yoso.csv\").dropna()\nfunnel_csv = pd.read_csv(\"funnel.csv\").dropna()\nbird_base_chris_csv = pd.read_csv(\"bigbird_base_chris.csv\").dropna()\nlsg_csv = pd.read_csv(\"lsg.csv\").dropna()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T10:00:32.486594Z","iopub.execute_input":"2022-02-23T10:00:32.487659Z","iopub.status.idle":"2022-02-23T10:00:32.591336Z","shell.execute_reply.started":"2022-02-23T10:00:32.487604Z","shell.execute_reply":"2022-02-23T10:00:32.589768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble Models with WBF\nWe will now read in the 10 submission files generated above and apply WBF to ensemble them. After applying WBF, it is important to remove predictions with confidence score below threshold. This is explained [here][1]. \n\nIf only 1 model out of 10 models makes a certain span prediction, that prediction will still be present in WBF's outcome. However that prediction will have a very low confidence score because that model's confidence score will be averaged with 9 zero confidence scores. We found optimal confidence scores per class by analyzing our CV OOF score. For each class, we vary the threshold and compute the corresponding class metric score.\n\n![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Mar-2022/conf_scores.png)\n\n[1]: https://www.kaggle.com/c/tensorflow-great-barrier-reef/discussion/307609","metadata":{}},{"cell_type":"code","source":"TEST_DIR = '../input/feedback-prize-2021/test/'\ntest_files = os.listdir(TEST_DIR)\nv_ids = [f.replace('.txt','') for f in test_files]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\nclass_to_label = {\n    'Claim': 0, \n    'Evidence': 1, \n    'Lead':2, \n    'Position':3, \n    'Concluding Statement':4,\n    'Counterclaim':5, \n    'Rebuttal':6\n}\n\n# Threshold found from CV\nlabel_to_threshold = {\n    0 : 0.275, #Claim\n    1 : 0.375, #Evidence\n    2 : 0.325, #Lead\n    3 : 0.325, #Position\n    4 : 0.4, #Concluding Statement\n    5 : 0.275, #Counterclaim\n    6 : 0.275 #Rebuttal\n}\n\nlabel_to_class = {v:k for k, v in class_to_label.items()}\n\ndef preprocess_for_wbf(df_list):\n    boxes_list=[]\n    scores_list=[]\n    labels_list=[]\n    \n    for df in df_list:\n        scores_list.append(df['scores'].values.tolist())\n        labels_list.append(df['class'].map(class_to_label).values.tolist())\n        predictionstring = df.predictionstring.str.split().values\n        df_box_list = []\n        for bb in predictionstring:\n            df_box_list.append([int(bb[0]), int(bb[-1])])\n        boxes_list.append(df_box_list)\n    return boxes_list, scores_list, labels_list\n\ndef postprocess_for_wbf(idx, boxes_list, scores_list, labels_list):\n    preds = []\n    for box, score, label in zip(boxes_list, scores_list, labels_list):\n        if score > label_to_threshold[label]: \n            start = math.ceil(box[0])\n            end = int(box[1])\n            preds.append((idx, label_to_class[label], ' '.join([str(x) for x in range(start, end+1)])))\n    return preds\n\ndef generate_wbf_for_id(i):\n    df1 = debertal_csv[debertal_csv['id']==i]\n    df2 = debertal_chris_csv[debertal_chris_csv['id']==i]\n    df3 = funnel_csv[funnel_csv['id']==i]\n    df4 = debertaxl_csv[debertaxl_csv['id']==i]\n    df5 = longformer_csv[longformer_csv['id']==i]\n    df6 = deberta_v3_csv[deberta_v3_csv['id']==i]\n    df7 = yoso_csv[yoso_csv['id']==i]\n    df8 = bird_base_chris_csv[bird_base_chris_csv['id']==i]\n    df9 = lsg_csv[lsg_csv['id']==i]\n    df10 = deberta_v2_csv[deberta_v2_csv['id']==i]\n    \n    boxes_list, scores_list, labels_list = preprocess_for_wbf([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10])\n    nboxes_list, nscores_list, nlabels_list = weighted_boxes_fusion(boxes_list, scores_list, labels_list, iou_thr=0.33, conf_type='avg')\n\n    return postprocess_for_wbf(i, nboxes_list, nscores_list, nlabels_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import multiprocessing as mp\n\nwith mp.Pool(2) as p:\n    list_of_list = p.map(generate_wbf_for_id, v_ids)\n\npreds = [x for sub_list in list_of_list for x in sub_list]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame(preds)\nsub.columns = [\"id\", \"class\", \"predictionstring\"]\nsub.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize Test Predictions\nBelow we visualize the test predictions","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nfrom spacy import displacy\n\ntest_path = Path('../input/feedback-prize-2021/test')\n\ncolors = {\n            'Lead': '#8000ff',\n            'Position': '#2b7ff6',\n            'Evidence': '#2adddd',\n            'Claim': '#80ffb4',\n            'Concluding Statement': 'd4dd80',\n            'Counterclaim': '#ff8042',\n            'Rebuttal': '#ff0000',\n            'Other': '#007f00',\n         }\n\ndef get_test_text(ids):\n    with open(test_path/f'{ids}.txt', 'r') as file: data = file.read()\n    return data\n\ndef visualize(df):\n    ids = df[\"id\"].unique()\n    for i in range(len(ids)):\n        ents = []\n        example = ids[i]\n        curr_df = df[df[\"id\"]==example]\n        text = \" \".join(get_test_text(example).split())\n        splitted_text = text.split()\n        for i, row in curr_df.iterrows():\n            predictionstring = row['predictionstring']\n            predictionstring = predictionstring.split()\n            wstart = int(predictionstring[0])\n            wend = int(predictionstring[-1])\n            ents.append({\n                             'start': len(\" \".join(splitted_text[:wstart])), \n                             'end': len(\" \".join(splitted_text[:wend+1])), \n                             'label': row['class']\n                        })\n        ents = sorted(ents, key = lambda i: i['start'])\n\n        doc2 = {\n            \"text\": text,\n            \"ents\": ents,\n            \"title\": example\n        }\n\n        options = {\"ents\": ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement', 'Counterclaim', 'Rebuttal'], \"colors\": colors}\n        displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if len(sub[\"id\"].unique())==5:\n    visualize(sub)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}