{"cells":[{"metadata":{"collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"},"cell_type":"markdown","source":"![](https://lh6.googleusercontent.com/-NzN4oVW-b8g/Rmpn3smdoCI/AAAAAAAAAKM/tRi8sd_STco/s1600/) \n> www.codeproject.com/KB/recipes/619039/SIFT.JPG\n\n[Cathedral of St. John the Baptist][trnava] in Trnava, Slovakia.\n\n# Feature Extraction\nFor this competition, we will be mostly matching images based on their local features, a.k.a. interest points.\n\nA local image feature is a tiny patch in the image that's invariant to image scaling, rotation and change in illumination.  It's like the tip of a tower, or the corner of a window in the image above.  Unlike a random point on the background (sky) in the image above, the tip of the tower can be precise detected in most images of the same scene.  It is geometricly (translation, rotation, ...) and photometricly (brightness, exposure, ...) invariant.\n\nA good local feature is like the piece you start with when solving a jigsaw puzzle, except on a much smaller scale.  It's the eye of the cat or the corner of the table, not a piece on a blank wall.\n\n\nThe extracted local features [must be][lectures]:\n- *Repeatable* and *precise* so they can be extracted from different images showing the same object.\n- *Distinctive* to the image, so images with different structure will not have them.\n\n\nThere could be hundreds or thousands of such features in an image.  An image matcher algorithm could still work if some of the features are blocked by an object or badly deformed due to change in brightness or exposure.  Many local feature algorithms are highly efficient and can be used in real-time applications.\n\nDue to these requirements, most local feature detectors extract corners and blobs.\n\n## Local Feature Detection and Description\nThere is a wealth of algorithms satisfying the above requirements for feature detection (finding interest points on an image) and description (generating a vector representation for them).  They include [Harris Corner Detection][harris], [Scale Invariant Feature Transform (SIFT)][sift], [Speeded-Up Robust Features (SURF)][surf], [Features from Accelerated Segment Test (FAST)][fast], and [Binary Robust Independent Elementary Features (BRIEF)][brief].\n\nIn this tutorial, we will use [Oriented FAST and Rotated BRIEF (ORB)][orb] for feature detection and description.  This algorithm was [developed][orb_paper] and [implemented][orb] by OpenCV Labs, and it's part of their [OpenCV library][opencv_lib] for computer vision.\n\nLet's start by extracting the local features of the image shown in the banner above.  It's the [Cathedral of St. John the Baptist][trnava] in Trnava, Slovakia.\n\n[lectures]: https://pdfs.semanticscholar.org/5255/490925aa1e01ac0b9a55e93ec8c82efc07b7.pdf\n[harris]: https://docs.opencv.org/3.3.1/dc/d0d/tutorial_py_features_harris.html\n[sift]: https://docs.opencv.org/3.3.1/da/df5/tutorial_py_sift_intro.html\n[surf]: https://docs.opencv.org/3.3.1/df/dd2/tutorial_py_surf_intro.html\n[fast]: https://docs.opencv.org/3.3.1/df/d0c/tutorial_py_fast.html\n[brief]: https://docs.opencv.org/3.3.1/dc/d7d/tutorial_py_brief.html\n[orb]: https://docs.opencv.org/3.3.1/d1/d89/tutorial_py_orb.html\n[orb_paper]: http://www.willowgarage.com/sites/default/files/orb_final.pdf\n[opencv_lib]: https://docs.opencv.org/3.3.1/index.html\n[trnava]: https://en.wikipedia.org/wiki/St._John_the_Baptist_Cathedral_(Trnava)\n[uwash]: https://courses.cs.washington.edu/courses/cse455/09wi/Lects/lect6.pdf"},{"metadata":{"collapsed":true,"_uuid":"205405a88c3f9a0b97e9de83921a8018291aa2af","_cell_guid":"a5540c06-1489-4ab4-ba9c-182c76c8fe65","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"99cf36bffb799cb43f6d00a3b503c8dc4d06346f","_cell_guid":"111a8032-409b-4456-bb26-0544f35334a0","trusted":false},"cell_type":"code","source":"dataset_path = '../input/google-image-recognition-tutorial'\nimg_building = cv2.imread(os.path.join(dataset_path, 'building_1.jpg'))\nimg_building = cv2.cvtColor(img_building, cv2.COLOR_BGR2RGB)  # Convert from cv's BRG default color order to RGB\n\norb = cv2.ORB_create()  # OpenCV 3 backward incompatibility: Do not create a detector with `cv2.ORB()`.\nkey_points, description = orb.detectAndCompute(img_building, None)\nimg_building_keypoints = cv2.drawKeypoints(img_building, \n                                           key_points, \n                                           img_building, \n                                           flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS) # Draw circles.\nplt.figure(figsize=(16, 16))\nplt.title('ORB Interest Points')\nplt.imshow(img_building_keypoints); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ccfb362d8f8c88a86cdb5505aaa3b0bcd06653bc","_cell_guid":"57e8f35c-1dd5-435b-b6ea-88668f347e74"},"cell_type":"markdown","source":"The found interest points/features are circled in the image above.  As we can see, some of these points are unique to this scene/building like the points near the top of the two towers.  However, others like the ones at the top of the tree may not be distinctive.\n\n## Feature Matching\nNow let's see if we can extract the same features from a different image of the same cathedral taken from a different angle."},{"metadata":{"collapsed":true,"_uuid":"4be79a604cc5cc15f755fc8009e0dbabb33f9831","_cell_guid":"1d1facf6-2139-4c01-af8d-b9c3facffbe2","trusted":false},"cell_type":"code","source":"def image_detect_and_compute(detector, img_name):\n    \"\"\"Detect and compute interest points and their descriptors.\"\"\"\n    img = cv2.imread(os.path.join(dataset_path, img_name))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    kp, des = detector.detectAndCompute(img, None)\n    return img, kp, des\n    \n\ndef draw_image_matches(detector, img1_name, img2_name, nmatches=10):\n    \"\"\"Draw ORB feature matches of the given two images.\"\"\"\n    img1, kp1, des1 = image_detect_and_compute(detector, img1_name)\n    img2, kp2, des2 = image_detect_and_compute(detector, img2_name)\n    \n    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n    matches = bf.match(des1, des2)\n    matches = sorted(matches, key = lambda x: x.distance) # Sort matches by distance.  Best come first.\n    \n    img_matches = cv2.drawMatches(img1, kp1, img2, kp2, matches[:nmatches], img2, flags=2) # Show top 10 matches\n    plt.figure(figsize=(16, 16))\n    plt.title(type(detector))\n    plt.imshow(img_matches); plt.show()\n    \n\norb = cv2.ORB_create()\ndraw_image_matches(orb, 'building_1.jpg', 'building_2.jpg')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"322e3664bc7b38fb1cdbf637d875d82f6443d286","_cell_guid":"9af77751-f8a6-4f20-8eba-0a73e6174238","trusted":false},"cell_type":"code","source":"sift = cv2.xfeatures2d.SIFT_create()\nkp, des = sift.detectAndCompute(img_building, None)\nimg_kp = cv2.drawKeypoints(img_building, kp, img_building)\n\nplt.figure(figsize=(15, 15))\nplt.imshow(img_kp); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"d50fab273159d0955390aff97231f4a99be29578","_cell_guid":"6e6033a7-2bd7-4fd4-9c16-c1b24baba18b","trusted":false},"cell_type":"code","source":"img1, kp1, des1 = image_detect_and_compute(sift, 'building_1.jpg')\nimg2, kp2, des2 = image_detect_and_compute(sift, 'building_2.jpg')\n\nFLANN_INDEX_KDTREE = 1\nindex_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\nsearch_params = dict(checks=50)\n\nflann = cv2.FlannBasedMatcher(index_params,search_params)\nmatches = flann.knnMatch(des1, des2, k=2)\n\nmatchesMask = [[0, 0] for i in range(len(matches))]\n# ratio test as per Lowe's paper\nfor i, (m, n) in enumerate(matches):\n    if m.distance < 0.55*n.distance:\n        matchesMask[i] = [1, 0]\n\ndraw_params = dict(matchColor=(0, 255, 0),\n                   singlePointColor=(255, 0, 0),\n                   matchesMask=matchesMask,\n                   flags=0)\n\nimg3 = cv2.drawMatchesKnn(img1, kp1, img2, kp2, matches, None, **draw_params)\nplt.figure(figsize=(18, 18))\nplt.imshow(img3); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c96b52917facfc7ebbf4b6f775067a443c6f229","_cell_guid":"5af65fd5-6895-4788-ada4-a487ea318cd8"},"cell_type":"markdown","source":"More than half of the top 10 matches were correct.  In real appliations, instead of using the top `x` matches,  a match distance threshold is used to filter out spurious matches.\n\n\n## References\n- Kristen Grauman and Bastian Leibe. 2011. [Visual Object Recognition (1st ed.)][lectures]. Morgan & Claypool Publishers.\n- Rublee, Ethan & Rabaud, Vincent & Konolige, Kurt & Bradski, Gary. (2011). [ORB: an efficient alternative to SIFT or SURF][orb_paper]. Proceedings of the IEEE International Conference on Computer Vision.\n\n[lectures]: https://pdfs.semanticscholar.org/5255/490925aa1e01ac0b9a55e93ec8c82efc07b7.pdf\n[orb_paper]: http://www.willowgarage.com/sites/default/files/orb_final.pdf\n\n"}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}