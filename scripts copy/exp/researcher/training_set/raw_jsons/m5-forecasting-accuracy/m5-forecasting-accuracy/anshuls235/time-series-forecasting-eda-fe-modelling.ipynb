{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>All you need to know when tackling Time Series Data!!</h1>\n<a id='top'><img src='https://ak.picdn.net/shutterstock/videos/641287/thumb/8.jpg' width=600></a>\n<br>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of Contents</h3>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#data\" role=\"tab\" aria-controls=\"profile\">Fetch the data<span class=\"badge badge-primary badge-pill\">1</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#down\" role=\"tab\" aria-controls=\"messages\">Downcasting<span class=\"badge badge-primary badge-pill\">2</span></a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#melt\" role=\"tab\" aria-controls=\"settings\">Melting the data<span class=\"badge badge-primary badge-pill\">3</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#eda\" role=\"tab\" aria-controls=\"settings\">Exploratory Data Analysis<span class=\"badge badge-primary badge-pill\">4</span></a> \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#fe\" role=\"tab\" aria-controls=\"settings\">Feature Engineering<span class=\"badge badge-primary badge-pill\">5</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#pred\" role=\"tab\" aria-controls=\"settings\">Modelling and Prediction<span class=\"badge badge-primary badge-pill\">6</span></a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport plotly_express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom lightgbm import LGBMRegressor\nimport joblib","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='data'>1. Fetch the data</a>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"sales = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')\nsales.name = 'sales'\ncalendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\ncalendar.name = 'calendar'\nprices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\nprices.name = 'prices'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since, the validation data is now available for the days 1914-1941, Adding zero sales for days: d_1942 - d_1969(Test)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add zero sales for the remaining days 1942-1969\nfor d in range(1942,1970):\n    col = 'd_' + str(d)\n    sales[col] = 0\n    sales[col] = sales[col].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='down'>2. Downcasting</a>\nIn this section I'll be downcasting the dataframes to reduce the amount of storage used by them and also to expidite the operations performed on them.\n\n<img src='https://media.giphy.com/media/VGP4koIBvIWDP2umDU/giphy.gif' style=\"width:500px;height:300px;\">\n\n- **Numerical Columns: ** Depending on your environment, pandas automatically creates int32, int64, float32 or float64 columns for numeric ones. If you know the min or max value of a column, you can use a subtype which is less memory consuming. You can also use an unsigned subtype if there is no negative value.<br>\nHere are the different subtypes you can use:<br>\n`int8 / uint8` : consumes 1 byte of memory, range between -128/127 or 0/255<br>\n`bool` : consumes 1 byte, true or false<br>\n`float16 / int16 / uint16`: consumes 2 bytes of memory, range between -32768 and 32767 or 0/65535<br>\n`float32 / int32 / uint32` : consumes 4 bytes of memory, range between -2147483648 and 2147483647<br>\n`float64 / int64 / uint64`: consumes 8 bytes of memory<br>\nIf one of your column has values between 1 and 10 for example, you will reduce the size of that column from 8 bytes per row to 1 byte, which is more than 85% memory saving on that column!\n- **Categorical Columns: ** Pandas stores categorical columns as objects. One of the reason this storage is not optimal is that it creates a list of pointers to the memory address of each value of your column. For columns with low cardinality (the amount of unique values is lower than 50% of the count of these values), this can be optimized by forcing pandas to use a virtual mapping table where all unique values are mapped via an integer instead of a pointer. This is done using the category datatype."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sales_bd = np.round(sales.memory_usage().sum()/(1024*1024),1)\ncalendar_bd = np.round(calendar.memory_usage().sum()/(1024*1024),1)\nprices_bd = np.round(prices.memory_usage().sum()/(1024*1024),1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Downcast in order to save memory\ndef downcast(df):\n    cols = df.dtypes.index.tolist()\n    types = df.dtypes.values.tolist()\n    for i,t in enumerate(types):\n        if 'int' in str(t):\n            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n                df[cols[i]] = df[cols[i]].astype(np.int8)\n            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n                df[cols[i]] = df[cols[i]].astype(np.int16)\n            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n                df[cols[i]] = df[cols[i]].astype(np.int32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.int64)\n        elif 'float' in str(t):\n            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n                df[cols[i]] = df[cols[i]].astype(np.float16)\n            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n                df[cols[i]] = df[cols[i]].astype(np.float32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.float64)\n        elif t == np.object:\n            if cols[i] == 'date':\n                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n            else:\n                df[cols[i]] = df[cols[i]].astype('category')\n    return df  \n\nsales = downcast(sales)\nprices = downcast(prices)\ncalendar = downcast(calendar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sales_ad = np.round(sales.memory_usage().sum()/(1024*1024),1)\ncalendar_ad = np.round(calendar.memory_usage().sum()/(1024*1024),1)\nprices_ad = np.round(prices.memory_usage().sum()/(1024*1024),1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below plot shows how much effect downcasting has had on the memory usage of DataFrames. Clearly, we have been able to reduce `sales` & `prices` to less than 1/4th of their actual memory usage. `calendar` is already a small dataframe."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dic = {'DataFrame':['sales','calendar','prices'],\n       'Before downcasting':[sales_bd,calendar_bd,prices_bd],\n       'After downcasting':[sales_ad,calendar_ad,prices_ad]}\n\nmemory = pd.DataFrame(dic)\nmemory = pd.melt(memory, id_vars='DataFrame', var_name='Status', value_name='Memory (MB)')\nmemory.sort_values('Memory (MB)',inplace=True)\nfig = px.bar(memory, x='DataFrame', y='Memory (MB)', color='Status', barmode='group', text='Memory (MB)')\nfig.update_traces(texttemplate='%{text} MB', textposition='outside')\nfig.update_layout(template='seaborn', title='Effect of Downcasting')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='melt'>3. Melting the data</a>\nCurrently, the data is in three dataframes: `sales`, `prices` & `calendar`. The `sales` dataframe contains daily sales data with days(d_1 - d_1969) as columns. The `prices` dataframe contains items' price details and `calendar` contains data about the days d.  \n<img src='https://media.giphy.com/media/Qbt9STC0qpSuc/giphy.gif' style=\"width:500px;height:300px;\">\n<h2 style=\"background-color:DodgerBlue; color:white\" >3.1 Convert from wide to long format</h2>\nHere's an example of conversion of a wide dataframe to a long dataframe.\n<img src='https://pandas.pydata.org/pandas-docs/version/0.25.0/_images/reshaping_melt.png' style=\"width:600px;height:300px;\">"},{"metadata":{},"cell_type":"markdown","source":"In this case what the melt function is doing is that it is converting the sales dataframe which is in wide format to a long format. I have kept the id variables as `id`, `item_id`, `dept_id`, `cat_id`, `store_id` and `state_id`. They have in total 30490 unique values when compunded together. Now the total number of days for which we have the data is 1969 days. Therefore the melted dataframe will be having 30490x1969 i.e. 60034810 rows"},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"df = pd.melt(sales, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='sold').dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:DodgerBlue; color:white\" >3.2 Combine the data</h2>\nCombine price data from prices dataframe and days data from calendar dataset."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"df = pd.merge(df, calendar, on='d', how='left')\ndf = pd.merge(df, prices, on=['store_id','item_id','wm_yr_wk'], how='left') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Now I have a single and a complete dataframe with all the data required. Let's Explore!</h3>"},{"metadata":{},"cell_type":"markdown","source":"# <a id='eda'>4. Exploratory Data Analysis</a>\n<h2 style=\"background-color:DodgerBlue; color:white\" >4.1 The Dataset</h2>\nThe M5 dataset, generously made available by Walmart, involves the unit sales of various products sold in the USA, organized in the form of grouped time series. More specifically, the dataset involves the unit sales of 3,049 products, classified in 3 product categories (Hobbies, Foods, and Household) and 7 product departments, in which the above-mentioned categories are disaggregated.  The products are sold across ten stores, located in three States (CA, TX, and WI).\n<img src='https://i.imgur.com/Ae5QBi9.png' style=\"width:500px;height:300px;\">\nI have drawn an interactive visualization showing the distribution of 3049 items across different aggregation levels."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"group = sales.groupby(['state_id','store_id','cat_id','dept_id'],as_index=False)['item_id'].count().dropna()\ngroup['USA'] = 'United States of America'\ngroup.rename(columns={'state_id':'State','store_id':'Store','cat_id':'Category','dept_id':'Department','item_id':'Count'},inplace=True)\nfig = px.treemap(group, path=['USA', 'State', 'Store', 'Category', 'Department'], values='Count',\n                  color='Count',\n                  color_continuous_scale= px.colors.sequential.Sunset,\n                  title='Walmart: Distribution of items')\nfig.update_layout(template='seaborn')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:DodgerBlue; color:white\" >4.2 Item Prices</h2>\nHere I'll be studying about the item prices and their distribution. Please note the prices vary weekly. So to study the distribution of prices I have taken their average."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"group_price_store = df.groupby(['state_id','store_id','item_id'],as_index=False)['sell_price'].mean().dropna()\nfig = px.violin(group_price_store, x='store_id', color='state_id', y='sell_price',box=True, hover_name='item_id')\nfig.update_xaxes(title_text='Store')\nfig.update_yaxes(title_text='Selling Price($)')\nfig.update_layout(template='seaborn',title='Distribution of Items prices wrt Stores',legend_title_text='State')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below are some of the observations from the above plot:-\n<ul style=\"list-style-type:circle;\">\n  <li>The distribution of item prices is almost uniform for all the stores across Califoria, Texas and Wisconsin.</li>\n  <li>Item <b>HOBBIES_1_361</b> priced at around 30.5 dollars is the costliest item being sold at walmarts across California.</li>\n  <li>Item <b>HOUSEHOLD_1_060</b> priced at around 29.875 dollars is the costliest item being sold at walmarts across Texas.</li>\n  <li>Item <b>HOBBIES_1_361</b> priced at around 30.48 dollars is the costliest item being sold at TX_1 and TX_3 in Texas. While item <b>HOBBIES_1_255</b> priced at around 30.5 dollars is the costliest at TX_2</li>\n</ul>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"group_price_cat = df.groupby(['store_id','cat_id','item_id'],as_index=False)['sell_price'].mean().dropna()\nfig = px.violin(group_price_cat, x='store_id', color='cat_id', y='sell_price',box=True, hover_name='item_id')\nfig.update_xaxes(title_text='Store')\nfig.update_yaxes(title_text='Selling Price($)')\nfig.update_layout(template='seaborn',title='Distribution of Items prices wrt Stores across Categories',\n                 legend_title_text='Category')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen from the plot above, food category items are quite cheap as compared with hobbies and household items. Hobbies and household items have almost the same price range."},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:DodgerBlue; color:white\" >4.3 Items Sold</h2>\nLet's study the sales accross all the stores."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"group = df.groupby(['year','date','state_id','store_id'], as_index=False)['sold'].sum().dropna()\nfig = px.violin(group, x='store_id', color='state_id', y='sold',box=True)\nfig.update_xaxes(title_text='Store')\nfig.update_yaxes(title_text='Total items sold')\nfig.update_layout(template='seaborn',title='Distribution of Items sold wrt Stores',legend_title_text='State')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below are some of the observations from the above plot:-\n<ul style=\"list-style-type:circle;\">\n  <li><b><u>California</u></b>: <b>CA_3</b> has sold the most number of items while, <b>CA_4</b> has sold the least number of items.</li>\n  <li><b><u>Texas</u></b>: <b>TX_2</b> and **TX_3** have sold the maximum number of items. <b>TX_1</b> has sold the least number of items.</li>\n  <li><b><u>Wisconsin</u></b>: <b>WI_2</b> has sold the maximum number of items while, <b>WI_3</b> has sold the least number of items.</li>\n  <li><b><u>USA</u></b>: <b>CA_3</b> has sold the most number of items while, <b>CA_4</b> has sold the least number of items.</li>\n</ul>\n\n**Let's study number of items sold over time across all the stores.**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = go.Figure()\ntitle = 'Items sold over time'\nyears = group.year.unique().tolist()\nbuttons = []\ny=3\nfor state in group.state_id.unique().tolist():\n    group_state = group[group['state_id']==state]\n    for store in group_state.store_id.unique().tolist():\n        group_state_store = group_state[group_state['store_id']==store]\n        fig.add_trace(go.Scatter(name=store, x=group_state_store['date'], y=group_state_store['sold'], showlegend=True, \n                                   yaxis='y'+str(y) if y!=1 else 'y'))\n    y-=1\n\nfig.update_layout(\n        xaxis=dict(\n        #autorange=True,\n        range = ['2011-01-29','2016-05-22'],\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1,\n                     label=\"1m\",\n                     step=\"month\",\n                     stepmode=\"backward\"),\n                dict(count=6,\n                     label=\"6m\",\n                     step=\"month\",\n                     stepmode=\"backward\"),\n                dict(count=1,\n                     label=\"YTD\",\n                     step=\"year\",\n                     stepmode=\"todate\"),\n                dict(count=1,\n                     label=\"1y\",\n                     step=\"year\",\n                     stepmode=\"backward\"),\n                dict(count=2,\n                     label=\"2y\",\n                     step=\"year\",\n                     stepmode=\"backward\"),\n                dict(count=3,\n                     label=\"3y\",\n                     step=\"year\",\n                     stepmode=\"backward\"),\n                dict(count=4,\n                     label=\"4y\",\n                     step=\"year\",\n                     stepmode=\"backward\"),\n                dict(step=\"all\")\n            ])\n        ),\n        rangeslider=dict(\n            autorange=True,\n        ),\n        type=\"date\"\n    ),\n    yaxis=dict(\n        anchor=\"x\",\n        autorange=True,\n        domain=[0, 0.33],\n        mirror=True,\n        showline=True,\n        side=\"left\",\n        tickfont={\"size\":10},\n        tickmode=\"auto\",\n        ticks=\"\",\n        title='WI',\n        titlefont={\"size\":20},\n        type=\"linear\",\n        zeroline=False\n    ),\n    yaxis2=dict(\n        anchor=\"x\",\n        autorange=True,\n        domain=[0.33, 0.66],\n        mirror=True,\n        showline=True,\n        side=\"left\",\n        tickfont={\"size\":10},\n        tickmode=\"auto\",\n        ticks=\"\",\n        title = 'TX',\n        titlefont={\"size\":20},\n        type=\"linear\",\n        zeroline=False\n    ),\n    yaxis3=dict(\n        anchor=\"x\",\n        autorange=True,\n        domain=[0.66, 1],\n        mirror=True,\n        showline=True,\n        side=\"left\",\n        tickfont={\"size\":10},\n        tickmode=\"auto\",\n        ticks='',\n        title=\"CA\",\n        titlefont={\"size\":20},\n        type=\"linear\",\n        zeroline=False\n    )\n    )\nfig.update_layout(template='seaborn', title=title)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:DodgerBlue; color:white\" >4.4 State wise Analysis</h2>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#cal\" role=\"tab\" aria-controls=\"profile\">California<span class=\"badge badge-primary badge-pill\">1</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#tex\" role=\"tab\" aria-controls=\"messages\">Texas<span class=\"badge badge-primary badge-pill\">2</span></a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#wis\" role=\"tab\" aria-controls=\"settings\">Wisconsin<span class=\"badge badge-primary badge-pill\">3</span></a>\n<br>\nIn this section, I will be studying the sales and revenue of all the stores individually across all the three states: California, Texas & Wisconsin. I have plotted total three plots for each store: CA_1, CA_2, CA_3, CA_4, TX_1, TX_2, TX_3, WI_1, WI_2 & WI_3. Details about the plots are as follows:-\n- First plot shows the daily sales of a store. I have plotted the values separately for SNAP days. Also, SNAP promotes food purchase, I have plotted food sales as well to check if it really affects the food sales.\n- Second plot shows the daily revenue of a store with separate plotting for <a href='#SNAP'>SNAP</a> days.\n- Third is a heatmap to show daily sales. It's plotted in such a way that it becomes easier to see day wise values.\n\n<div class=\"alert alert-info\" role=\"alert\">\n<a id='SNAP'><b>What is SNAP?</b></a><br>\nThe United States federal government provides a nutrition assistance benefit called the Supplement Nutrition Assistance Program (SNAP).  SNAP provides low income families and individuals with an Electronic Benefits Transfer debit card to purchase food products.  In many states, the monetary benefits are dispersed to people across 10 days of the month and on each of these days 1/10 of the people will receive the benefit on their card.  More information about the SNAP program can be found [here.](https://www.fns.usda.gov/snap/supplemental-nutrition-assistance-program)\n</div>\n\n<div class=\"alert alert-danger\" role=\"alert\">\nFor the heatmaps, the data is till 16th week of 2016 and datetime.weekofyear of function is returning 1,2 & 3 january of 2016 in 53rd week. Plotly's heatmap is connecting the data gap between the 16th and 53rd week. Still figuring out on how to remove this gap.\n</div>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df['revenue'] = df['sold']*df['sell_price'].astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def introduce_nulls(df):\n    idx = pd.date_range(df.date.dt.date.min(), df.date.dt.date.max())\n    df = df.set_index('date')\n    df = df.reindex(idx)\n    df.reset_index(inplace=True)\n    df.rename(columns={'index':'date'},inplace=True)\n    return df\n\ndef plot_metric(df,state,store,metric):\n    store_sales = df[(df['state_id']==state)&(df['store_id']==store)&(df['date']<='2016-05-22')]\n    food_sales = store_sales[store_sales['cat_id']=='FOODS']\n    store_sales = store_sales.groupby(['date','snap_'+state],as_index=False)['sold','revenue'].sum()\n    snap_sales = store_sales[store_sales['snap_'+state]==1]\n    non_snap_sales = store_sales[store_sales['snap_'+state]==0]\n    food_sales = food_sales.groupby(['date','snap_'+state],as_index=False)['sold','revenue'].sum()\n    snap_foods = food_sales[food_sales['snap_'+state]==1]\n    non_snap_foods = food_sales[food_sales['snap_'+state]==0]\n    non_snap_sales = introduce_nulls(non_snap_sales)\n    snap_sales = introduce_nulls(snap_sales)\n    non_snap_foods = introduce_nulls(non_snap_foods)\n    snap_foods = introduce_nulls(snap_foods)\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=non_snap_sales['date'],y=non_snap_sales[metric],\n                           name='Total '+metric+'(Non-SNAP)'))\n    fig.add_trace(go.Scatter(x=snap_sales['date'],y=snap_sales[metric],\n                           name='Total '+metric+'(SNAP)'))\n    fig.add_trace(go.Scatter(x=non_snap_foods['date'],y=non_snap_foods[metric],\n                           name='Food '+metric+'(Non-SNAP)'))\n    fig.add_trace(go.Scatter(x=snap_foods['date'],y=snap_foods[metric],\n                           name='Food '+metric+'(SNAP)'))\n    fig.update_yaxes(title_text='Total items sold' if metric=='sold' else 'Total revenue($)')\n    fig.update_layout(template='seaborn',title=store)\n    fig.update_layout(\n        xaxis=dict(\n        #autorange=True,\n        range = ['2011-01-29','2016-05-22'],\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1,\n                     label=\"1m\",\n                     step=\"month\",\n                     stepmode=\"backward\"),\n                dict(count=6,\n                     label=\"6m\",\n                     step=\"month\",\n                     stepmode=\"backward\"),\n                dict(count=1,\n                     label=\"YTD\",\n                     step=\"year\",\n                     stepmode=\"todate\"),\n                dict(count=1,\n                     label=\"1y\",\n                     step=\"year\",\n                     stepmode=\"backward\"),\n                dict(count=2,\n                     label=\"2y\",\n                     step=\"year\",\n                     stepmode=\"backward\"),\n                dict(count=3,\n                     label=\"3y\",\n                     step=\"year\",\n                     stepmode=\"backward\"),\n                dict(count=4,\n                     label=\"4y\",\n                     step=\"year\",\n                     stepmode=\"backward\"),\n                dict(step=\"all\")\n            ])\n        ),\n        rangeslider=dict(\n            autorange=True,\n        ),\n        type=\"date\"\n    ))\n    return fig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cal_data = group.copy()\ncal_data = cal_data[cal_data.date <= '22-05-2016']\ncal_data['week'] = cal_data.date.dt.weekofyear\ncal_data['day_name'] = cal_data.date.dt.day_name()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def calmap(cal_data, state, store, scale):\n    cal_data = cal_data[(cal_data['state_id']==state)&(cal_data['store_id']==store)]\n    years = cal_data.year.unique().tolist()\n    fig = make_subplots(rows=len(years),cols=1,shared_xaxes=True,vertical_spacing=0.005)\n    r=1\n    for year in years:\n        data = cal_data[cal_data['year']==year]\n        data = introduce_nulls(data)\n        fig.add_trace(go.Heatmap(\n            z=data.sold,\n            x=data.week,\n            y=data.day_name,\n            hovertext=data.date.dt.date,\n            coloraxis = \"coloraxis\",name=year,\n        ),r,1)\n        fig.update_yaxes(title_text=year,tickfont=dict(size=5),row = r,col = 1)\n        r+=1\n    fig.update_xaxes(range=[1,53],tickfont=dict(size=10), nticks=53)\n    fig.update_layout(coloraxis = {'colorscale':scale})\n    fig.update_layout(template='seaborn', title=store)\n    return fig","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"cal\" class=\"btn btn-primary btn-lg btn-block active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">California</a>\n<img src='https://www.pixel4k.com/wp-content/uploads/2018/09/san-francisco-california-cityscape-4k_1538070292.jpg'\nstyle=\"width:800px;height:300px;\">\n\n<a id='csn'><span class=\"label label-info\">Store Navigator</span></a>\n<nav aria-label=\"Store Navigator\">\n  <ul class=\"pagination\">\n    <li class=\"page-item\"><a class=\"page-link\" href='#C1'>CA_1</a></li>\n    <li class=\"page-item\"><a class=\"page-link\" href=\"#C2\">CA_2</a></li>\n    <li class=\"page-item\"><a class=\"page-link\" href=\"#C3\">CA_3</a></li>\n    <li class=\"page-item\"><a class=\"page-link\" href=\"#C4\">CA_4</a></li>\n  </ul>\n</nav>"},{"metadata":{},"cell_type":"markdown","source":"### <a id='C1'>CA_1</a>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = plot_metric(df,'CA','CA_1','sold')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plot_metric(df,'CA','CA_1','revenue')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = calmap(cal_data, 'CA', 'CA_1', 'magma')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='C2'>CA_2</a>\n<a href='#csn'><span class=\"label label-info\">Go to the Store Navigator</span></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plot_metric(df,'CA','CA_2','sold')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plot_metric(df,'CA','CA_2','revenue')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = calmap(cal_data, 'CA', 'CA_2', 'magma')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='C3'>CA_3</a>\n<a href='#csn'><span class=\"label label-info\">Go to the Store Navigator</span></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plot_metric(df,'CA','CA_3','sold')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plot_metric(df,'CA','CA_3','revenue')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = calmap(cal_data, 'CA', 'CA_3', 'magma')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='C4'>CA_4</a>\n<a href='#csn'><span class=\"label label-info\">Go to the Store Navigator</span></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plot_metric(df,'CA','CA_4','sold')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plot_metric(df,'CA','CA_4','revenue')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = calmap(cal_data, 'CA', 'CA_4', 'magma')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"tex\" class=\"btn btn-primary btn-lg btn-block active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Texas</a>\n<img src='https://wallpaperaccess.com/full/227248.jpg'\nstyle=\"width:800px;height:300px;\">\n<a id='wsn'><span class=\"label label-info\">Store Navigator</span></a>\n<a id='tsn'><nav aria-label=\"Store Navigator\"></a>\n  <ul class=\"pagination\">\n    <li class=\"page-item\"><a class=\"page-link\" href='#T1'>TX_1</a></li>\n    <li class=\"page-item\"><a class=\"page-link\" href=\"#T2\">TX_2</a></li>\n    <li class=\"page-item\"><a class=\"page-link\" href=\"#T3\">TX_3</a></li>\n  </ul>\n</nav>"},{"metadata":{},"cell_type":"markdown","source":"### <a id='T1'>TX_1</a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plot_metric(df,'TX','TX_1','sold')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plot_metric(df,'TX','TX_1','revenue')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = calmap(cal_data, 'TX', 'TX_1', 'viridis')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='T2'>TX_2</a>\n<a href='#tsn'><span class=\"label label-info\">Go to the Store Navigator</span></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plot_metric(df,'TX','TX_2','sold')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plot_metric(df,'TX','TX_2','revenue')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = calmap(cal_data, 'TX', 'TX_2', 'viridis')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='T3'>TX_3</a>\n<a href='#wsn'><span class=\"label label-info\">Go to the Store Navigator</span></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plot_metric(df,'TX','TX_3','sold')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plot_metric(df,'TX','TX_3','revenue')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = calmap(cal_data, 'TX', 'TX_3', 'viridis')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"wis\" class=\"btn btn-primary btn-lg btn-block active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Wisconsin</a>\n<img src='https://i.ytimg.com/vi/RzETB_wVAKI/maxresdefault.jpg'\nstyle=\"width:800px;height:300px;\">\n<span class=\"label label-info\">Store Navigator</span>\n<nav aria-label=\"Store Navigator\">\n  <ul class=\"pagination\">\n    <li class=\"page-item\"><a class=\"page-link\" href='#W1'>WI_1</a></li>\n    <li class=\"page-item\"><a class=\"page-link\" href=\"#W2\">WI_2</a></li>\n    <li class=\"page-item\"><a class=\"page-link\" href=\"#W3\">WI_3</a></li>\n  </ul>\n</nav>"},{"metadata":{},"cell_type":"markdown","source":"### <a id='W1'>WI_1</a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plot_metric(df,'WI','WI_1','sold')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plot_metric(df,'WI','WI_1','revenue')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = calmap(cal_data, 'WI', 'WI_1', 'twilight')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='W2'>WI_2</a>\n<a href='#wsn'><span class=\"label label-info\">Go to the Store Navigator</span></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plot_metric(df,'WI','WI_2','sold')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plot_metric(df,'WI','WI_2','revenue')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = calmap(cal_data, 'WI', 'WI_2', 'twilight')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='W3'>WI_3</a>\n<a href='#tsn'><span class=\"label label-info\">Go to the Store Navigator</span></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plot_metric(df,'WI','WI_3','sold')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plot_metric(df,'WI','WI_3','revenue')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = calmap(cal_data, 'WI', 'WI_3', 'twilight')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='fe'>5. Feature Engineering</a>\nTime Series data must be re-framed as a supervised learning dataset before we can start using machine learning algorithms.\n\nThere is no concept of input and output features in time series. Instead, we must choose the variable to be predicted and use feature engineering to construct all of the inputs that will be used to make predictions for future time steps.\n\n<img src='https://media.giphy.com/media/yv1ggi3Cbase05a8iS/giphy.gif' style=\"width:500px;height:300px;\">\n\nThe goal of feature engineering is to provide strong and ideally simple relationships between new input features and the output feature for the supervised learning algorithm to model.\n\n<a id='topics'><span class=\"label label-info\">Topics</span></a>\n<nav aria-label=\"Store Navigator\">\n  <ul class=\"pagination\">\n    <li class=\"page-item\"><a class=\"page-link\" href='#F1'>Label Encoding</a></li>\n    <li class=\"page-item\"><a class=\"page-link\" href=\"#F2\">Introduce Lags</a></li>\n    <li class=\"page-item\"><a class=\"page-link\" href=\"#F3\">Mean Encoding</a></li>\n    <li class=\"page-item\"><a class=\"page-link\" href=\"#F4\">Rolling Window Stats</a></li>\n    <li class=\"page-item\"><a class=\"page-link\" href=\"#F5\">Expanding Window Stats</a></li>\n    <li class=\"page-item\"><a class=\"page-link\" href=\"#F6\">Trends</a></li>\n    <li class=\"page-item\"><a class=\"page-link\" href=\"#F7\">Save the data</a></li>\n  </ul>\n</nav>"},{"metadata":{},"cell_type":"markdown","source":"<a id = 'F1'><h2>5.1 Label Encoding</h2></a>\n<ol>\n  <li>Remove unwanted data to create space in RAM for further processing.</li>\n  <li>Label Encode categorical features.(I had converted already converted categorical variable to category type. So, I can simply use their codes instead of using LableEncoder)</li>\n  <li>Remove date as its features are already present.</li>\n</ol>"},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\">\n<b>Please note:</b><br>\nI'm storing the categories correponding to their respective category codes so that I'can use them later on while making the submission.\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Store the categories along with their codes\nd_id = dict(zip(df.id.cat.codes, df.id))\nd_item_id = dict(zip(df.item_id.cat.codes, df.item_id))\nd_dept_id = dict(zip(df.dept_id.cat.codes, df.dept_id))\nd_cat_id = dict(zip(df.cat_id.cat.codes, df.cat_id))\nd_store_id = dict(zip(df.store_id.cat.codes, df.store_id))\nd_state_id = dict(zip(df.state_id.cat.codes, df.state_id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#1\ndel group, group_price_cat, group_price_store, group_state, group_state_store, cal_data\ngc.collect();\n\n#2\ndf.d = df['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)\ncols = df.dtypes.index.tolist()\ntypes = df.dtypes.values.tolist()\nfor i,type in enumerate(types):\n    if type.name == 'category':\n        df[cols[i]] = df[cols[i]].cat.codes\n        \n#3\ndf.drop('date',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'F2'><h2>5.2 Introduce Lags</h2></a>\n<a href='#topics'><span class=\"label label-info\">Go back to topics</span></a>\n\nLag features are the classical way that time series forecasting problems are transformed into supervised learning problems.\n\nIntroduce lags to the the target variable `sold`. The maximum lag I have introduced is 36 days. It's purely upto you how many lags you want to introduce."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Introduce lags\nlags = [1,2,3,6,12,24,36]\nfor lag in lags:\n    df['sold_lag_'+str(lag)] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['sold'].shift(lag).astype(np.float16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'F3'><h2>5.3 Mean Encoding</h2></a>\n<a href='#topics'><span class=\"label label-info\">Go back to topics</span></a>\n\nFrom a mathematical point of view, mean encoding represents a probability of your target variable, conditional on each value of the feature. In a way, it embodies the target variable in its encoded value. I have calculated mean encodings on the basis of following logical features I could think of:-\n- item\n- state\n- store\n- category\n- department\n- category & department\n- store & item\n- category & item\n- department & item\n- state & store\n- state, store and category\n- store, category and department\n\n<h2>Feel free to add more!</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['iteam_sold_avg'] = df.groupby('item_id')['sold'].transform('mean').astype(np.float16)\ndf['state_sold_avg'] = df.groupby('state_id')['sold'].transform('mean').astype(np.float16)\ndf['store_sold_avg'] = df.groupby('store_id')['sold'].transform('mean').astype(np.float16)\ndf['cat_sold_avg'] = df.groupby('cat_id')['sold'].transform('mean').astype(np.float16)\ndf['dept_sold_avg'] = df.groupby('dept_id')['sold'].transform('mean').astype(np.float16)\ndf['cat_dept_sold_avg'] = df.groupby(['cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)\ndf['store_item_sold_avg'] = df.groupby(['store_id','item_id'])['sold'].transform('mean').astype(np.float16)\ndf['cat_item_sold_avg'] = df.groupby(['cat_id','item_id'])['sold'].transform('mean').astype(np.float16)\ndf['dept_item_sold_avg'] = df.groupby(['dept_id','item_id'])['sold'].transform('mean').astype(np.float16)\ndf['state_store_sold_avg'] = df.groupby(['state_id','store_id'])['sold'].transform('mean').astype(np.float16)\ndf['state_store_cat_sold_avg'] = df.groupby(['state_id','store_id','cat_id'])['sold'].transform('mean').astype(np.float16)\ndf['store_cat_dept_sold_avg'] = df.groupby(['store_id','cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'F4'><h2>5.4 Rolling Window Statistics</h2></a>\n<a href='#topics'><span class=\"label label-info\">Go back to topics</span></a>\n\nHere’s an awesome gif that explains this idea in a wonderfully intuitive way:\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/11/3hotmk.gif)\n`This method is called the rolling window method because the window would be different for every data point.`\n\nI'll be calculating weekly rolling avearge of the items sold. More features like rolling min, max or sum can also be calculated. Also, same features can be calculated for revenue as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['rolling_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.rolling(window=7).mean()).astype(np.float16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'F5'><h2>5.5 Expanding Window Statistics</h2></a>\n<a href='#topics'><span class=\"label label-info\">Go back to topics</span></a>\n\nThis is simply an advanced version of the rolling window technique. In the case of a rolling window, the size of the window is constant while the window slides as we move forward in time. Hence, we consider only the most recent values and ignore the past values. Here’s a gif that explains how our expanding window function works:\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/output_B4KHcT.gif)\n\nI'll be calculating expanding avearge of the items sold. More features like expanding min, max or sum can also be calculated. Also, same features can be calculated for revenue as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['expanding_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.expanding(2).mean()).astype(np.float16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'F6'><h2>5.6 Trends</h2></a>\n<a href='#topics'><span class=\"label label-info\">Go back to topics</span></a>\n\nI will be creating a selling trend feature, which will be some positive value if the daily items sold are greater than the entire duration average **( d_1 - d_1969 )** else negative. More trend features can be added but I'll only add this one to keep it simple."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['daily_avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','d'])['sold'].transform('mean').astype(np.float16)\ndf['avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform('mean').astype(np.float16)\ndf['selling_trend'] = (df['daily_avg_sold'] - df['avg_sold']).astype(np.float16)\ndf.drop(['daily_avg_sold','avg_sold'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'F6'><h2>5.7 Save the data</h2></a>\n<a href='#topics'><span class=\"label label-info\">Go back to topics</span></a>\n\nNow since all the new features are created, let's save the data so that it can be trained separately.Also, lags introduce a lot of Null values, so I'll remove data for first 35 days as I have introduced lags till 36 days."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df['d']>=36]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at our new dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save the data for training."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_pickle('data.pkl')\ndel df\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='pred'>6. Modelling and Prediction</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_pickle('data.pkl')\nvalid = data[(data['d']>=1914) & (data['d']<1942)][['id','d','sold']]\ntest = data[data['d']>=1942][['id','d','sold']]\neval_preds = test['sold']\nvalid_preds = valid['sold']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the store ids\nstores = sales.store_id.cat.codes.unique().tolist()\nfor store in stores:\n    df = data[data['store_id']==store]\n    \n    #Split the data\n    X_train, y_train = df[df['d']<1914].drop('sold',axis=1), df[df['d']<1914]['sold']\n    X_valid, y_valid = df[(df['d']>=1914) & (df['d']<1942)].drop('sold',axis=1), df[(df['d']>=1914) & (df['d']<1942)]['sold']\n    X_test = df[df['d']>=1942].drop('sold',axis=1)\n    \n    #Train and validate\n    model = LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.3,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        max_depth=8,\n        num_leaves=50,\n        min_child_weight=300\n    )\n    print('*****Prediction for Store: {}*****'.format(d_store_id[store]))\n    model.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_valid,y_valid)],\n             eval_metric='rmse', verbose=20, early_stopping_rounds=20)\n    valid_preds[X_valid.index] = model.predict(X_valid)\n    eval_preds[X_test.index] = model.predict(X_test)\n    filename = 'model'+str(d_store_id[store])+'.pkl'\n    # save model\n    joblib.dump(model, filename)\n    del model, X_train, y_train, X_valid, y_valid\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting feature importances**"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_df = pd.DataFrame()\nfeatures = [f for f in data.columns if f != 'sold']\nfor filename in os.listdir('/kaggle/working/'):\n    if 'model' in filename:\n        # load model\n        model = joblib.load(filename)\n        store_importance_df = pd.DataFrame()\n        store_importance_df[\"feature\"] = features\n        store_importance_df[\"importance\"] = model.feature_importances_\n        store_importance_df[\"store\"] = filename[5:9]\n        feature_importance_df = pd.concat([feature_importance_df, store_importance_df], axis=0)\n    \ndef display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:20].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (averaged over store predictions)')\n    plt.tight_layout()\n    \ndisplay_importances(feature_importance_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Make the submission**\n\nIf you remember for EDA, feature engineering and training I had melted the provided data from wide format to long format. Now, I have the predictions in long format but the format to be evaluated for the competition is in long format. Therefore, I'll convert it into wide format using `pivot` function in pandas. Below is an image explaining the pivot function.\n<img src='https://pandas.pydata.org/pandas-docs/version/0.25.3/_images/reshaping_pivot.png' style=\"width:500px;height:300px;\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set actual equal to false if you want to top in the public leaderboard :P\nactual = False\nif actual == False:\n    #Get the validation results(We already have them as less than one month left for competition to end)\n    validation = sales[['id']+['d_' + str(i) for i in range(1914,1942)]]\n    validation['id']=pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv').id\n    validation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\nelse:\n    #Get the actual validation results\n    valid['sold'] = valid_preds\n    validation = valid[['id','d','sold']]\n    validation = pd.pivot(validation, index='id', columns='d', values='sold').reset_index()\n    validation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\n    validation.id = validation.id.map(d_id).str.replace('evaluation','validation')\n\n#Get the evaluation results\ntest['sold'] = eval_preds\nevaluation = test[['id','d','sold']]\nevaluation = pd.pivot(evaluation, index='id', columns='d', values='sold').reset_index()\nevaluation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\n#Remap the category id to their respective categories\nevaluation.id = evaluation.id.map(d_id)\n\n#Prepare the submission\nsubmit = pd.concat([validation,evaluation]).reset_index(drop=True)\nsubmit.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://media.giphy.com/media/APHFMUIaTnLIA/giphy.gif)\n## Do leave an upvote if you liked it. It will encourage me to produce more quality content :)\n<a href=\"#top\" class=\"btn btn-success btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOP</a>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}