{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndef load_data(base_path = \"data\"):\n    proteins = pd.read_csv(f\"{base_path}/train_proteins.csv\")\n    peptides = pd.read_csv(f\"{base_path}/train_peptides.csv\")\n    clinical = pd.read_csv(f\"{base_path}/train_clinical_data.csv\")\n    supplement = pd.read_csv(f\"{base_path}/supplemental_clinical_data.csv\")\n    return proteins, peptides, clinical, supplement\n\nproteins, peptides, clinical, supplement = load_data(\"../input/amp-parkinsons-disease-progression-prediction\")\nsupplement.loc[supplement[\"visit_month\"] == 5, \"visit_month\"] = 6\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:39:26.671814Z","iopub.execute_input":"2023-05-15T19:39:26.672344Z","iopub.status.idle":"2023-05-15T19:39:27.731793Z","shell.execute_reply.started":"2023-05-15T19:39:26.672299Z","shell.execute_reply":"2023-05-15T19:39:27.730875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def smape1p_ind(A, F):\n    val = 200 * np.abs(F - A) / (np.abs(A+1) + np.abs(F+1))\n    return val\n\ndef smape1p(A, F):\n    return smape1p_ind(A, F).mean()\n\ndef max_dif(val, lst):\n    lst0 = [x for x in lst if x < val]\n    if len(lst0) == 0:\n        return -1\n    return val - max(lst0)\n\ndef count_prev_visits(val, lst):\n    lst0 = [x for x in lst if x < val]\n    return len(lst0)\n\nclass DataPrep:\n    def __init__(self, target_horizons=[0, 6, 12, 24], test_vmonths = [0, 6, 12, 18, 24, 36, 48, 60, 72, 84]):\n        self.target_horizons = target_horizons\n        self.test_vmonths = test_vmonths\n\n    def fit(self, proteins_df, peptides_df, clinical_df):\n        pass\n\n    def fe(self, sample, proteins_df, peptides_df, clinical_df):\n        for v_month in [0, 6, 12, 18, 24, 36, 48, 60, 72, 84]:\n            p = list(clinical_df[clinical_df[\"visit_month\"] == v_month][\"patient_id\"].unique())\n            sample[f\"visit_{v_month}m\"] = sample.apply(lambda x: (x[\"patient_id\"] in p) and (x[\"visit_month\"] >= v_month), axis=1).astype(int)\n\n            p = list(proteins_df[proteins_df[\"visit_month\"] == v_month][\"patient_id\"].unique())\n            sample[f\"btest_{v_month}m\"] = sample.apply(lambda x: (x[\"patient_id\"] in p) and (x[\"visit_month\"] >= v_month), axis=1).astype(int)\n\n            sample[f\"t_month_eq_{v_month}\"] = (sample[\"target_month\"] == v_month).astype(int)\n            sample[f\"v_month_eq_{v_month}\"] = (sample[\"visit_month\"] == v_month).astype(int)\n\n        for hor in self.target_horizons:\n            sample[f\"hor_eq_{hor}\"] = (sample[\"horizon\"] == hor).astype(int)\n\n        sample[\"horizon_scaled\"] = sample[\"horizon\"] / 24.0\n\n        blood_samples = proteins_df[\"visit_id\"].unique()\n        sample[\"blood_taken\"] = sample.apply(lambda x: x[\"visit_id\"] in blood_samples, axis=1).astype(int)\n        \n        all_visits = clinical_df.groupby(\"patient_id\")[\"visit_month\"].apply(lambda x: list(set(x))).to_dict()\n        all_non12_visits = sample.apply(lambda x: [xx for xx in all_visits.get(x[\"patient_id\"], []) if xx <= x[\"visit_month\"] and xx%12 != 0], axis=1)\n        sample[\"count_non12_visits\"] = all_non12_visits.apply(lambda x: len(x)) \n\n        return sample\n\n    def transform_train(self, proteins_df, peptides_df, clinical_df):\n        sample = clinical_df.rename({\"visit_month\":\"target_month\", \"visit_id\":\"visit_id_target\"}, axis=1).\\\n            merge(clinical_df[[\"patient_id\", \"visit_month\", \"visit_id\"]], how=\"left\", on=\"patient_id\")\n        sample[\"horizon\"] = sample[\"target_month\"] - sample[\"visit_month\"]\n        sample = sample[sample[\"horizon\"].isin(self.target_horizons)]\n        sample = sample[sample[\"visit_month\"].isin(self.test_vmonths)]\n\n        # Features\n        sample = self.fe(sample,\n            proteins_df[proteins_df[\"visit_month\"].isin(self.test_vmonths)],\n            peptides_df[peptides_df[\"visit_month\"].isin(self.test_vmonths)],\n            clinical_df[clinical_df[\"visit_month\"].isin(self.test_vmonths)])\n\n        # Targets reshape\n        res = []\n        for tgt_i in np.arange(1, 5):\n            delta_df = sample.copy()\n            if f\"updrs_{tgt_i}\" in delta_df.columns:\n                delta_df[\"target\"] = delta_df[f\"updrs_{tgt_i}\"]\n                delta_df[\"target_norm\"] = delta_df[\"target\"] / 100\n            delta_df[\"target_i\"] = tgt_i\n            res.append(delta_df)\n\n        sample = pd.concat(res, axis=0).reset_index(drop=True)\n        if f\"updrs_1\" in sample.columns:\n            sample = sample.drop([\"updrs_1\", \"updrs_2\", \"updrs_3\", \"updrs_4\"], axis=1)\n        \n        for tgt_i in np.arange(1, 5):\n            sample[f\"target_n_{tgt_i}\"] = (sample[\"target_i\"] == tgt_i).astype(int)\n\n        return sample\n    \n    def transform_test(self, proteins_df, peptides_df, test_df, sub_df):\n        sub = sub_df.copy()\n        sub[\"patient_id\"] = sub[\"prediction_id\"].apply(lambda x: int(x.split(\"_\")[0]))\n        sub[\"visit_month\"] = sub[\"prediction_id\"].apply(lambda x: int(x.split(\"_\")[1]))\n        sub[\"visit_id\"] = sub.apply(lambda x: str(x[\"patient_id\"]) + \"_\" + str(x[\"visit_month\"]), axis=1)\n\n        sample = sub[[\"patient_id\", \"visit_month\", \"visit_id\", \"prediction_id\"]]\n\n        sample[\"horizon\"] = sample[\"prediction_id\"].apply(lambda x: int(x.split(\"_\")[5]))\n        sample[\"target_i\"] = sample[\"prediction_id\"].apply(lambda x: int(x.split(\"_\")[3]))\n        sample[\"visit_month\"] = sample[\"visit_month\"]\n        sample[\"target_month\"] = sample[\"visit_month\"] + sample[\"horizon\"]\n        del sample[\"prediction_id\"]\n\n        # Features\n        sample = self.fe(sample, proteins_df, peptides_df, test_df)\n\n        for tgt_i in np.arange(1, 5):\n            sample[f\"target_n_{tgt_i}\"] = (sample[\"target_i\"] == tgt_i).astype(int)\n\n        return sample\n\ndp3 = DataPrep()\ndp3.fit(proteins, peptides, clinical)\n\nsample3 = dp3.transform_train(proteins, peptides, clinical)\nsample3 = sample3[~sample3[\"target\"].isnull()]\nsample3[\"is_suppl\"] = 0\n\nsup_sample3 = dp3.transform_train(proteins, peptides, supplement)\nsup_sample3 = sup_sample3[~sup_sample3[\"target\"].isnull()]\nsup_sample3[\"is_suppl\"] = 1\n\nprint(sample3.shape)\nprint(sup_sample3.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:41:39.930262Z","iopub.execute_input":"2023-05-15T19:41:39.930835Z","iopub.status.idle":"2023-05-15T19:41:52.144577Z","shell.execute_reply.started":"2023-05-15T19:41:39.930783Z","shell.execute_reply":"2023-05-15T19:41:52.143223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_df(df, folds_mapping, fold_id:int = 0):\n    folds = df[\"patient_id\"].map(folds_mapping)\n\n    df_train = df[folds != fold_id]\n    df_train = df_train[~df_train[\"target\"].isnull()].reset_index(drop=True)\n\n    df_valid = df[folds == fold_id]\n    df_valid = df_valid[~df_valid[\"target\"].isnull()].reset_index(drop=True)\n    \n    return df_train, df_valid\n\ndef create_folds_mapping(df, n_folds=5, random_state=42):\n    folds_df = pd.DataFrame({\"patient_id\":df[\"patient_id\"].unique()})\n    folds_df[\"fold\"] = -1\n\n    for i, (_, test_index) in enumerate(KFold(n_splits=n_folds, \n            shuffle=True, random_state=random_state).split(folds_df)):\n        folds_df.loc[test_index, \"fold\"] = i\n    folds_mapping = folds_df.set_index([\"patient_id\"])[\"fold\"]\n    return folds_mapping\n\nfrom joblib import Parallel, delayed\n\ndef smape1p_ind(A, F):\n    val = 200 * np.abs(F - A) / (np.abs(A+1) + np.abs(F+1))\n    return val\n\ndef smape1p(A, F):\n    return smape1p_ind(A, F).mean()\n\ndef smape1p_opt(x):\n    #return np.median(x)\n    tgts = np.arange(0, 61)\n    #tgts = [smape(x, val) for val in np.arange(0, 61)]\n    scores = [smape1p(x, val) for val in tgts]\n    return tgts[np.argmin(scores)]\n\ndef split_df(df, folds_mapping, fold_id:int = 0):\n    folds = df[\"patient_id\"].map(folds_mapping)\n\n    df_train = df[folds != fold_id]\n    df_train = df_train[~df_train[\"target\"].isnull()].reset_index(drop=True)\n\n    df_valid = df[folds == fold_id]\n    df_valid = df_valid[~df_valid[\"target\"].isnull()].reset_index(drop=True)\n    \n    return df_train, df_valid\n\ndef create_folds_mapping(df, n_folds=5, random_state=42):\n    folds_df = pd.DataFrame({\"patient_id\":df[\"patient_id\"].unique()})\n    folds_df[\"fold\"] = -1\n\n    for i, (_, test_index) in enumerate(KFold(n_splits=n_folds, \n            shuffle=True, random_state=random_state).split(folds_df)):\n        folds_df.loc[test_index, \"fold\"] = i\n    folds_mapping = folds_df.set_index([\"patient_id\"])[\"fold\"]\n    return folds_mapping\n\ndef run_single_fit(model, df_train, df_valid, fold_id, seed, probs):\n    if probs:\n        p = model.fit_predict_proba(df_train, df_valid)\n        p = pd.DataFrame(p, columns=[f\"prob_{i}\" for i in range(p.shape[1])]).reset_index(drop=True)\n        res = pd.DataFrame({\"seed\":seed, \"fold\": fold_id, \\\n            \"patient_id\":df_valid[\"patient_id\"], \"visit_month\":df_valid[\"visit_month\"], \\\n            \"target_month\":df_valid[\"target_month\"], \"target_i\":df_valid[\"target_i\"], \\\n            \"target\":df_valid[\"target\"]}).reset_index(drop=True)\n        return pd.concat([res, p], axis=1)\n    else:\n        p = model.fit_predict(df_train, df_valid)\n        return pd.DataFrame({\"seed\":seed, \"fold\": fold_id, \\\n            \"patient_id\":df_valid[\"patient_id\"], \"visit_month\":df_valid[\"visit_month\"], \\\n            \"target_month\":df_valid[\"target_month\"], \"target_i\":df_valid[\"target_i\"], \\\n            \"target\":df_valid[\"target\"], \"preds\":p})\n\nclass BaseModel:\n    def fit(self, df_train):\n        raise \"NotImplemented\"\n\n    def predict(self, df_valid):\n        raise \"NotImplemented\"\n\n    def predict_proba(self, df_valid):\n        raise \"NotImplemented\"\n\n    def fit_predict(self, df_train, df_valid):\n        self.fit(df_train)\n        return self.predict(df_valid)\n\n    def fit_predict_proba(self, df_train, df_valid):\n        self.fit(df_train)\n        return self.predict_proba(df_valid)\n\n    def cv(self, sample, sup_sample=None, n_folds=5, random_state=42):\n        folds_mapping = create_folds_mapping(sample, n_folds, random_state)\n\n        res = None\n        for fold_id in sorted(folds_mapping.unique()):\n            df_train, df_valid = split_df(sample, folds_mapping, fold_id)\n            if sup_sample is not None:\n                df_train = pd.concat([df_train, sup_sample], axis=0)\n            p = self.fit_predict(df_train, df_valid)\n            delta = pd.DataFrame({\"fold\": fold_id,  \\\n                    \"patient_id\":df_valid[\"patient_id\"], \"visit_month\":df_valid[\"visit_month\"], \\\n                    \"target_month\":df_valid[\"target_month\"], \"target_i\":df_valid[\"target_i\"], \\\n                    \"target\":df_valid[\"target\"], \"preds\":p})\n            res = pd.concat([res, delta], axis=0)\n\n        return res\n\n    def cvx(self, sample, sup_sample=None, n_runs=1, n_folds=5, random_state=42, probs=False):\n        np.random.seed(random_state)\n        seeds = np.random.randint(0, 1e6, n_runs)\n\n        run_args = []\n        for seed in seeds:\n            folds_mapping = create_folds_mapping(sample, n_folds, seed)\n            for fold_id in sorted(folds_mapping.unique()):\n                df_train, df_valid = split_df(sample, folds_mapping, fold_id)\n                if sup_sample is not None:\n                    df_train = pd.concat([df_train, sup_sample], axis=0)\n                run_args.append(dict(\n                    df_train = df_train,\n                    df_valid = df_valid,\n                    fold_id = fold_id,\n                    seed = seed,\n                    probs = probs\n                ))\n\n        res = Parallel(-1)(delayed(run_single_fit)(self, **args) for args in run_args)\n        #res = [run_single_fit(self, **args) for args in run_args]\n        return pd.concat(res, axis=0)\n\n    def loo(self, sample, sup_sample=None, probs=False, sample2=None):\n        if sample2 is None:\n            sample2 = sample\n        run_args = []\n        for patient_id in sample[\"patient_id\"].unique():\n            df_train = sample[sample[\"patient_id\"] != patient_id]\n            df_valid = sample2[sample2[\"patient_id\"] == patient_id]\n            if sup_sample is not None:\n                df_train = pd.concat([df_train, sup_sample], axis=0)\n            run_args.append(dict(\n                df_train = df_train,\n                df_valid = df_valid,\n                fold_id = None,\n                seed = None,\n                probs=probs\n            ))\n\n        res = Parallel(-1)(delayed(run_single_fit)(self, **args) for args in run_args)\n        return pd.concat(res, axis=0)\n\ndef print_cvx_summary(res_df):\n    scores = res_df.groupby([\"seed\", \"fold\"]).apply(lambda x: smape1p(x[\"target\"], x[\"preds\"])).values\n    print(\"# \", len(scores), \" runs\")\n    #print(\"# 05   :      \", np.quantile(scores, 0.05))\n    #print(\"# 25   :   \", np.quantile(scores, 0.25))\n    print(\"# mean :\", scores.mean())\n    #print(\"# 75   :   \", np.quantile(scores, 0.75))\n    #print(\"# 95   :      \", np.quantile(scores, 0.95))\n    print(\"# ovrl :\", smape1p(res_df[\"target\"], res_df[\"preds\"]))\n\ndef print_loo_summary(res_df):\n    scores = res_df.groupby([\"patient_id\"]).apply(lambda x: smape1p(x[\"target\"], x[\"preds\"])).values\n    print(\"# \", len(scores), \" runs\")\n    #print(\"# 05   :      \", np.quantile(scores, 0.05))\n    #print(\"# 25   :   \", np.quantile(scores, 0.25))\n    print(\"# mean :\", scores.mean())\n    #print(\"# 75   :   \", np.quantile(scores, 0.75))\n    #print(\"# 95   :      \", np.quantile(scores, 0.95))\n    print(\"# ovrl :\", smape1p(res_df[\"target\"], res_df[\"preds\"]))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:41:52.147037Z","iopub.execute_input":"2023-05-15T19:41:52.147826Z","iopub.status.idle":"2023-05-15T19:41:52.201485Z","shell.execute_reply.started":"2023-05-15T19:41:52.147775Z","shell.execute_reply":"2023-05-15T19:41:52.199895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sys\nimport random\nfrom tqdm import tqdm\nimport gc\nimport torch\nfrom torch import optim\nfrom torch.cuda.amp import GradScaler, autocast\nfrom collections import defaultdict\nfrom copy import copy\nimport os\nfrom transformers import get_cosine_schedule_with_warmup\nfrom torch.utils.data import SequentialSampler, DataLoader\nfrom sklearn.metrics import roc_auc_score, f1_score, cohen_kappa_score\nfrom torch.utils.data import Dataset\nfrom scipy.special import softmax\n\ntorch.set_num_threads(1)\n\ndef single_smape1p(preds, tgt):\n    x = np.tile(np.arange(preds.shape[1]), (preds.shape[0], 1))\n    x = np.abs(x - tgt) / (2 + x + tgt)\n    return (x * preds).sum(axis=1)\n\ndef opt_smape1p(preds):\n    x = np.hstack([single_smape1p(preds, i).reshape(-1,1) for i in range(preds.shape[1])])\n    return x.argmin(axis=1)\n\n\nfrom types import SimpleNamespace\nfrom torch import nn\nimport torch\n\nclass CustomDataset(Dataset):\n    def __init__(self, df, cfg, aug, mode=\"train\"):\n        self.cfg = cfg\n        self.mode = mode\n        self.df = df.copy()\n        self.features = df[cfg.features].values\n        if self.mode != \"test\":\n            self.targets = df[self.cfg.target_column].values.astype(np.float32)\n        else:\n            self.targets = np.zeros(len(df))\n\n    def __getitem__(self, idx):\n        features = self.features[idx]\n        targets = self.targets[idx]\n        \n        feature_dict = {\n            \"input\": torch.tensor(features),\n            \"target_norm\": torch.tensor(targets),\n        }\n        return feature_dict\n\n    def __len__(self):\n        return len(self.df)\n\n\nclass Net(nn.Module):\n    def __init__(self, cfg):\n        super(Net, self).__init__()\n        self.cfg = cfg\n        self.n_classes = cfg.n_classes\n        self.cnn = nn.Sequential(*([\n            nn.Linear(len(self.cfg.features), cfg.n_hidden),\n            nn.LeakyReLU(),\n            ] +\n            [\n            nn.Linear(cfg.n_hidden, cfg.n_hidden),\n            nn.LeakyReLU(),\n            ] * self.cfg.n_layers)\n        )\n\n        self.head = nn.Sequential(\n            nn.Linear(cfg.n_hidden, self.n_classes),\n            nn.LeakyReLU(),\n        )\n\n    def forward(self, batch):\n        input = batch[\"input\"].float()\n        y = batch[\"target_norm\"]\n        x = input\n        x = self.cnn(x)\n        preds = self.head(x).squeeze(-1)\n        loss = (torch.abs(y - preds) / (torch.abs(0.01 + y) + torch.abs(0.01 + preds))).mean()\n        return {\"loss\": loss, \"preds\": preds, \"target_norm\": y}\n\n\ndef worker_init_fn(worker_id):\n    np.random.seed(np.random.get_state()[1][0] + worker_id)\n\ndef get_train_dataloader(train_ds, cfg, verbose):\n    train_dataloader = DataLoader(\n        train_ds,\n        sampler=None,\n        shuffle=True,\n        batch_size=cfg.batch_size,\n        num_workers=cfg.num_workers,\n        pin_memory=False,\n        collate_fn=cfg.tr_collate_fn,\n        drop_last=cfg.drop_last,\n        worker_init_fn=worker_init_fn,\n    )\n    if verbose:\n        print(f\"train: dataset {len(train_ds)}, dataloader {len(train_dataloader)}\")\n    return train_dataloader\n\n\ndef get_val_dataloader(val_ds, cfg, verbose):\n    sampler = SequentialSampler(val_ds)\n    if cfg.batch_size_val is not None:\n        batch_size = cfg.batch_size_val\n    else:\n        batch_size = cfg.batch_size\n    val_dataloader = DataLoader(\n        val_ds,\n        sampler=sampler,\n        batch_size=batch_size,\n        num_workers=cfg.num_workers,\n        pin_memory=False,\n        collate_fn=cfg.val_collate_fn,\n        worker_init_fn=worker_init_fn,\n    )\n    if verbose:\n        print(f\"valid: dataset {len(val_ds)}, dataloader {len(val_dataloader)}\")\n    return val_dataloader\n\n\ndef get_scheduler(cfg, optimizer, total_steps):\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=cfg.warmup * (total_steps // cfg.batch_size),\n        num_training_steps=cfg.epochs * (total_steps // cfg.batch_size),\n    )\n    return scheduler\n\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    #torch.backends.cudnn.deterministic = False\n    #torch.backends.cudnn.benchmark = True\n\ndef batch_to_device(batch, device):\n    batch_dict = {key: batch[key].to(device) for key in batch}\n    return batch_dict\n\ndef run_eval(model, val_dataloader, cfg, pre=\"val\", verbose=True):\n    model.eval()\n    torch.set_grad_enabled(False)\n    val_data = defaultdict(list)\n    if verbose:\n        progress_bar = tqdm(val_dataloader)\n    else:\n        progress_bar = val_dataloader\n    for data in progress_bar:\n        batch = batch_to_device(data, cfg.device)\n        if cfg.mixed_precision:\n            with autocast():\n                output = model(batch)\n        else:\n            output = model(batch)\n        for key, val in output.items():\n            val_data[key] += [output[key]]\n    for key, val in output.items():\n        value = val_data[key]\n        if len(value[0].shape) == 0:\n            val_data[key] = torch.stack(value)\n        else:\n            val_data[key] = torch.cat(value, dim=0)\n\n    preds = val_data[\"preds\"].cpu().numpy()\n    if (pre == \"val\") and verbose:\n        metric = smape1p(100*val_data[\"target_norm\"].cpu().numpy(), 100*preds)\n        print(f\"{pre}_metric 1 \", metric)\n        metric = smape1p(100*val_data[\"target_norm\"].cpu().numpy(), np.round(100*preds))\n        print(f\"{pre}_metric 2 \", metric)\n    \n    return 100*preds\n\n\ndef run_train(cfg, train_df, val_df, test_df=None, verbose=True):\n    os.makedirs(str(cfg.output_dir + \"/\"), exist_ok=True)\n\n    if cfg.seed < 0:\n        cfg.seed = np.random.randint(1_000_000)\n    if verbose:\n        print(\"seed\", cfg.seed)\n    set_seed(cfg.seed)\n\n    train_dataset = CustomDataset(train_df, cfg, aug=None, mode=\"train\")\n    train_dataloader = get_train_dataloader(train_dataset, cfg, verbose)\n    \n    if val_df is not None:\n        val_dataset = CustomDataset(val_df, cfg, aug=None, mode=\"val\")\n        val_dataloader = get_val_dataloader(val_dataset, cfg, verbose)\n\n    if test_df is not None:\n        test_dataset = CustomDataset(test_df, cfg, aug=None, mode=\"test\")\n        test_dataloader = get_val_dataloader(test_dataset, cfg, verbose)\n\n    model = Net(cfg)\n    model.to(cfg.device)\n\n    total_steps = len(train_dataset)\n    params = model.parameters()\n    optimizer = optim.Adam(params, lr=cfg.lr, weight_decay=0)\n    scheduler = get_scheduler(cfg, optimizer, total_steps)\n\n    if cfg.mixed_precision:\n        scaler = GradScaler()\n    else:\n        scaler = None\n\n    cfg.curr_step = 0\n    i = 0\n    optimizer.zero_grad()\n    for epoch in range(cfg.epochs):\n        set_seed(cfg.seed + epoch)\n        if verbose:\n            print(\"EPOCH:\", epoch)\n            progress_bar = tqdm(range(len(train_dataloader)))\n        else:\n            progress_bar = range(len(train_dataloader))\n        tr_it = iter(train_dataloader)\n        losses = []\n        gc.collect()\n\n        for itr in progress_bar:\n            i += 1\n            data = next(tr_it)\n            model.train()\n            torch.set_grad_enabled(True)\n            batch = batch_to_device(data, cfg.device)\n            if cfg.mixed_precision:\n                with autocast():\n                    output_dict = model(batch)\n            else:\n                output_dict = model(batch)\n            loss = output_dict[\"loss\"]\n            losses.append(loss.item())\n            if cfg.mixed_precision:\n                scaler.scale(loss).backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.gradient_clip)\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n            else:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.gradient_clip)\n                optimizer.step()\n                optimizer.zero_grad()\n            if scheduler is not None:\n                scheduler.step()\n        if val_df is not None:\n            if (epoch + 1) % cfg.eval_epochs == 0 or (epoch + 1) == cfg.epochs:\n                run_eval(model, val_dataloader, cfg, pre=\"val\", verbose=verbose)\n\n    if test_df is not None:\n        return run_eval(model, test_dataloader, cfg, pre=\"test\", verbose=verbose)\n    else:\n        return model\n\ndef run_test(model, cfg, test_df):\n    test_dataset = CustomDataset(test_df, cfg, aug=None, mode=\"test\")\n    test_dataloader = get_val_dataloader(test_dataset, cfg, verbose=False)\n    return run_eval(model, test_dataloader, cfg, pre=\"test\", verbose=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:41:52.203566Z","iopub.execute_input":"2023-05-15T19:41:52.20411Z","iopub.status.idle":"2023-05-15T19:41:52.273356Z","shell.execute_reply.started":"2023-05-15T19:41:52.204072Z","shell.execute_reply":"2023-05-15T19:41:52.271977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\n\ndef single_smape1p(preds, tgt):\n    x = np.tile(np.arange(preds.shape[1]), (preds.shape[0], 1))\n    x = np.abs(x - tgt) / (2 + x + tgt)\n    return (x * preds).sum(axis=1)\n\ndef opt_smape1p(preds):\n    x = np.hstack([single_smape1p(preds, i).reshape(-1,1) for i in range(preds.shape[1])])\n    return x.argmin(axis=1)\n\nclass LGBClassModel1(BaseModel):\n    def __init__(self, params, features) -> None:\n        self.params = params\n        self.features = features\n    \n    def fit(self, df_train):\n        if self.features is None:\n            self.features = [col for col in df_train.columns if col.startswith(\"v_\")]\n        lgb_train = lgb.Dataset(df_train[self.features], df_train[\"target\"])\n        params0 = {k:v for k,v in self.params.items() if k not in [\"n_estimators\"]}\n        self.m_gbm = lgb.train(params0, lgb_train, num_boost_round=self.params[\"n_estimators\"])\n        return self\n\n    def predict_proba(self, df_valid):\n        return self.m_gbm.predict(df_valid[self.features])\n\n    def predict(self, df_valid):\n        return opt_smape1p(self.predict_proba(df_valid))\n\n\n\nparams = {\n        'boosting_type': 'gbdt',\n        'objective': 'multiclass',\n        'num_class': 87,\n        \"n_estimators\": 300,\n\n        'learning_rate': 0.019673004699536346,\n        'num_leaves': 208,\n        'max_depth': 14,\n        'min_data_in_leaf': 850,\n        'feature_fraction': 0.5190632906197453,\n        'lambda_l1': 7.405660751699475e-08,\n        'lambda_l2': 0.14583961675675494,\n        'max_bin': 240,\n    \n        'verbose': -1,\n        'force_col_wise': True,\n        'n_jobs': -1,\n    }\n\nfeatures = [\"target_i\", \"target_month\", \"horizon\", \"visit_month\", \"visit_6m\", \"blood_taken\"]\nfeatures += [\"visit_18m\", \"is_suppl\"]\nfeatures += [\"count_non12_visits\"]\nfeatures += [\"visit_48m\"]\n\nmodel_lgb = LGBClassModel1(params, features)\nmodel_lgb = model_lgb.fit(pd.concat([sample3, sup_sample3], axis=0))\n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:41:52.276252Z","iopub.execute_input":"2023-05-15T19:41:52.276788Z","iopub.status.idle":"2023-05-15T19:43:25.703453Z","shell.execute_reply.started":"2023-05-15T19:41:52.276729Z","shell.execute_reply":"2023-05-15T19:43:25.701965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NNRegModel1(BaseModel):\n    def __init__(self, cfg, features=None) -> None:\n        self.cfg = cfg\n        #self.features = features\n    \n    def fit(self, df_train):\n        self.models = [run_train(self.cfg, df_train, None, None, verbose=False) for _ in range(self.cfg.bag_size)]\n        return self\n\n    def predict(self, df_valid):\n        preds = np.vstack([run_test(model, self.cfg, df_valid) for model in self.models])\n        if self.cfg.bag_agg_function == \"max\":\n            return np.max(preds, axis=0)\n        elif self.cfg.bag_agg_function == \"median\":\n            return np.median(preds, axis=0)\n        else:\n            return np.mean(preds, axis=0)\n\n\ncfg = SimpleNamespace(**{})\n\ncfg.tr_collate_fn = None\ncfg.val_collate_fn = None\n#cfg.CustomDataset = CustomDataset\n#cfg.net = Net\n\ncfg.target_column = \"target_norm\"\ncfg.output_dir = \"results/nn_temp\"\ncfg.seed = -1\ncfg.eval_epochs = 1\ncfg.mixed_precision = False\n#cfg.device = \"cuda:2\"\ncfg.device = \"cpu\"\n\ncfg.n_classes = 1\ncfg.batch_size = 128\ncfg.batch_size_val = 256\ncfg.n_hidden = 64\ncfg.n_layers = 2 #3\ncfg.num_workers = 0\ncfg.drop_last = False\ncfg.gradient_clip = 1.0\n\ncfg.bag_size = 1\ncfg.bag_agg_function = \"mean\"\ncfg.lr = 2e-3\ncfg.warmup = 0\ncfg.epochs = 10\n\ncfg.features = [\"visit_6m\"]\n#cfg.features += [\"blood_taken\"]\ncfg.features += [c for c in sample3.columns if c.startswith(\"t_month_eq_\")]\ncfg.features += [c for c in sample3.columns if c.startswith(\"v_month_eq_\")]\ncfg.features += [c for c in sample3.columns if c.startswith(\"hor_eq_\")]\ncfg.features += [c for c in sample3.columns if c.startswith(\"target_n_\")]\ncfg.features += [\"visit_18m\"]\ncfg.features += [\"visit_48m\"]\ncfg.features += [\"is_suppl\"]\ncfg.features += [\"horizon_scaled\"]\n\nmodel_nn = NNRegModel1(cfg)\nmodel_nn = model_nn.fit(pd.concat([sample3, sup_sample3], axis=0))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:40:33.576402Z","iopub.execute_input":"2023-05-15T19:40:33.576846Z","iopub.status.idle":"2023-05-15T19:40:49.164387Z","shell.execute_reply.started":"2023-05-15T19:40:33.57681Z","shell.execute_reply":"2023-05-15T19:40:49.163162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/amp-pd')\n\nimport amp_pd_peptide\nenv = amp_pd_peptide.make_env()\niter_test = env.iter_test()\n\ndef repl(x1, x2, cond):\n    res = x1.copy()\n    res[cond] = x2[cond]\n    return res\n\nall_test_peptides = None\nall_test_proteins = None\nall_test_df = None\n\nfor (test_df, test_peptides, test_proteins, sample_submission) in iter_test:\n    all_test_df = pd.concat([all_test_df, test_df], axis=0)\n    all_test_proteins = pd.concat([all_test_proteins, test_proteins], axis=0)\n    all_test_peptides = pd.concat([all_test_peptides, test_peptides], axis=0)\n    sample_test = dp3.transform_test(all_test_proteins, all_test_peptides, all_test_df, sample_submission)\n    sample_test[\"is_suppl\"] = 0\n    \n    sample_test[\"preds_lgb\"] = model_lgb.predict(sample_test)\n    sample_test[\"preds_nn\"] = np.round(np.clip(model_nn.predict(sample_test), 0, None))\n    \n    sample_submission[\"rating\"] = np.round( (sample_test[\"preds_lgb\"] + sample_test[\"preds_nn\"]) / 2)\n    \n    env.predict(sample_submission)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:51:03.587357Z","iopub.execute_input":"2023-05-15T19:51:03.58783Z","iopub.status.idle":"2023-05-15T19:51:04.171721Z","shell.execute_reply.started":"2023-05-15T19:51:03.587792Z","shell.execute_reply":"2023-05-15T19:51:04.170563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/working/submission.csv')\nsub","metadata":{"execution":{"iopub.status.busy":"2023-05-15T19:51:08.455573Z","iopub.execute_input":"2023-05-15T19:51:08.455995Z","iopub.status.idle":"2023-05-15T19:51:08.475794Z","shell.execute_reply.started":"2023-05-15T19:51:08.455961Z","shell.execute_reply":"2023-05-15T19:51:08.474515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}