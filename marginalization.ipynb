{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c32f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import json\n",
    "from enum import Enum\n",
    "class HypothesisComponent(str, Enum):\n",
    "    DataLoadSpec = \"DataLoadSpec\"\n",
    "    FeatureEng = \"FeatureEng\"\n",
    "    Model = \"Model\"\n",
    "    Ensemble = \"Ensemble\"\n",
    "    Workflow = \"Workflow\"\n",
    "\n",
    "class HypothesisEvaluationReasoningScore(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description=\"What is the quality of the hypothesis under this criteria? Answer in 1-2 sentence.\"\n",
    "    )\n",
    "    score: float = Field(description=\"The score of the hypothesis under this criteria between 1 and 10.\")\n",
    "\n",
    "\n",
    "class HypothesisEvaluation(BaseModel):\n",
    "    alignment: HypothesisEvaluationReasoningScore = Field(\n",
    "        description=\"The alignment of the proposed hypothesis with the identified challenge.\"\n",
    "    )\n",
    "    impact: HypothesisEvaluationReasoningScore = Field(\n",
    "        description=\"The expected impact of the proposed hypothesis on the current SOTA implementation.\"\n",
    "    )\n",
    "    novelty: HypothesisEvaluationReasoningScore = Field(\n",
    "        description=\"The novelty of the proposed hypothesis compared to existing solutions.\"\n",
    "    )\n",
    "    feasibility: HypothesisEvaluationReasoningScore = Field(\n",
    "        description=\"The feasibility of implementing the proposed hypothesis in the current SOTA implementation.\"\n",
    "    )\n",
    "    risk_reward_balance: HypothesisEvaluationReasoningScore = Field(\n",
    "        description=\"The risk-reward balance of implementing the proposed hypothesis.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class HypothesisDetail(BaseModel):\n",
    "    caption: str = Field(description=\"The caption of the challenge it is based on.\")\n",
    "    challenge: str = Field(\n",
    "        description=\"Reaffirm the challenge within the current context (e.g., trace history, domain principles, or competition constraints). It should be no more than 2-3 sentences.\"\n",
    "    )\n",
    "    hypothesis: str = Field(\n",
    "        description=\"The statement of the hypothesis. It could be a design of a new component, or a concise, testable statement derived from previous experimental outcomes.\"\n",
    "    )\n",
    "    metric_impact: str = Field(\n",
    "        description=(\n",
    "            \"Brief explanation (max 2 sentences) of the expected impact of the hypothesis on the target metric.\"\n",
    "        )\n",
    "    )\n",
    "    component: HypothesisComponent = Field(description=\"The component tag of the hypothesis.\")\n",
    "    evaluation: HypothesisEvaluation = Field(description=\"Evaluate the quality of the hypothesis.\")\n",
    "\n",
    "\n",
    "class HypothesisList(BaseModel):\n",
    "    deduplicated_challenges: List[str] = Field(\n",
    "        description=\"A list of deduplicated challenge captions. Each must retain its original wording. If multiple captions are semantically identical, keep the first one.\"\n",
    "    )\n",
    "    hypotheses: List[HypothesisDetail] = Field(\n",
    "        description=\"A non-empty list of hypotheses proposed for the next iteration, each corresponding to one challenge. The list length should match the number of challenges.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0044b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-17 09:50:23.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrdagent.oai.backend.litellm\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mbackend='rdagent.oai.backend.LiteLLMAPIBackend' chat_model='o3' embedding_model='text-embedding-ada-002' reasoning_effort=None enable_response_schema=True reasoning_think_rm=True log_llm_chat_content=True use_azure=False chat_use_azure=False embedding_use_azure=False chat_use_azure_token_provider=False embedding_use_azure_token_provider=False managed_identity_client_id=None max_retry=12000 retry_wait_seconds=5 dump_chat_cache=True use_chat_cache=False dump_embedding_cache=True use_embedding_cache=False prompt_cache_path='./log/prompt_cache.db' max_past_message_include=10 timeout_fail_limit=10 violation_fail_limit=1 use_auto_chat_cache_seed_gen=False init_chat_cache_seed=42 openai_api_key='sk-1234' chat_openai_api_key=None chat_openai_base_url=None chat_azure_api_base='' chat_azure_api_version='' chat_max_tokens=None chat_temperature=1.0 chat_stream=True chat_seed=None chat_frequency_penalty=0.0 chat_presence_penalty=0.0 chat_token_limit=100000 default_system_prompt=\"You are an AI assistant who helps to answer user's questions.\" system_prompt_role='system' embedding_openai_api_key='' embedding_openai_base_url='' embedding_azure_api_base='' embedding_azure_api_version='' embedding_max_str_num=50 use_llama2=False llama2_ckpt_dir='Llama-2-7b-chat' llama2_tokenizer_path='Llama-2-7b-chat/tokenizer.model' llams2_max_batch_size=8 use_gcr_endpoint=False gcr_endpoint_type='llama2_70b' llama2_70b_endpoint='' llama2_70b_endpoint_key='' llama2_70b_endpoint_deployment='' llama3_70b_endpoint='' llama3_70b_endpoint_key='' llama3_70b_endpoint_deployment='' phi2_endpoint='' phi2_endpoint_key='' phi2_endpoint_deployment='' phi3_4k_endpoint='' phi3_4k_endpoint_key='' phi3_4k_endpoint_deployment='' phi3_128k_endpoint='' phi3_128k_endpoint_key='' phi3_128k_endpoint_deployment='' gcr_endpoint_temperature=0.7 gcr_endpoint_top_p=0.9 gcr_endpoint_do_sample=False gcr_endpoint_max_token=100 chat_use_azure_deepseek=False chat_azure_deepseek_endpoint='' chat_azure_deepseek_key='' chat_model_map={}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from rdagent.oai.llm_utils import APIBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13f40896",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "# Scenario Description\n",
    "====== Background ======\n",
    "You are a world-class data scientist and machine learning engineer with deep expertise in statistics, mathematics, and computer science. \n",
    "Your knowledge spans cutting-edge data analysis techniques, advanced machine learning algorithms, and their practical applications to solve complex real-world problems.\n",
    "You are dedicated to producing accurate, efficient, and innovative solutions.\n",
    "\n",
    "The task type for this competition is **Classification (Ordinal classification of essay scores 1–6)**.\n",
    "The data type used in this competition is **Text (Natural Language Processing)**.\n",
    "\n",
    "Briefly, the competition involves: Participants must build an automated essay–scoring model that assigns a holistic score from 1 to 6 to each student-written argumentative essay, reducing the need for manual grading..\n",
    "\n",
    "The dataset used in this competition is:\n",
    "Processed data consist of three CSV files.\n",
    "1. train.csv – 15,576 rows × 3 columns: essay_id (string), full_text (the complete essay), score (integer 1-6).\n",
    "2. test.csv – 1,731 rows × 2 columns: essay_id, full_text (no score column).\n",
    "3. sample_submission.csv – 1,731 rows × 2 columns matching the required submission format (essay_id, score) with sample integer scores.\n",
    "These files come from a larger corpus (~24 000 essays) described in the competition overview. The processed version keeps only the essential columns and is already un-zipped and ready for modeling; no other folders or files are present..\n",
    "\n",
    "Submission channel number to each sample is: 1.\n",
    "\n",
    "The evaluation metric of this competition is:\n",
    "Leaderboard ranking is based on Quadratic Weighted Kappa (QWK), which measures agreement between the true scores and the submitted scores.\n",
    " 1. Let N = number of distinct score levels (here 6).\n",
    " 2. Build an N×N observed matrix O where O_{i,j} counts essays with true label i and predicted label j.\n",
    " 3. Build an N×N weight matrix w with w_{i,j} = ((i−j)^2)/(N−1)^2.\n",
    " 4. Build an N×N expected matrix E as the outer product of the true-label histogram and the predicted-label histogram, scaled so that sum(E)=sum(O).\n",
    " 5. QWK = 1 − ( Σ_{i,j} w_{i,j} O_{i,j} ) / ( Σ_{i,j} w_{i,j} E_{i,j} ).\n",
    "The metric ranges from –∞ to 1; higher values indicate better agreement..\n",
    "\n",
    "The following is the output of the exploratory data analysis (EDA) performed on the dataset, You should carefully analyze it to better craft your feature engineering and model training strategies.\n",
    "====== Data Overview (EDA) ======\n",
    "\n",
    "Train data shape: (15576, 3)\n",
    "Test data shape: (1731, 2)\n",
    "Train first 5 rows:\n",
    "essay_id                                          full_text  score\n",
    "0  663d2cf  Dear State Senator,\\n\\nI am arguing in favor o...      3\n",
    "1  3a20bfb  In \" The Challenge of Exploring Venus\" The aut...      2\n",
    "2  6adae64  Teachers can have a hard time telling if their...      3\n",
    "3  c81ccdf  Using dirverless cars can be very dangrous in ...      3\n",
    "4  9549d7f  In \" The Challenge of Exploring Venus\" the aut...      3\n",
    "Test first 5 rows:\n",
    "essay_id                                          full_text\n",
    "0  d550b2d  The face was not created by aliens because the...\n",
    "1  0c10954  Hello my name is Luke Bomberger and I was seag...\n",
    "2  ef04816  The technology to read the emotional expressio...\n",
    "3  88ab8d9  Do you ever wonder how many soomething is or h...\n",
    "4  cee8c0f  This story is about discovering mars. Witch is...\n",
    "Train data types:\n",
    "essay_id     object\n",
    "full_text    object\n",
    "score         int64\n",
    "dtype: object\n",
    "Test data types:\n",
    "essay_id     object\n",
    "full_text    object\n",
    "dtype: object\n",
    "Train missing values per column:\n",
    "essay_id     0\n",
    "full_text    0\n",
    "score        0\n",
    "dtype: int64\n",
    "Test missing values per column:\n",
    "essay_id     0\n",
    "full_text    0\n",
    "dtype: int64\n",
    "Train unique values per column:\n",
    "essay_id: 15576\n",
    "full_text: 15576\n",
    "score: 6\n",
    "Test unique values per column:\n",
    "essay_id: 1731\n",
    "full_text: 1731\n",
    "Target variable (score) distribution:\n",
    "score\n",
    "1    1124\n",
    "2    4249\n",
    "3    5629\n",
    "4    3563\n",
    "5     876\n",
    "6     135\n",
    "Name: count, dtype: int64\n",
    "Train essay lengths (chars): min=712 max=20459 mean=2073.5 std=930.1\n",
    "\n",
    "====== Submission Format ======\n",
    "Please ensure your submission adheres to the following specifications:\n",
    "Create a UTF-8 CSV named submission.csv with a header and exactly two columns:\n",
    "  • essay_id – copied from test.csv\n",
    "  • score – your predicted integer score (1–6) for each essay\n",
    "The rows must appear for every essay_id in the test set; order is not enforced but recommended to match the sample format.\n",
    "\n",
    "====== Important Guidelines ======\n",
    "Before submitting your results, please note the following:\n",
    "- We have numerous tests in place to check your code.\n",
    "- Ensure your submission is genuine.\n",
    "- Do not manipulate data or return values solely to pass preliminary tests, as this will not lead to successful final evaluation.\n",
    "\n",
    "====== Evaluation ======\n",
    "\n",
    "The primary evaluation metric for this task is: **Quadratic Weighted Kappa**.\n",
    "\n",
    "This metric is considered better when it is **larger**.\n",
    "\n",
    "Additional Evaluation Details:\n",
    "Leaderboard ranking is based on Quadratic Weighted Kappa (QWK), which measures agreement between the true scores and the submitted scores.\n",
    " 1. Let N = number of distinct score levels (here 6).\n",
    " 2. Build an N×N observed matrix O where O_{i,j} counts essays with true label i and predicted label j.\n",
    " 3. Build an N×N weight matrix w with w_{i,j} = ((i−j)^2)/(N−1)^2.\n",
    " 4. Build an N×N expected matrix E as the outer product of the true-label histogram and the predicted-label histogram, scaled so that sum(E)=sum(O).\n",
    " 5. QWK = 1 − ( Σ_{i,j} w_{i,j} O_{i,j} ) / ( Σ_{i,j} w_{i,j} E_{i,j} ).\n",
    "The metric ranges from –∞ to 1; higher values indicate better agreement.\n",
    "\n",
    "====== Time Limit ======\n",
    "Your code's execution is limited to ** 5.00 hours**. After this time limit, your code will be terminated. But remember your main target is to achieve the best performance and you have several times to modify your code. So please be bold to make the best use of all the time limit and don't be too conservative.\n",
    "During this time limit, you have all the resources available to you. Please fully leverage all the computational resources(CPUs and GPUs) to achieve the best performance like choose a powerful model, use a large batch size, enable data sampler with big parallel.\n",
    "\n",
    "# Previous Experiments and Feedbacks\n",
    "## Experiment Index: 1\n",
    "Target Problem: Design an ordinal-aware modelling and inference pipeline (e.g., ordinal regression head or cumulative link loss) that outputs monotonic score probabilities and converts them to integer labels using validation-tuned thresholds.\n",
    "\n",
    "Proposed Hypothesis: Fine-tune a pretrained DeBERTa-v3-base encoder with a CORAL (cumulative link) ordinal regression head that outputs five monotonic logits P(score>k). Convert these probabilities into the final 1-6 integer scores using validation-optimised thresholds that maximise QWK.\n",
    "Code Change Summary: Entire pipeline built from scratch: (1) load and validate CSVs, (2) extensive engineered linguistic features with caching, (3) DeBERTa-v3-base encoder combined with CORAL ordinal head that concatenates CLS embedding with engineered features, (4) class-balanced focal CORAL loss, (5) 5-fold infra but only fold-0 trained, (6) validation-based Powell optimisation of continuous thresholds, (7) test inference with same pipeline, (8) generation of scores.csv and submission.csv following required format, (9) mixed-precision, selective layer freezing, LanguageTool integration, and graceful resource handling.\n",
    "Surpass Previous SOTA: True\n",
    "\n",
    "Experiment Score: 0.8295875060477553\n",
    "Experiment Feedback: [Experiment Analysis] Submission format is correct and evaluation matches official QWK; predictions are generated with identical logic for validation and test, avoiding leakage. As the first valid run, the 0.8296 QWK establishes the initial best result to beat.\n",
    "\n",
    "# Current SOTA Implementation\n",
    "## Best of previous exploration of the scenario\n",
    "\n",
    "### Code\n",
    "Here is the complete code of the solution.\n",
    "\n",
    "File Path: main.py\n",
    "```\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import gc\n",
    "import re\n",
    "import importlib\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# ============ DEPENDENCY MANAGEMENT ============\n",
    "def pip_install(package, module=None, version=None):\n",
    "    pkg_string = f\"{package}=={version}\" if version else package\n",
    "    try:\n",
    "        return importlib.import_module(module or package)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {pkg_string}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", pkg_string])\n",
    "        return importlib.import_module(module or package)\n",
    "\n",
    "# textstat\n",
    "try:\n",
    "    import textstat\n",
    "except ImportError:\n",
    "    textstat = pip_install(\"textstat\")\n",
    "\n",
    "# language_tool_python & Java check\n",
    "def check_java_installed():\n",
    "    from shutil import which\n",
    "    if which(\"java\") is None:\n",
    "        print(\"WARNING: Java is not installed. Skipping grammar-based features.\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "language_tool_available = False\n",
    "try:\n",
    "    language_tool_python = importlib.import_module(\"language_tool_python\")\n",
    "    language_tool_available = check_java_installed()\n",
    "except ImportError:\n",
    "    try:\n",
    "        language_tool_python = pip_install(\"language-tool-python\", \"language_tool_python\")\n",
    "        language_tool_available = check_java_installed()\n",
    "    except Exception:\n",
    "        print(\"WARNING: language-tool-python could not be installed. Skipping grammar-based features.\")\n",
    "        language_tool_available = False\n",
    "\n",
    "# transformers (required)\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "except ImportError:\n",
    "    pip_install(\"transformers\", version=\"4.44.2\")\n",
    "    pip_install(\"accelerate\", version=\"0.33.0\")\n",
    "    from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "\n",
    "# ============ GLOBAL SEEDS & UTILS ===============\n",
    "def set_global_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "set_global_seed(42)\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def qwk_metric(y_true, y_pred, min_rating=1, max_rating=6):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "# ============ FP16 & CUDA DETECTION ==========\n",
    "def get_device():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    return torch.device(device)\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "FP16 = torch.cuda.is_available()  # Enable fp16 if possible\n",
    "\n",
    "# ========== PATHS ==========\n",
    "DATA_DIR = \"./workspace_input/\"\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, \"train.csv\")\n",
    "TEST_PATH = os.path.join(DATA_DIR, \"test.csv\")\n",
    "SAMPLE_SUBMISSION_PATH = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "ENGINEERED_FEATURES_CACHE = os.path.join(DATA_DIR, \"eng_feats_cache.pkl\")\n",
    "\n",
    "# ============ ARGPARSE FOR DEBUG MODE =========\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--debug', action='store_true', default=False)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# ============ DATA LOADING ===============\n",
    "def load_dataset():\n",
    "    print(\"Loading data...\")\n",
    "    train = pd.read_csv(TRAIN_PATH, encoding='utf-8')\n",
    "    test = pd.read_csv(TEST_PATH, encoding='utf-8')\n",
    "    expected_train_cols = ['essay_id', 'full_text', 'score']\n",
    "    expected_test_cols = ['essay_id', 'full_text']\n",
    "    # Minimal: ensure presence, don't fail on order\n",
    "    for col in expected_train_cols:\n",
    "        if col not in train.columns:\n",
    "            raise ValueError(f\"Train columns missing: expected {col}\")\n",
    "    for col in expected_test_cols:\n",
    "        if col not in test.columns:\n",
    "            raise ValueError(f\"Test columns missing: expected {col}\")\n",
    "    train['full_text'] = train['full_text'].astype(str)\n",
    "    test['full_text'] = test['full_text'].astype(str)\n",
    "    train['score'] = train['score'].astype('int64')\n",
    "    print(f\"Train shape: {train.shape}, Test shape: {test.shape}\")\n",
    "    return train, test\n",
    "\n",
    "train_df, test_df = load_dataset()\n",
    "\n",
    "# =========== EDA SECTION ==============\n",
    "def print_eda(train, test):\n",
    "    content = []\n",
    "    content.append(f\"Train data shape: {train.shape}\")\n",
    "    content.append(f\"Test data shape: {test.shape}\")\n",
    "    content.append(\"\\nTrain first 5 rows:\\n\" + str(train.head()))\n",
    "    content.append(\"\\nTest first 5 rows:\\n\" + str(test.head()))\n",
    "    content.append(\"\\nTrain data types:\\n\" + str(train.dtypes))\n",
    "    content.append(\"\\nTest data types:\\n\" + str(test.dtypes))\n",
    "    content.append(\"\\nTrain missing values per column:\\n\" + str(train.isnull().sum()))\n",
    "    content.append(\"\\nTest missing values per column:\\n\" + str(test.isnull().sum()))\n",
    "    content.append(\"\\nTrain unique values per column:\")\n",
    "    for col in train.columns:\n",
    "        content.append(f\"{col}: {train[col].nunique()}\")\n",
    "    content.append(\"\\nTest unique values per column:\")\n",
    "    for col in test.columns:\n",
    "        content.append(f\"{col}: {test[col].nunique()}\")\n",
    "    if 'score' in train.columns:\n",
    "        content.append(\"\\nTarget variable (score) distribution:\")\n",
    "        content.append(str(train['score'].value_counts(sort=False).sort_index()))\n",
    "    else:\n",
    "        content.append(\"\\nNo target variable found in train set.\")\n",
    "    if 'full_text' in train.columns:\n",
    "        lens = train['full_text'].str.len()\n",
    "        content.append(f\"\\nTrain essay lengths (chars): min={lens.min()} max={lens.max()} mean={lens.mean():.1f} std={lens.std():.1f}\")\n",
    "    return \"\\n\".join(content)\n",
    "\n",
    "eda_output = print_eda(train_df, test_df)\n",
    "print(\"=== Start of EDA part ===\")\n",
    "print(eda_output)\n",
    "print(\"=== End of EDA part ===\")\n",
    "\n",
    "# ========== FEATURE ENGINEERING ===================\n",
    "DISCOURSE_MARKERS = [\n",
    "    \"however\", \"therefore\", \"moreover\", \"furthermore\", \"for example\", \"for instance\",\n",
    "    \"in addition\", \"besides\", \"nonetheless\", \"consequently\", \"as a result\", \"thus\",\n",
    "    \"otherwise\", \"hence\", \"in conclusion\", \"in summary\", \"nevertheless\", \"on the other hand\",\n",
    "    \"similarly\", \"in contrast\", \"undoubtedly\", \"admittedly\", \"certainly\", \"obviously\",\n",
    "    \"in fact\", \"of course\", \"after all\", \"in short\", \"for this reason\", \"as such\",\n",
    "    \"accordingly\", \"and yet\", \"despite\", \"to sum up\", \"even though\", \"because\",\n",
    "    \"since\", \"although\", \"though\", \"meanwhile\", \"unless\", \"whereas\", \"when\", \"while\",\n",
    "    \"before\", \"after\", \"until\", \"whether\", \"once\"\n",
    "]\n",
    "PRONOUNS = [\n",
    "    \"i\", \"me\", \"my\", \"mine\", \"myself\", \"we\", \"us\", \"our\", \"ours\", \"ourselves\",\n",
    "    \"you\", \"your\", \"yours\", \"yourself\", \"he\", \"him\", \"his\", \"himself\",\n",
    "    \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
    "    \"they\", \"them\", \"their\", \"theirs\", \"themselves\"\n",
    "]\n",
    "CONJUNCTIONS = [\n",
    "    \"and\", \"but\", \"or\", \"nor\", \"for\", \"yet\", \"so\", \"although\", \"because\",\n",
    "    \"since\", \"unless\", \"while\", \"whereas\", \"even though\", \"though\"\n",
    "]\n",
    "\n",
    "def sentence_split(text):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    num_sentences = sum(1 for s in sentences if s.strip())\n",
    "    return num_sentences\n",
    "\n",
    "def count_markers(text, markers):\n",
    "    t = text.lower()\n",
    "    total = 0\n",
    "    for word in markers:\n",
    "        total += t.count(word)\n",
    "    return total\n",
    "\n",
    "def count_words(text, word_list):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return sum(1 for tok in tokens if tok in word_list)\n",
    "\n",
    "def compute_engineered_features(df_combined, language_tool=None):\n",
    "    print(\"Computing engineered features...\")\n",
    "    features = []\n",
    "    for idx, row in df_combined.iterrows():\n",
    "        essay = row['full_text']\n",
    "        feats = {}\n",
    "        feats['char_count'] = len(essay)\n",
    "        tokens = re.findall(r'\\b\\w+\\b', essay)\n",
    "        feats['token_count'] = len(tokens)\n",
    "        feats['sentence_count'] = sentence_split(essay)\n",
    "        feats['avg_token_length'] = np.mean([len(tok) for tok in tokens]) if tokens else 0\n",
    "        feats['avg_sentence_length_tokens'] = feats['token_count'] / feats['sentence_count'] if feats['sentence_count'] > 0 else 0\n",
    "        feats['discourse_marker_count'] = count_markers(essay, DISCOURSE_MARKERS)\n",
    "        feats['pronoun_count'] = count_words(essay, PRONOUNS)\n",
    "        feats['conjunction_count'] = count_words(essay, CONJUNCTIONS)\n",
    "        try:\n",
    "            feats['flesch_reading_ease'] = textstat.flesch_reading_ease(essay)\n",
    "        except Exception:\n",
    "            feats['flesch_reading_ease'] = 0.0\n",
    "        try:\n",
    "            feats['flesch_kincaid_grade'] = textstat.flesch_kincaid_grade(essay)\n",
    "        except Exception:\n",
    "            feats['flesch_kincaid_grade'] = 0.0\n",
    "        try:\n",
    "            feats['smog_index'] = textstat.smog_index(essay)\n",
    "        except Exception:\n",
    "            feats['smog_index'] = 0.0\n",
    "        try:\n",
    "            feats['readability_grade'] = textstat.text_standard(essay, float_output=True)\n",
    "        except Exception:\n",
    "            feats['readability_grade'] = 0.0\n",
    "        grammar_errors = 0\n",
    "        if language_tool is not None:\n",
    "            try:\n",
    "                matches = language_tool.check(essay)\n",
    "                grammar_errors = len(matches)\n",
    "            except Exception as e:\n",
    "                grammar_errors = 0\n",
    "        feats['grammar_error_count'] = grammar_errors\n",
    "        features.append(feats)\n",
    "        if idx % 2500 == 0 and idx != 0:\n",
    "            print(f\"   Processed {idx}/{len(df_combined)} essays.\")\n",
    "    feats_df = pd.DataFrame(features, index=df_combined.index)\n",
    "    return feats_df.astype(np.float32)\n",
    "\n",
    "def get_engineered_features(train, test, cache_path=ENGINEERED_FEATURES_CACHE):\n",
    "    if os.path.exists(cache_path):\n",
    "        print(\"Loading engineered features from cache...\")\n",
    "        all_feats = load_pickle(cache_path)\n",
    "        feats_train = all_feats.iloc[:len(train)].reset_index(drop=True)\n",
    "        feats_test = all_feats.iloc[len(train):].reset_index(drop=True)\n",
    "        return feats_train, feats_test\n",
    "    else:\n",
    "        lt_tool = None\n",
    "        if language_tool_available:\n",
    "            print(\"Instantiating LanguageTool for grammar checking...\")\n",
    "            try:\n",
    "                lt_tool = language_tool_python.LanguageTool('en-US')\n",
    "            except Exception as ex:\n",
    "                print(\"Failed to instantiate LanguageTool (Java may be missing). Skipping grammar feature.\")\n",
    "                lt_tool = None\n",
    "        dfs = [train[['full_text']].copy(), test[['full_text']].copy()]\n",
    "        combined = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "        feats_df = compute_engineered_features(combined, language_tool=lt_tool)\n",
    "        if lt_tool is not None:\n",
    "            lt_tool.close()\n",
    "        print(\"Applying z-score normalization on engineered features...\")\n",
    "        scaler = StandardScaler()\n",
    "        train_vals = feats_df.iloc[:len(train)].values\n",
    "        scaler.fit(train_vals)\n",
    "        normed = scaler.transform(feats_df.values)\n",
    "        feats_normed = pd.DataFrame(normed, columns=feats_df.columns, index=feats_df.index)\n",
    "        save_pickle(feats_normed, cache_path)\n",
    "        feats_train = feats_normed.iloc[:len(train)].reset_index(drop=True)\n",
    "        feats_test = feats_normed.iloc[len(train):].reset_index(drop=True)\n",
    "        return feats_train, feats_test\n",
    "\n",
    "eng_feats_train, eng_feats_test = get_engineered_features(train_df, test_df, ENGINEERED_FEATURES_CACHE)\n",
    "print(f\"Engineered feature shape (train): {eng_feats_train.shape}, test: {eng_feats_test.shape}\")\n",
    "\n",
    "# ============ TOKENIZATION ================\n",
    "TOKENIZER_NAME = \"microsoft/deberta-v3-base\"\n",
    "MAX_LEN = 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "# ============ DATASET CLASS ================\n",
    "class EssayDataset(Dataset):\n",
    "    def __init__(self, texts, engineered_feats, targets=None):\n",
    "        self.texts = texts.reset_index(drop=True)\n",
    "        self.engineered_feats = engineered_feats.reset_index(drop=True)\n",
    "        self.targets = targets.reset_index(drop=True) if targets is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        feats = torch.tensor(self.engineered_feats.iloc[idx].values, dtype=torch.float32)\n",
    "        item = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        for k in item.keys():\n",
    "            item[k] = item[k].squeeze(0)\n",
    "        item['engineered_feats'] = feats\n",
    "        if self.targets is not None:\n",
    "            item['labels'] = int(self.targets[idx])\n",
    "        return item\n",
    "\n",
    "# ================ 5-FOLD SPLIT & SAMPLING (DEBUG) ================\n",
    "N_CLASSES = 6\n",
    "K_THRESHOLDS = N_CLASSES - 1\n",
    "N_SPLITS = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "folds = np.zeros(len(train_df), dtype=int)\n",
    "for i, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['score'])):\n",
    "    folds[val_idx] = i\n",
    "\n",
    "fold_col = \"fold\"\n",
    "train_df[fold_col] = folds\n",
    "\n",
    "def get_debug_indices(idx_list, debug=False):\n",
    "    if not debug:\n",
    "        return idx_list\n",
    "    n = int(0.1 * len(idx_list))\n",
    "    n = max(n, 1)\n",
    "    np.random.seed(42)\n",
    "    idx_list = np.random.choice(idx_list, size=n, replace=False)\n",
    "    return np.sort(idx_list)\n",
    "\n",
    "def create_dataloaders(dataset, train_idx, valid_idx, args,\n",
    "                       batch_size=8):\n",
    "    train_indices = get_debug_indices(train_idx, args.debug)\n",
    "    valid_indices = get_debug_indices(valid_idx, args.debug)\n",
    "    print(f\"Training samples: {len(train_indices)}, Validation samples: {len(valid_indices)} (debug={args.debug})\")\n",
    "    train_ds = Subset(dataset, train_indices)\n",
    "    valid_ds = Subset(dataset, valid_indices)\n",
    "    drop_last = False\n",
    "    dl_args = dict(num_workers=2, pin_memory=True, drop_last=drop_last)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, **dl_args)\n",
    "    valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, **dl_args)\n",
    "    return train_loader, valid_loader, train_indices, valid_indices\n",
    "\n",
    "# ================ LABEL ORDINAL ENCODING FOR CORAL ===================\n",
    "def build_coral_targets(y, num_classes=N_CLASSES):\n",
    "    K = num_classes - 1\n",
    "    coral_targets = np.zeros((len(y), K), dtype=np.float32)\n",
    "    for i in range(K):\n",
    "        coral_targets[:, i] = (y > (i + 1)).astype(np.float32)\n",
    "    return torch.tensor(coral_targets, dtype=torch.float32)\n",
    "\n",
    "# ================ EFFECTIVE NUMBER CLASS WEIGHTS =================\n",
    "def effective_number_weights(y, num_classes=N_CLASSES, beta=0.999):\n",
    "    # y is 1-based in [1, num_classes]\n",
    "    # For CORAL, binary classifiers for thresholds k=1,...,K (K=num_classes-1)\n",
    "    K = num_classes - 1\n",
    "    threshold_pos_counts = [(y > (k+1)).sum() for k in range(K)]\n",
    "    threshold_neg_counts = [len(y) - threshold_pos_counts[k] for k in range(K)]\n",
    "    # Compute effective number for positive and negative samples for each threshold\n",
    "    eff_num_pos = 1.0 - np.power(beta, threshold_pos_counts)\n",
    "    eff_num_neg = 1.0 - np.power(beta, threshold_neg_counts)\n",
    "    weight_pos = (1.0 - beta) / (eff_num_pos + 1e-8)\n",
    "    weight_neg = (1.0 - beta) / (eff_num_neg + 1e-8)\n",
    "    # Classifier weight = average of pos/neg, then normalized across all K\n",
    "    weights = (weight_pos + weight_neg) / 2.0\n",
    "    weights = weights / weights.sum() * K\n",
    "    return weights.astype(np.float32)  # shape (K,)\n",
    "\n",
    "# =========== CORAL ORDINAL HEAD ============\n",
    "class CoralOrdinalHead(nn.Module):\n",
    "    def __init__(self, in_dim, engineered_dim, n_layers=1, hidden_dim=768, K=K_THRESHOLDS):\n",
    "        super().__init__()\n",
    "        self.fc_eng = nn.Linear(engineered_dim, hidden_dim)\n",
    "        self.head = nn.Linear(hidden_dim * 2, 512)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.out = nn.Linear(512, K)\n",
    "        self.bias = nn.Parameter(torch.zeros(K).float())\n",
    "\n",
    "    def forward(self, x_cls, feats):\n",
    "        eng_out = torch.relu(self.fc_eng(feats))\n",
    "        x = torch.cat([x_cls, eng_out], dim=-1)\n",
    "        x = self.head(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.out(x) + torch.cumsum(self.bias, dim=0)\n",
    "        return logits\n",
    "\n",
    "# ============ MODEL ============\n",
    "class OrdinalEssayModel(nn.Module):\n",
    "    def __init__(self, engineered_dim, hidden_dim=768, K=K_THRESHOLDS):\n",
    "        super().__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(TOKENIZER_NAME)\n",
    "        self.engineered_dim = engineered_dim\n",
    "        self.head = CoralOrdinalHead(in_dim=hidden_dim, engineered_dim=engineered_dim, hidden_dim=hidden_dim, K=K)\n",
    "        self.K = K\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, engineered_feats):\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden = outputs['last_hidden_state'][:, 0, :]\n",
    "        logits = self.head(last_hidden, engineered_feats)\n",
    "        return logits\n",
    "\n",
    "    def freeze_layers(self, n=6):\n",
    "        print(f\"Freezing bottom {n} transformer layers...\")\n",
    "        for name, param in self.transformer.named_parameters():\n",
    "            if 'encoder.layer' in name:\n",
    "                layer_num = int(name.split(\"encoder.layer.\")[1].split(\".\")[0])\n",
    "                if layer_num < n:\n",
    "                    param.requires_grad = False\n",
    "        for param in self.transformer.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_all(self):\n",
    "        print(\"Unfreezing all transformer layers...\")\n",
    "        for param in self.transformer.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "# =========== CORAL LOSS (CB Focal) ==========\n",
    "class CBFocalLoss(nn.Module):\n",
    "    def __init__(self, class_weights, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # logits: (bs, K), targets: (bs, K), weights: (K,)\n",
    "        device = logits.device\n",
    "        weights = self.class_weights.to(device)\n",
    "        if logits.shape[-1] != weights.shape[0]:\n",
    "            raise RuntimeError(f\"CBFocalLoss: logits shape {logits.shape}, weights shape {weights.shape}\")\n",
    "        probas = torch.sigmoid(logits)\n",
    "        pt = torch.where(targets == 1, probas, 1 - probas)\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        focal_weight = (1 - pt) ** self.gamma\n",
    "        loss = focal_weight * ce_loss\n",
    "        loss = loss * weights.view(1, -1)\n",
    "        return loss.mean()\n",
    "\n",
    "# ========== TRAIN/EVAL UTILITIES ============\n",
    "def train_one_epoch(model, train_loader, optimizer, scheduler, criterion, scaler,\n",
    "                    epoch, fp16):\n",
    "    model.train()\n",
    "    loss_meter = []\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attn_mask = batch['attention_mask'].to(device)\n",
    "        eng_feats = batch['engineered_feats'].to(device)\n",
    "        labels = batch['labels']\n",
    "        coral_labels = build_coral_targets(labels.cpu().numpy(), num_classes=N_CLASSES).to(device)\n",
    "        with autocast(enabled=fp16):\n",
    "            logits = model(input_ids, attn_mask, eng_feats)\n",
    "            loss = criterion(logits, coral_labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        loss_meter.append(loss.item())\n",
    "    return float(np.mean(loss_meter))\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, loader, fp16):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    for batch in loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attn_mask = batch['attention_mask'].to(device)\n",
    "        eng_feats = batch['engineered_feats'].to(device)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        with autocast(enabled=fp16):\n",
    "            logits = model(input_ids, attn_mask, eng_feats)\n",
    "        all_logits.append(logits.cpu())\n",
    "        all_labels.append(labels)\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    return all_logits, all_labels\n",
    "\n",
    "def coral_logits_to_scores(logits):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    score = 1 + (probs > 0.5).sum(dim=-1).cpu().numpy()\n",
    "    score = score.astype(np.int64)\n",
    "    return score\n",
    "\n",
    "def coral_logits_to_expected(logits):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    expected = 1 + probs.sum(dim=1)\n",
    "    return expected.cpu().numpy()\n",
    "\n",
    "def save_best_checkpoint(model, best_path, only_model_head=False):\n",
    "    if only_model_head:\n",
    "        state_dict = {k: v for k, v in model.state_dict().items() if \"head\" in k}\n",
    "        torch.save(state_dict, best_path)\n",
    "    else:\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "\n",
    "def load_best_checkpoint(model, best_path, strict=True):\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device), strict=strict)\n",
    "\n",
    "# ========== THRESHOLD OPTIMIZATION ==============\n",
    "def optimize_thresholds(y_true, preds, num_classes=N_CLASSES):\n",
    "    from scipy.optimize import minimize\n",
    "\n",
    "    def apply_thresholds(values, thresholds):\n",
    "        cuts = np.sort(thresholds)\n",
    "        labels = [np.sum(v > cuts) + 1 for v in values]\n",
    "        return np.array(labels)\n",
    "\n",
    "    def neg_qwk(thresholds):\n",
    "        y_pred = apply_thresholds(preds, thresholds)\n",
    "        return -qwk_metric(y_true, y_pred)\n",
    "\n",
    "    initial = np.percentile(preds, np.linspace(5, 95, num_classes-1))\n",
    "    bounds = []\n",
    "    minv, maxv = preds.min(), preds.max()\n",
    "    for _ in range(num_classes-1):\n",
    "        bounds.append((minv, maxv))\n",
    "    best = minimize(neg_qwk, initial, method='Powell', bounds=bounds, options={'maxiter':1000})\n",
    "    thresholds = np.sort(best.x)\n",
    "    return thresholds\n",
    "\n",
    "def apply_thresholds_batch(values, thresholds):\n",
    "    cuts = np.sort(thresholds)\n",
    "    return (np.add.reduce([values > t for t in cuts], axis=0) + 1).astype(np.int64)\n",
    "\n",
    "# ========== TRAINING LOOP (SINGLE FOLD) =============\n",
    "def train_one_fold(fold, train_df, feats_train, args,\n",
    "                   batch_size=8, epochs=5,\n",
    "                   patience=2, K=K_THRESHOLDS, lr=2e-5, weight_decay=0.01,\n",
    "                   beta=0.999, gamma=2.0):\n",
    "    print(f\"Starting fold {fold}...\")\n",
    "    set_global_seed(42)\n",
    "    train_idx = np.where(train_df[fold_col] != fold)[0]\n",
    "    valid_idx = np.where(train_df[fold_col] == fold)[0]\n",
    "\n",
    "    train_targets = train_df.loc[train_idx, 'score'].reset_index(drop=True)\n",
    "    valid_targets = train_df.loc[valid_idx, 'score'].reset_index(drop=True)\n",
    "\n",
    "    full_dataset = EssayDataset(\n",
    "        texts=train_df['full_text'], \n",
    "        engineered_feats=feats_train, \n",
    "        targets=train_df['score']\n",
    "    )\n",
    "    train_loader, valid_loader, real_train_idx, real_valid_idx = create_dataloaders(\n",
    "        full_dataset, train_idx, valid_idx, args, batch_size=batch_size)\n",
    "\n",
    "    y_labels = train_targets.values\n",
    "    class_weights = effective_number_weights(y_labels, num_classes=N_CLASSES, beta=beta)\n",
    "    print(f\"CORAL (threshold) class weights: {class_weights} (len={len(class_weights)})\")\n",
    "\n",
    "    model = OrdinalEssayModel(\n",
    "        engineered_dim=feats_train.shape[1], hidden_dim=768, K=K\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    model.freeze_layers(n=6)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = CBFocalLoss(class_weights=class_weights, gamma=gamma)\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    num_train_steps = steps_per_epoch * epochs\n",
    "    warmup_steps = int(num_train_steps * 0.1)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_train_steps\n",
    "    )\n",
    "    scaler = GradScaler(enabled=FP16)\n",
    "\n",
    "    best_qwk = -float('inf')\n",
    "    best_epoch = -1\n",
    "    best_path = f\"best_fold{fold}.pt\"\n",
    "    patience_counter = 0\n",
    "    start_time = time.time()\n",
    "    real_epochs = (1 if args.debug else epochs)\n",
    "\n",
    "    for ep in range(real_epochs):\n",
    "        if ep == 1:\n",
    "            model.unfreeze_all()\n",
    "        print(f\"Epoch {ep+1}/{real_epochs}\")\n",
    "        ep_start = time.time()\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, criterion, scaler, ep, FP16)\n",
    "        print(f\"Train loss: {train_loss:.4f}\")\n",
    "        logits, val_labels = eval_model(model, valid_loader, FP16)\n",
    "        pred_scores = coral_logits_to_scores(logits)\n",
    "        qwk = qwk_metric(val_labels, pred_scores)\n",
    "        print(f\"Epoch {ep+1} QWK: {qwk:.5f}\")\n",
    "        if qwk > best_qwk:\n",
    "            best_qwk = qwk\n",
    "            best_epoch = ep\n",
    "            save_best_checkpoint(model, best_path)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience and ep + 1 >= min(2, real_epochs):\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "        print(f\"Epoch time: {time.time()-ep_start:.1f}s\")\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f\"Best valid QWK: {best_qwk:.5f} at epoch {best_epoch+1}.\")\n",
    "    load_best_checkpoint(model, best_path)\n",
    "    return model, valid_loader, best_qwk, valid_idx, best_path\n",
    "\n",
    "# =============== THRESHOLD HOOK + EVALUATION/SCORES.CSV =============\n",
    "def save_scores_csv(model_name, qwk_value):\n",
    "    df = pd.DataFrame(\n",
    "        {'Quadratic Weighted Kappa': [qwk_value, qwk_value]},\n",
    "        index=[model_name, \"ensemble\"]\n",
    "    )\n",
    "    df.to_csv('scores.csv', encoding='utf-8')\n",
    "\n",
    "# =============== TEST SET PREDICTION AND SUBMISSION ===============\n",
    "def generate_submission(essay_ids, pred_scores):\n",
    "    df_sub = pd.DataFrame({'essay_id': essay_ids, 'score': pred_scores})\n",
    "    df_sub['score'] = df_sub['score'].astype(np.int64)\n",
    "    df_sub.to_csv('submission.csv', index=False, encoding='utf-8')\n",
    "    print(f\"submission.csv generated ({len(df_sub)} rows).\")\n",
    "\n",
    "# ============== MAIN EXECUTION ======================\n",
    "if __name__ == \"__main__\":\n",
    "    timer0 = time.time()\n",
    "    FOLD = 0\n",
    "    batch_size = 8\n",
    "    epochs = 5\n",
    "    patience = 2\n",
    "\n",
    "    model, valid_loader, best_qwk, valid_idx, best_path = train_one_fold(\n",
    "        FOLD, train_df, eng_feats_train, args=args, batch_size=batch_size, epochs=epochs, patience=patience)\n",
    "\n",
    "    print(\"Optimizing thresholds on validation set...\")\n",
    "    model.eval()\n",
    "    logits, val_labels = eval_model(model, valid_loader, FP16)\n",
    "    expected_vals = coral_logits_to_expected(logits)\n",
    "    opt_thresholds = optimize_thresholds(val_labels, expected_vals, num_classes=N_CLASSES)\n",
    "    print(f\"Optimized thresholds: {opt_thresholds}\")\n",
    "\n",
    "    preds_final = apply_thresholds_batch(expected_vals, opt_thresholds)\n",
    "    final_qwk = qwk_metric(val_labels, preds_final)\n",
    "    print(f\"Final validation QWK after threshold optimization: {final_qwk:.5f}\")\n",
    "\n",
    "    save_scores_csv(model_name=\"deberta_v3_coral\", qwk_value=final_qwk)\n",
    "\n",
    "    print(\"Predicting on test set...\")\n",
    "    test_dataset = EssayDataset(\n",
    "        texts=test_df['full_text'],\n",
    "        engineered_feats=eng_feats_test,\n",
    "        targets=None\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    all_logits = []\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attn_mask = batch['attention_mask'].to(device)\n",
    "        eng_feats = batch['engineered_feats'].to(device)\n",
    "        with torch.no_grad():\n",
    "            with autocast(enabled=FP16):\n",
    "                logits = model(input_ids, attn_mask, eng_feats)\n",
    "        all_logits.append(logits.cpu())\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    expected_test = coral_logits_to_expected(all_logits)\n",
    "    pred_scores = apply_thresholds_batch(expected_test, opt_thresholds)\n",
    "    generate_submission(test_df['essay_id'], pred_scores)\n",
    "    timer1 = time.time()\n",
    "\n",
    "    if args.debug:\n",
    "        debug_time = timer1 - timer0\n",
    "        est_epochs = 5\n",
    "        scale = (1.0/0.1) * (est_epochs/1)\n",
    "        estimated_time = debug_time * scale\n",
    "        print(\"=== Start of Debug Information ===\")\n",
    "        print(f\"debug_time: {debug_time:.2f}\")\n",
    "        print(f\"estimated_time: {estimated_time:.2f}\")\n",
    "        print(\"=== End of Debug Information ===\")\n",
    "```\n",
    "\n",
    "### Hypothesis for the experiment\n",
    "the experiment is designed based on hypothesis: Target Problem Name: Ordinal-aware modelling & thresholding\n",
    "Target Problem: Design an ordinal-aware modelling and inference pipeline (e.g., ordinal regression head or cumulative link loss) that outputs monotonic score probabilities and converts them to integer labels using validation-tuned thresholds.\n",
    "Chosen Component: Model\n",
    "Hypothesis: Fine-tune a pretrained DeBERTa-v3-base encoder with a CORAL (cumulative link) ordinal regression head that outputs five monotonic logits P(score>k). Convert these probabilities into the final 1-6 integer scores using validation-optimised thresholds that maximise QWK.\n",
    "Reason: Current baselines for essay scoring often treat the task as plain 6-way classification, ignoring the ordered nature of the labels, which can hurt QWK. An explicitly ordinal approach should align the optimisation objective with the evaluation metric.\n",
    "\n",
    "### Results\n",
    "\n",
    "Evaluated results on validation are:\n",
    "                  Quadratic Weighted Kappa\n",
    "deberta_v3_coral                  0.829588\n",
    "ensemble                          0.829588\n",
    "\n",
    "Submission format check result is:\n",
    "Submission is valid.\n",
    "Total running time: 1.638 seconds.\n",
    "\n",
    "Running time: 3314.4708676338196 seconds\n",
    "\n",
    "# Identified Challenges\n",
    "## 1. Capture full essay context beyond 512 tokens\n",
    "Enable the model to ingest the entire essay (or a substantially larger portion) using long-context transformers, hierarchical chunk aggregation, or sliding-window ensembling instead of simple 512-token truncation.\n",
    "\n",
    "## 2. Mitigate severe high-score class imbalance\n",
    "Design specialised strategies (e.g., focal-CB loss tuned per threshold, minority-focused sampling, ordinal margin penalties, or calibrated posterior mapping) to improve recall and calibration for the rare top-score essays.\n",
    "\n",
    "## 3. Inject argumentative & discourse quality features\n",
    "Incorporate domain-specific features such as argument-component detection, discourse relational counts, grammar/fluency error density, and vocabulary richness into the model, possibly via multi-task learning or feature fusion.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d9d5b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"\n",
    "You are a Kaggle Grandmaster and expert ML engineer with deep expertise in statistics, machine learning, and competition optimization.\n",
    "The user is iteratively improving a Kaggle competition implementation. Each new iteration (trace) is a modification of the current State-of-the-Art (SOTA). If a new trace surpasses the current SOTA, it becomes the new SOTA. Otherwise, it's a failed experiment.\n",
    "You will be provided with:\n",
    "1. A detailed competition scenario description.\n",
    "2. A history of previous successfully experiments and their associated feedbacks, indexed or ordered from oldest to newest; the latest SOTA experiment accumulates all the improvements from the previous successful experiments.\n",
    "3. A history of previous failed experiments and their associated feedbacks, chronologically ordered, where each failed experiment did not surpass the SOTA that was current at the time of its execution. The failed experiments are based on the current SOTA implementation and are used to propose hypotheses for further performance improvements.\n",
    "4. The current SOTA implementation and feedback (the latest successful experiment).\n",
    "5. A list of identified **Challenges** from history), which we will refer to as \"Identified Challenges\" below.\n",
    "\n",
    "Your task is to perform two main steps:\n",
    "1. **Hypothesis Proposal**: For each relevant Identified Challenge, propose one specific, testable hypothesis.\n",
    "2. **Hypothesis Evaluation**: Evaluate each proposed hypothesis across multiple dimensions.\n",
    "\n",
    "# Task 1: Hypothesis Proposal\n",
    "First note that the user might provide a list of challenges containing duplicates. You should only propose one hypothesis for each unique challenge. If a challenge is a duplicate of a previous one, you can skip it.\n",
    "For each Identified Challenge, propose one hypothesis corresponding to the Challenge, aimed at improving the current SOTA implementation or establishing a robust initial SOTA.\n",
    "\n",
    "## 1.1. Steps to Hypothesize\n",
    "Follow these steps to formulate effective hypotheses:\n",
    "\n",
    "1. **Understanding the Challenge**:\n",
    "  - Analyze the Identified Challenge to understand its root cause and potential impact on the competition's target metric or successful execution.\n",
    "  - If the Challenge stems from past experiments (SOTA or failed), review the specifics of those experiments to ensure the proposed hypothesis offers a novel, more effective, or correctly implemented solution.\n",
    "  - If the Challenge relates to persistent problems from failed experiments (e.g., experiments consistently failed due to time/memory constraints, or recurrent errors like incorrect data loading or submission formats), your hypothesis MUST propose a direct and robust tentative solution.\n",
    "2. **Drafting the First Implementation (if no SOTA exists)**:\n",
    "  - If there is no SOTA implementation yet (i.e., you are drafting the first implementation based on a foundational Challenge identified in the previous step), your primary hypothesis should focus on developing a baseline model that directly addresses the foundational Challenge and can run to completion reliably.\n",
    "  - This initial hypothesis should define the core data processing, feature engineering, model choice, and submission generation steps in a clear and executable way. Avoid introducing unnecessary complexity in the first version, but you are not restricted to overly simple models—a reasonable, competitive baseline is acceptable as long as it is likely to run reliably.\n",
    "3. **Actionable Changes**:\n",
    "  - If a Challenge involves underperforming models (e.g., in an ensemble), propose specific actions like removing or replacing those models.\n",
    "  - If a Challenge relates to hyperparameter tuning, recommend a specific method or strategy (e.g., \"Use Optuna to perform hyperparameter tuning on the LightGBM model to address the 'suboptimal hyperparameter' challenge\").\n",
    "  - If a Challenge points to data loading, preprocessing, or submission format errors, the hypothesis must detail the exact changes required to rectify these issues.\n",
    "\n",
    "## 1.2. Guidelines for Writing Hypotheses\n",
    "\n",
    "1. **Be Specific and Decisive**:\n",
    "  - Clearly state the exact, unambiguous change(s) being proposed. Avoid vague goals like \"improve the model\" or \"optimize the pipeline.\"\n",
    "  - The hypothesis must propose a single, clear course of action. Do not suggest alternatives (e.g., \"try method A or method B\").\n",
    "  - The hypothesis statement must be direct and definitive, without phrases like \"for example,\" \"e.g.,\" or \"might involve.\"\n",
    "  - The hypothesis must be more informative and decisive than the Challenge it addresses. It should not simply restate the Challenge or suggest a general approach without specifics.\n",
    "2. **Ensure Testability and Actionability**:\n",
    "  - The hypothesis must describe an action or change that can be practically implemented and tested.\n",
    "  - If the hypothesis is about improving SOTA, it should clearly state the expected improvement, typically related to a measurable performance metric or successful execution.\n",
    "  - If the hypothesis is about establishing the first solution, it should clearly outline the expected outcome -- RUNNABILITY and CORRECTNESS. Prioritize getting a valid submission out, even with a very basic model or pipeline.\n",
    "3. **Align with Current SOTA and Identified Challenges**:\n",
    "  - The hypothesis must be directly relevant to improving the *current* State-of-the-Art (SOTA) implementation or establishing a new SOTA if none exists.\n",
    "  - It must directly address one of the `Identified Challenges` provided as input.\n",
    "4. **Maintain Singular Focus within Hypothesis**:\n",
    "  - If a hypothesis involves multiple adjustments, these must be tightly correlated and contribute to a single, unified conceptual change addressing the core of the Identified Challenge.\n",
    "  - Avoid bundling multiple independent or unrelated ideas into a single hypothesis. Each hypothesis should test one core concept.\n",
    "5. **Address the Overall Pipeline (for Pipeline-Focused Tasks)**:\n",
    "  - The hypothesis should address improvements to the end-to-end pipeline.\n",
    "  - It can propose coordinated changes across multiple parts of the SOTA implementation if these are necessary to achieve a significant pipeline-level improvement to address the Challenge. (Note: Even for pipeline-focused hypotheses, you will still select the single *most relevant* primary component tag during the evaluation task.)\n",
    "\n",
    "# Task 2: Hypothesis Evaluation\n",
    "After proposing one hypothesis for each relevant Identified Challenge, evaluate each one.\n",
    "\n",
    "## 2.1. Evaluation Instruction\n",
    "For each individual hypothesis you proposed in Task 1, perform the following two evaluation steps:\n",
    "\n",
    "1. **Assign a Component Tag:** Assign a single component tag to the hypothesis. Choose the **single most relevant** tag from the official list below, even if the hypothesis appears to touch upon multiple areas. Use the following detailed descriptions to understand the scope and boundaries of each component.\n",
    "\n",
    "  - **`DataLoadSpec`**: Responsible for loading raw competition data, ensuring data is converted to the correct types, and potentially providing an initial exploratory data analysis (EDA) summary. (e.g., fixing `zipfile.BadZipFile` by improving loading logic).\n",
    "  - **`FeatureEng`**: Focuses on transforming raw data into meaningful features suitable for model consumption. Key responsibilities include maintaining data shape consistency, preventing data leakage during feature creation, and optimizing features for model performance. Feature engineering should be model-agnostic.\n",
    "  - **`Model`**: Involves model building (developing new models to address the problem), model tuning (optimizing existing models for better performance), or model removal. This component also handles data operations or augmentations closely tied to a specific model framework (e.g., PyTorch `Datasets` & `DataLoaders`, TensorFlow `tf.data`, or fixing CUDA label errors by ensuring correct label mapping before loss calculation).\n",
    "  - **`Ensemble`**: Combines predictions from multiple models using various ensemble strategies.\n",
    "  - **`Workflow`**: Integrates all pipeline components, orchestrating the flow from data loading through to final output generation (e.g., correcting `submission.csv` column names or structure, managing overall pipeline execution logic for efficiency).\n",
    "\n",
    "2. **Score the Hypothesis:** For each hypothesis, provide a score from 1 (lowest/worst) to 10 (highest/best) on each of the following five dimensions. Base your scores on all provided information.\n",
    "  - **Challenge-Hypothesis Alignment (Score: 1-10):** How directly and effectively does the hypothesis address the core issues of the `Identified Challenge` it targets? A higher score means a stronger, more direct alignment.\n",
    "  - **Expected Impact (Score: 1-10):** What is the estimated magnitude of improvement (e.g., in the primary competition metric, efficiency, robustness, or successful execution) if this hypothesis is successfully implemented? Higher scores for greater positive impact.\n",
    "  - **Novelty (Score: 1-10):** How innovative or original is this hypothesis when compared to the approaches and ideas evident in the `previous SOTA experiments` and `previous failed experiments`? Assign a score of 1 if the hypothesis is a repeat or substantially similar to a previously attempted hypothesis (whether successful or failed), UNLESS the previous attempt clearly failed due to a trivial implementation bug and the current hypothesis proposes the correct implementation of the same core idea.\n",
    "  - **Feasibility (Score: 1-10):** How easily and practically can this hypothesis be implemented and *run to completion* within the existing SOTA codebase and operational constraints (e.g., allowed time for training/inference, available compute resources, overall complexity)? Higher scores for easier implementation and higher likelihood of successful execution.\n",
    "  - **Risk-Reward Balance (Score: 1-10):** Considering the potential for significant improvement (reward) versus the probability of failure, negative side-effects, or excessive resource consumption (risk), how optimal is this balance? A high score indicates a favorable balance.\n",
    "  - **Prioritization for Critical Challenges:** If a hypothesis directly and credibly addresses a **critical Challenge that caused prior experiment failures** (e.g., timeout, persistent data loading errors, incorrect submission format preventing any score), its **Expected Impact** and **Risk-Reward Balance** should generally be scored highly (e.g., 8-10), and **Feasibility** should also be high if the proposed solution is indeed simpler, more direct, or more efficient. This ensures such critical hypotheses are prioritized.\n",
    "\n",
    "  Please extract at least 6 deduplicated challenges, and generate one hypothesis for each.\n",
    "  \n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4815a939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/userdata/v-lijingyuan/anaconda3/envs/rdagent/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/data/userdata/v-lijingyuan/anaconda3/envs/rdagent/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Optional, Tuple\n",
    "from hashlib import md5\n",
    "from rdagent.scenarios.data_science.proposal.exp_gen.base import DSHypothesis\n",
    "# 假设 DSHypothesis 是一个 dataclass，你需要确保它定义在别处\n",
    "# from your_module import DSHypothesis\n",
    "\n",
    "def md5_hash(s: str) -> str:\n",
    "    return md5(s.encode('utf-8')).hexdigest()\n",
    "\n",
    "\n",
    "def compute_top_scores(hypothesis_dict: dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute weighted total scores for each hypothesis and return the top five.\n",
    "    \"\"\"\n",
    "    weights = {\n",
    "        \"alignment_score\": 0.2,\n",
    "        \"impact_score\": 0.4,\n",
    "        \"novelty_score\": 0.2,\n",
    "        \"feasibility_score\": 0.1,\n",
    "        \"risk_reward_balance_score\": 0.1,\n",
    "    }\n",
    "    scores_dict = {}\n",
    "    for problem_name in hypothesis_dict:\n",
    "        if \"hypothesis\" not in hypothesis_dict[problem_name]:\n",
    "            continue\n",
    "        scores_dict[problem_name] = {}\n",
    "        for score_key in weights:\n",
    "            if score_key not in hypothesis_dict[problem_name].get(\"evaluation\", {}):\n",
    "                scores_dict[problem_name][score_key] = 0\n",
    "            else:\n",
    "                try:\n",
    "                    scores_dict[problem_name][score_key] = (\n",
    "                        float(hypothesis_dict[problem_name][\"evaluation\"][score_key]) * weights[score_key]\n",
    "                    )\n",
    "                except (ValueError, TypeError):\n",
    "                    scores_dict[problem_name][score_key] = 0\n",
    "\n",
    "    scores = pd.DataFrame(scores_dict)\n",
    "    scores_sorted = scores.sum().sort_values(ascending=False)\n",
    "    return scores_sorted[:5]\n",
    "\n",
    "\n",
    "def select_hypothesis(\n",
    "    scores_sorted: pd.Series,\n",
    "    hypothesis_dict: dict,\n",
    "    problem_dict: dict,\n",
    "    scen_prob_multiplier: int = 2\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    From the top five hypotheses (by weighted score), select one based on additional weighting rules.\n",
    "    \"\"\"\n",
    "    index_to_pick_pool_list = []\n",
    "    for j, problem_name in enumerate(scores_sorted.index):\n",
    "        if hypothesis_dict[problem_name].get(\"inspired\", False):\n",
    "            index_to_pick_pool_list.extend([j] * 2)\n",
    "        if problem_dict[problem_name][\"label\"] == \"SCENARIO_PROBLEM\":\n",
    "            index_to_pick_pool_list.extend([j] * scen_prob_multiplier)\n",
    "        elif problem_dict[problem_name][\"label\"] == \"FEEDBACK_PROBLEM\":\n",
    "            index_to_pick_pool_list.extend([j] * (3 - scen_prob_multiplier))\n",
    "        else:\n",
    "            index_to_pick_pool_list.extend([j] * 1)\n",
    "\n",
    "    reproducible_int = int.from_bytes(\n",
    "        bytes.fromhex(md5_hash(scores_sorted.to_string())), byteorder=\"big\"\n",
    "    ) % len(index_to_pick_pool_list)\n",
    "\n",
    "    return index_to_pick_pool_list[reproducible_int]\n",
    "\n",
    "\n",
    "def hypothesis_rank(\n",
    "    hypothesis_dict: dict,\n",
    "    problem_dict: dict,\n",
    "    selected_idx: Optional[int] = None,\n",
    "    scen_prob_multiplier: int = 2\n",
    ") -> Tuple[str, \"DSHypothesis\"]:\n",
    "    \"\"\"\n",
    "    Wrapper method that computes the top five hypotheses by weighted scoring and then selects one.\n",
    "    \"\"\"\n",
    "    scores_sorted = compute_top_scores(hypothesis_dict)\n",
    "    if selected_idx is None:\n",
    "        selected_idx = select_hypothesis(\n",
    "            scores_sorted=scores_sorted,\n",
    "            hypothesis_dict=hypothesis_dict,\n",
    "            problem_dict=problem_dict,\n",
    "            scen_prob_multiplier=scen_prob_multiplier\n",
    "        )\n",
    "\n",
    "    max_score_problem_name = scores_sorted.index[selected_idx]\n",
    "    problem_info = problem_dict.get(max_score_problem_name, {})\n",
    "\n",
    "    return max_score_problem_name, DSHypothesis(\n",
    "        component=hypothesis_dict[max_score_problem_name].get(\"component\", \"Model\"),\n",
    "        hypothesis=hypothesis_dict[max_score_problem_name].get(\"hypothesis\", \"Hypothesis not provided\"),\n",
    "        reason=hypothesis_dict[max_score_problem_name].get(\"reason\", \"Reason not provided\"),\n",
    "        problem_name=max_score_problem_name,\n",
    "        problem_desc=problem_info.get(\"problem\", \"Problem description not provided\"),\n",
    "        problem_label=problem_info.get(\"label\", \"FEEDBACK_PROBLEM\"),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1439c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02ead4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-17 10:04:16.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrdagent.oai.backend.litellm\u001b[0m:\u001b[36m_create_chat_completion_inner_function\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96msystem\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96m\n",
      "You are a Kaggle Grandmaster and expert ML engineer with deep expertise in statistics, machine learning, and competition optimization.\n",
      "The user is iteratively improving a Kaggle competition implementation. Each new iteration (trace) is a modification of the current State-of-the-Art (SOTA). If a new trace surpasses the current SOTA, it becomes the new SOTA. Otherwise, it's a failed experiment.\n",
      "You will be provided with:\n",
      "1. A detailed competition scenario description.\n",
      "2. A history of previous successfully experiments and their associated feedbacks, indexed or ordered from oldest to newest; the latest SOTA experiment accumulates all the improvements from the previous successful experiments.\n",
      "3. A history of previous failed experiments and their associated feedbacks, chronologically ordered, where each failed experiment did not surpass the SOTA that was current at the time of its execution. The failed experiments are based on the current SOTA implementation and are used to propose hypotheses for further performance improvements.\n",
      "4. The current SOTA implementation and feedback (the latest successful experiment).\n",
      "5. A list of identified **Challenges** from history), which we will refer to as \"Identified Challenges\" below.\n",
      "\n",
      "Your task is to perform two main steps:\n",
      "1. **Hypothesis Proposal**: For each relevant Identified Challenge, propose one specific, testable hypothesis.\n",
      "2. **Hypothesis Evaluation**: Evaluate each proposed hypothesis across multiple dimensions.\n",
      "\n",
      "# Task 1: Hypothesis Proposal\n",
      "First note that the user might provide a list of challenges containing duplicates. You should only propose one hypothesis for each unique challenge. If a challenge is a duplicate of a previous one, you can skip it.\n",
      "For each Identified Challenge, propose one hypothesis corresponding to the Challenge, aimed at improving the current SOTA implementation or establishing a robust initial SOTA.\n",
      "\n",
      "## 1.1. Steps to Hypothesize\n",
      "Follow these steps to formulate effective hypotheses:\n",
      "\n",
      "1. **Understanding the Challenge**:\n",
      "  - Analyze the Identified Challenge to understand its root cause and potential impact on the competition's target metric or successful execution.\n",
      "  - If the Challenge stems from past experiments (SOTA or failed), review the specifics of those experiments to ensure the proposed hypothesis offers a novel, more effective, or correctly implemented solution.\n",
      "  - If the Challenge relates to persistent problems from failed experiments (e.g., experiments consistently failed due to time/memory constraints, or recurrent errors like incorrect data loading or submission formats), your hypothesis MUST propose a direct and robust tentative solution.\n",
      "2. **Drafting the First Implementation (if no SOTA exists)**:\n",
      "  - If there is no SOTA implementation yet (i.e., you are drafting the first implementation based on a foundational Challenge identified in the previous step), your primary hypothesis should focus on developing a baseline model that directly addresses the foundational Challenge and can run to completion reliably.\n",
      "  - This initial hypothesis should define the core data processing, feature engineering, model choice, and submission generation steps in a clear and executable way. Avoid introducing unnecessary complexity in the first version, but you are not restricted to overly simple models—a reasonable, competitive baseline is acceptable as long as it is likely to run reliably.\n",
      "3. **Actionable Changes**:\n",
      "  - If a Challenge involves underperforming models (e.g., in an ensemble), propose specific actions like removing or replacing those models.\n",
      "  - If a Challenge relates to hyperparameter tuning, recommend a specific method or strategy (e.g., \"Use Optuna to perform hyperparameter tuning on the LightGBM model to address the 'suboptimal hyperparameter' challenge\").\n",
      "  - If a Challenge points to data loading, preprocessing, or submission format errors, the hypothesis must detail the exact changes required to rectify these issues.\n",
      "\n",
      "## 1.2. Guidelines for Writing Hypotheses\n",
      "\n",
      "1. **Be Specific and Decisive**:\n",
      "  - Clearly state the exact, unambiguous change(s) being proposed. Avoid vague goals like \"improve the model\" or \"optimize the pipeline.\"\n",
      "  - The hypothesis must propose a single, clear course of action. Do not suggest alternatives (e.g., \"try method A or method B\").\n",
      "  - The hypothesis statement must be direct and definitive, without phrases like \"for example,\" \"e.g.,\" or \"might involve.\"\n",
      "  - The hypothesis must be more informative and decisive than the Challenge it addresses. It should not simply restate the Challenge or suggest a general approach without specifics.\n",
      "2. **Ensure Testability and Actionability**:\n",
      "  - The hypothesis must describe an action or change that can be practically implemented and tested.\n",
      "  - If the hypothesis is about improving SOTA, it should clearly state the expected improvement, typically related to a measurable performance metric or successful execution.\n",
      "  - If the hypothesis is about establishing the first solution, it should clearly outline the expected outcome -- RUNNABILITY and CORRECTNESS. Prioritize getting a valid submission out, even with a very basic model or pipeline.\n",
      "3. **Align with Current SOTA and Identified Challenges**:\n",
      "  - The hypothesis must be directly relevant to improving the *current* State-of-the-Art (SOTA) implementation or establishing a new SOTA if none exists.\n",
      "  - It must directly address one of the `Identified Challenges` provided as input.\n",
      "4. **Maintain Singular Focus within Hypothesis**:\n",
      "  - If a hypothesis involves multiple adjustments, these must be tightly correlated and contribute to a single, unified conceptual change addressing the core of the Identified Challenge.\n",
      "  - Avoid bundling multiple independent or unrelated ideas into a single hypothesis. Each hypothesis should test one core concept.\n",
      "5. **Address the Overall Pipeline (for Pipeline-Focused Tasks)**:\n",
      "  - The hypothesis should address improvements to the end-to-end pipeline.\n",
      "  - It can propose coordinated changes across multiple parts of the SOTA implementation if these are necessary to achieve a significant pipeline-level improvement to address the Challenge. (Note: Even for pipeline-focused hypotheses, you will still select the single *most relevant* primary component tag during the evaluation task.)\n",
      "\n",
      "# Task 2: Hypothesis Evaluation\n",
      "After proposing one hypothesis for each relevant Identified Challenge, evaluate each one.\n",
      "\n",
      "## 2.1. Evaluation Instruction\n",
      "For each individual hypothesis you proposed in Task 1, perform the following two evaluation steps:\n",
      "\n",
      "1. **Assign a Component Tag:** Assign a single component tag to the hypothesis. Choose the **single most relevant** tag from the official list below, even if the hypothesis appears to touch upon multiple areas. Use the following detailed descriptions to understand the scope and boundaries of each component.\n",
      "\n",
      "  - **`DataLoadSpec`**: Responsible for loading raw competition data, ensuring data is converted to the correct types, and potentially providing an initial exploratory data analysis (EDA) summary. (e.g., fixing `zipfile.BadZipFile` by improving loading logic).\n",
      "  - **`FeatureEng`**: Focuses on transforming raw data into meaningful features suitable for model consumption. Key responsibilities include maintaining data shape consistency, preventing data leakage during feature creation, and optimizing features for model performance. Feature engineering should be model-agnostic.\n",
      "  - **`Model`**: Involves model building (developing new models to address the problem), model tuning (optimizing existing models for better performance), or model removal. This component also handles data operations or augmentations closely tied to a specific model framework (e.g., PyTorch `Datasets` & `DataLoaders`, TensorFlow `tf.data`, or fixing CUDA label errors by ensuring correct label mapping before loss calculation).\n",
      "  - **`Ensemble`**: Combines predictions from multiple models using various ensemble strategies.\n",
      "  - **`Workflow`**: Integrates all pipeline components, orchestrating the flow from data loading through to final output generation (e.g., correcting `submission.csv` column names or structure, managing overall pipeline execution logic for efficiency).\n",
      "\n",
      "2. **Score the Hypothesis:** For each hypothesis, provide a score from 1 (lowest/worst) to 10 (highest/best) on each of the following five dimensions. Base your scores on all provided information.\n",
      "  - **Challenge-Hypothesis Alignment (Score: 1-10):** How directly and effectively does the hypothesis address the core issues of the `Identified Challenge` it targets? A higher score means a stronger, more direct alignment.\n",
      "  - **Expected Impact (Score: 1-10):** What is the estimated magnitude of improvement (e.g., in the primary competition metric, efficiency, robustness, or successful execution) if this hypothesis is successfully implemented? Higher scores for greater positive impact.\n",
      "  - **Novelty (Score: 1-10):** How innovative or original is this hypothesis when compared to the approaches and ideas evident in the `previous SOTA experiments` and `previous failed experiments`? Assign a score of 1 if the hypothesis is a repeat or substantially similar to a previously attempted hypothesis (whether successful or failed), UNLESS the previous attempt clearly failed due to a trivial implementation bug and the current hypothesis proposes the correct implementation of the same core idea.\n",
      "  - **Feasibility (Score: 1-10):** How easily and practically can this hypothesis be implemented and *run to completion* within the existing SOTA codebase and operational constraints (e.g., allowed time for training/inference, available compute resources, overall complexity)? Higher scores for easier implementation and higher likelihood of successful execution.\n",
      "  - **Risk-Reward Balance (Score: 1-10):** Considering the potential for significant improvement (reward) versus the probability of failure, negative side-effects, or excessive resource consumption (risk), how optimal is this balance? A high score indicates a favorable balance.\n",
      "  - **Prioritization for Critical Challenges:** If a hypothesis directly and credibly addresses a **critical Challenge that caused prior experiment failures** (e.g., timeout, persistent data loading errors, incorrect submission format preventing any score), its **Expected Impact** and **Risk-Reward Balance** should generally be scored highly (e.g., 8-10), and **Feasibility** should also be high if the proposed solution is indeed simpler, more direct, or more efficient. This ensures such critical hypotheses are prioritized.\n",
      "\n",
      "  Please extract at least 6 deduplicated challenges, and generate one hypothesis for each.\n",
      "  \n",
      "  \u001b[0m\n",
      "\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96muser\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96m\n",
      "# Scenario Description\n",
      "====== Background ======\n",
      "You are a world-class data scientist and machine learning engineer with deep expertise in statistics, mathematics, and computer science. \n",
      "Your knowledge spans cutting-edge data analysis techniques, advanced machine learning algorithms, and their practical applications to solve complex real-world problems.\n",
      "You are dedicated to producing accurate, efficient, and innovative solutions.\n",
      "\n",
      "The task type for this competition is **Classification (Ordinal classification of essay scores 1–6)**.\n",
      "The data type used in this competition is **Text (Natural Language Processing)**.\n",
      "\n",
      "Briefly, the competition involves: Participants must build an automated essay–scoring model that assigns a holistic score from 1 to 6 to each student-written argumentative essay, reducing the need for manual grading..\n",
      "\n",
      "The dataset used in this competition is:\n",
      "Processed data consist of three CSV files.\n",
      "1. train.csv – 15,576 rows × 3 columns: essay_id (string), full_text (the complete essay), score (integer 1-6).\n",
      "2. test.csv – 1,731 rows × 2 columns: essay_id, full_text (no score column).\n",
      "3. sample_submission.csv – 1,731 rows × 2 columns matching the required submission format (essay_id, score) with sample integer scores.\n",
      "These files come from a larger corpus (~24 000 essays) described in the competition overview. The processed version keeps only the essential columns and is already un-zipped and ready for modeling; no other folders or files are present..\n",
      "\n",
      "Submission channel number to each sample is: 1.\n",
      "\n",
      "The evaluation metric of this competition is:\n",
      "Leaderboard ranking is based on Quadratic Weighted Kappa (QWK), which measures agreement between the true scores and the submitted scores.\n",
      " 1. Let N = number of distinct score levels (here 6).\n",
      " 2. Build an N×N observed matrix O where O_{i,j} counts essays with true label i and predicted label j.\n",
      " 3. Build an N×N weight matrix w with w_{i,j} = ((i−j)^2)/(N−1)^2.\n",
      " 4. Build an N×N expected matrix E as the outer product of the true-label histogram and the predicted-label histogram, scaled so that sum(E)=sum(O).\n",
      " 5. QWK = 1 − ( Σ_{i,j} w_{i,j} O_{i,j} ) / ( Σ_{i,j} w_{i,j} E_{i,j} ).\n",
      "The metric ranges from –∞ to 1; higher values indicate better agreement..\n",
      "\n",
      "The following is the output of the exploratory data analysis (EDA) performed on the dataset, You should carefully analyze it to better craft your feature engineering and model training strategies.\n",
      "====== Data Overview (EDA) ======\n",
      "\n",
      "Train data shape: (15576, 3)\n",
      "Test data shape: (1731, 2)\n",
      "Train first 5 rows:\n",
      "essay_id                                          full_text  score\n",
      "0  663d2cf  Dear State Senator,\n",
      "\n",
      "I am arguing in favor o...      3\n",
      "1  3a20bfb  In \" The Challenge of Exploring Venus\" The aut...      2\n",
      "2  6adae64  Teachers can have a hard time telling if their...      3\n",
      "3  c81ccdf  Using dirverless cars can be very dangrous in ...      3\n",
      "4  9549d7f  In \" The Challenge of Exploring Venus\" the aut...      3\n",
      "Test first 5 rows:\n",
      "essay_id                                          full_text\n",
      "0  d550b2d  The face was not created by aliens because the...\n",
      "1  0c10954  Hello my name is Luke Bomberger and I was seag...\n",
      "2  ef04816  The technology to read the emotional expressio...\n",
      "3  88ab8d9  Do you ever wonder how many soomething is or h...\n",
      "4  cee8c0f  This story is about discovering mars. Witch is...\n",
      "Train data types:\n",
      "essay_id     object\n",
      "full_text    object\n",
      "score         int64\n",
      "dtype: object\n",
      "Test data types:\n",
      "essay_id     object\n",
      "full_text    object\n",
      "dtype: object\n",
      "Train missing values per column:\n",
      "essay_id     0\n",
      "full_text    0\n",
      "score        0\n",
      "dtype: int64\n",
      "Test missing values per column:\n",
      "essay_id     0\n",
      "full_text    0\n",
      "dtype: int64\n",
      "Train unique values per column:\n",
      "essay_id: 15576\n",
      "full_text: 15576\n",
      "score: 6\n",
      "Test unique values per column:\n",
      "essay_id: 1731\n",
      "full_text: 1731\n",
      "Target variable (score) distribution:\n",
      "score\n",
      "1    1124\n",
      "2    4249\n",
      "3    5629\n",
      "4    3563\n",
      "5     876\n",
      "6     135\n",
      "Name: count, dtype: int64\n",
      "Train essay lengths (chars): min=712 max=20459 mean=2073.5 std=930.1\n",
      "\n",
      "====== Submission Format ======\n",
      "Please ensure your submission adheres to the following specifications:\n",
      "Create a UTF-8 CSV named submission.csv with a header and exactly two columns:\n",
      "  • essay_id – copied from test.csv\n",
      "  • score – your predicted integer score (1–6) for each essay\n",
      "The rows must appear for every essay_id in the test set; order is not enforced but recommended to match the sample format.\n",
      "\n",
      "====== Important Guidelines ======\n",
      "Before submitting your results, please note the following:\n",
      "- We have numerous tests in place to check your code.\n",
      "- Ensure your submission is genuine.\n",
      "- Do not manipulate data or return values solely to pass preliminary tests, as this will not lead to successful final evaluation.\n",
      "\n",
      "====== Evaluation ======\n",
      "\n",
      "The primary evaluation metric for this task is: **Quadratic Weighted Kappa**.\n",
      "\n",
      "This metric is considered better when it is **larger**.\n",
      "\n",
      "Additional Evaluation Details:\n",
      "Leaderboard ranking is based on Quadratic Weighted Kappa (QWK), which measures agreement between the true scores and the submitted scores.\n",
      " 1. Let N = number of distinct score levels (here 6).\n",
      " 2. Build an N×N observed matrix O where O_{i,j} counts essays with true label i and predicted label j.\n",
      " 3. Build an N×N weight matrix w with w_{i,j} = ((i−j)^2)/(N−1)^2.\n",
      " 4. Build an N×N expected matrix E as the outer product of the true-label histogram and the predicted-label histogram, scaled so that sum(E)=sum(O).\n",
      " 5. QWK = 1 − ( Σ_{i,j} w_{i,j} O_{i,j} ) / ( Σ_{i,j} w_{i,j} E_{i,j} ).\n",
      "The metric ranges from –∞ to 1; higher values indicate better agreement.\n",
      "\n",
      "====== Time Limit ======\n",
      "Your code's execution is limited to ** 5.00 hours**. After this time limit, your code will be terminated. But remember your main target is to achieve the best performance and you have several times to modify your code. So please be bold to make the best use of all the time limit and don't be too conservative.\n",
      "During this time limit, you have all the resources available to you. Please fully leverage all the computational resources(CPUs and GPUs) to achieve the best performance like choose a powerful model, use a large batch size, enable data sampler with big parallel.\n",
      "\n",
      "# Previous Experiments and Feedbacks\n",
      "## Experiment Index: 1\n",
      "Target Problem: Design an ordinal-aware modelling and inference pipeline (e.g., ordinal regression head or cumulative link loss) that outputs monotonic score probabilities and converts them to integer labels using validation-tuned thresholds.\n",
      "\n",
      "Proposed Hypothesis: Fine-tune a pretrained DeBERTa-v3-base encoder with a CORAL (cumulative link) ordinal regression head that outputs five monotonic logits P(score>k). Convert these probabilities into the final 1-6 integer scores using validation-optimised thresholds that maximise QWK.\n",
      "Code Change Summary: Entire pipeline built from scratch: (1) load and validate CSVs, (2) extensive engineered linguistic features with caching, (3) DeBERTa-v3-base encoder combined with CORAL ordinal head that concatenates CLS embedding with engineered features, (4) class-balanced focal CORAL loss, (5) 5-fold infra but only fold-0 trained, (6) validation-based Powell optimisation of continuous thresholds, (7) test inference with same pipeline, (8) generation of scores.csv and submission.csv following required format, (9) mixed-precision, selective layer freezing, LanguageTool integration, and graceful resource handling.\n",
      "Surpass Previous SOTA: True\n",
      "\n",
      "Experiment Score: 0.8295875060477553\n",
      "Experiment Feedback: [Experiment Analysis] Submission format is correct and evaluation matches official QWK; predictions are generated with identical logic for validation and test, avoiding leakage. As the first valid run, the 0.8296 QWK establishes the initial best result to beat.\n",
      "\n",
      "# Current SOTA Implementation\n",
      "## Best of previous exploration of the scenario\n",
      "\n",
      "### Code\n",
      "Here is the complete code of the solution.\n",
      "\n",
      "File Path: main.py\n",
      "```\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "import random\n",
      "import pickle\n",
      "import argparse\n",
      "import gc\n",
      "import re\n",
      "import importlib\n",
      "import subprocess\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import Dataset, DataLoader, Subset\n",
      "from torch.cuda.amp import autocast, GradScaler\n",
      "\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.metrics import cohen_kappa_score\n",
      "\n",
      "# ============ DEPENDENCY MANAGEMENT ============\n",
      "def pip_install(package, module=None, version=None):\n",
      "    pkg_string = f\"{package}=={version}\" if version else package\n",
      "    try:\n",
      "        return importlib.import_module(module or package)\n",
      "    except ImportError:\n",
      "        print(f\"Installing {pkg_string}...\")\n",
      "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", pkg_string])\n",
      "        return importlib.import_module(module or package)\n",
      "\n",
      "# textstat\n",
      "try:\n",
      "    import textstat\n",
      "except ImportError:\n",
      "    textstat = pip_install(\"textstat\")\n",
      "\n",
      "# language_tool_python & Java check\n",
      "def check_java_installed():\n",
      "    from shutil import which\n",
      "    if which(\"java\") is None:\n",
      "        print(\"WARNING: Java is not installed. Skipping grammar-based features.\")\n",
      "        return False\n",
      "    return True\n",
      "\n",
      "language_tool_available = False\n",
      "try:\n",
      "    language_tool_python = importlib.import_module(\"language_tool_python\")\n",
      "    language_tool_available = check_java_installed()\n",
      "except ImportError:\n",
      "    try:\n",
      "        language_tool_python = pip_install(\"language-tool-python\", \"language_tool_python\")\n",
      "        language_tool_available = check_java_installed()\n",
      "    except Exception:\n",
      "        print(\"WARNING: language-tool-python could not be installed. Skipping grammar-based features.\")\n",
      "        language_tool_available = False\n",
      "\n",
      "# transformers (required)\n",
      "try:\n",
      "    from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
      "except ImportError:\n",
      "    pip_install(\"transformers\", version=\"4.44.2\")\n",
      "    pip_install(\"accelerate\", version=\"0.33.0\")\n",
      "    from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
      "\n",
      "# ============ GLOBAL SEEDS & UTILS ===============\n",
      "def set_global_seed(seed: int = 42):\n",
      "    random.seed(seed)\n",
      "    np.random.seed(seed)\n",
      "    torch.manual_seed(seed)\n",
      "    torch.cuda.manual_seed_all(seed)\n",
      "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
      "\n",
      "set_global_seed(42)\n",
      "\n",
      "def save_pickle(obj, path):\n",
      "    with open(path, 'wb') as f:\n",
      "        pickle.dump(obj, f)\n",
      "\n",
      "def load_pickle(path):\n",
      "    with open(path, 'rb') as f:\n",
      "        return pickle.load(f)\n",
      "\n",
      "def qwk_metric(y_true, y_pred, min_rating=1, max_rating=6):\n",
      "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
      "\n",
      "# ============ FP16 & CUDA DETECTION ==========\n",
      "def get_device():\n",
      "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "    return torch.device(device)\n",
      "\n",
      "device = get_device()\n",
      "print(f\"Using device: {device}\")\n",
      "\n",
      "FP16 = torch.cuda.is_available()  # Enable fp16 if possible\n",
      "\n",
      "# ========== PATHS ==========\n",
      "DATA_DIR = \"./workspace_input/\"\n",
      "TRAIN_PATH = os.path.join(DATA_DIR, \"train.csv\")\n",
      "TEST_PATH = os.path.join(DATA_DIR, \"test.csv\")\n",
      "SAMPLE_SUBMISSION_PATH = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
      "ENGINEERED_FEATURES_CACHE = os.path.join(DATA_DIR, \"eng_feats_cache.pkl\")\n",
      "\n",
      "# ============ ARGPARSE FOR DEBUG MODE =========\n",
      "parser = argparse.ArgumentParser()\n",
      "parser.add_argument('--debug', action='store_true', default=False)\n",
      "args = parser.parse_args()\n",
      "\n",
      "# ============ DATA LOADING ===============\n",
      "def load_dataset():\n",
      "    print(\"Loading data...\")\n",
      "    train = pd.read_csv(TRAIN_PATH, encoding='utf-8')\n",
      "    test = pd.read_csv(TEST_PATH, encoding='utf-8')\n",
      "    expected_train_cols = ['essay_id', 'full_text', 'score']\n",
      "    expected_test_cols = ['essay_id', 'full_text']\n",
      "    # Minimal: ensure presence, don't fail on order\n",
      "    for col in expected_train_cols:\n",
      "        if col not in train.columns:\n",
      "            raise ValueError(f\"Train columns missing: expected {col}\")\n",
      "    for col in expected_test_cols:\n",
      "        if col not in test.columns:\n",
      "            raise ValueError(f\"Test columns missing: expected {col}\")\n",
      "    train['full_text'] = train['full_text'].astype(str)\n",
      "    test['full_text'] = test['full_text'].astype(str)\n",
      "    train['score'] = train['score'].astype('int64')\n",
      "    print(f\"Train shape: {train.shape}, Test shape: {test.shape}\")\n",
      "    return train, test\n",
      "\n",
      "train_df, test_df = load_dataset()\n",
      "\n",
      "# =========== EDA SECTION ==============\n",
      "def print_eda(train, test):\n",
      "    content = []\n",
      "    content.append(f\"Train data shape: {train.shape}\")\n",
      "    content.append(f\"Test data shape: {test.shape}\")\n",
      "    content.append(\"\n",
      "Train first 5 rows:\n",
      "\" + str(train.head()))\n",
      "    content.append(\"\n",
      "Test first 5 rows:\n",
      "\" + str(test.head()))\n",
      "    content.append(\"\n",
      "Train data types:\n",
      "\" + str(train.dtypes))\n",
      "    content.append(\"\n",
      "Test data types:\n",
      "\" + str(test.dtypes))\n",
      "    content.append(\"\n",
      "Train missing values per column:\n",
      "\" + str(train.isnull().sum()))\n",
      "    content.append(\"\n",
      "Test missing values per column:\n",
      "\" + str(test.isnull().sum()))\n",
      "    content.append(\"\n",
      "Train unique values per column:\")\n",
      "    for col in train.columns:\n",
      "        content.append(f\"{col}: {train[col].nunique()}\")\n",
      "    content.append(\"\n",
      "Test unique values per column:\")\n",
      "    for col in test.columns:\n",
      "        content.append(f\"{col}: {test[col].nunique()}\")\n",
      "    if 'score' in train.columns:\n",
      "        content.append(\"\n",
      "Target variable (score) distribution:\")\n",
      "        content.append(str(train['score'].value_counts(sort=False).sort_index()))\n",
      "    else:\n",
      "        content.append(\"\n",
      "No target variable found in train set.\")\n",
      "    if 'full_text' in train.columns:\n",
      "        lens = train['full_text'].str.len()\n",
      "        content.append(f\"\n",
      "Train essay lengths (chars): min={lens.min()} max={lens.max()} mean={lens.mean():.1f} std={lens.std():.1f}\")\n",
      "    return \"\n",
      "\".join(content)\n",
      "\n",
      "eda_output = print_eda(train_df, test_df)\n",
      "print(\"=== Start of EDA part ===\")\n",
      "print(eda_output)\n",
      "print(\"=== End of EDA part ===\")\n",
      "\n",
      "# ========== FEATURE ENGINEERING ===================\n",
      "DISCOURSE_MARKERS = [\n",
      "    \"however\", \"therefore\", \"moreover\", \"furthermore\", \"for example\", \"for instance\",\n",
      "    \"in addition\", \"besides\", \"nonetheless\", \"consequently\", \"as a result\", \"thus\",\n",
      "    \"otherwise\", \"hence\", \"in conclusion\", \"in summary\", \"nevertheless\", \"on the other hand\",\n",
      "    \"similarly\", \"in contrast\", \"undoubtedly\", \"admittedly\", \"certainly\", \"obviously\",\n",
      "    \"in fact\", \"of course\", \"after all\", \"in short\", \"for this reason\", \"as such\",\n",
      "    \"accordingly\", \"and yet\", \"despite\", \"to sum up\", \"even though\", \"because\",\n",
      "    \"since\", \"although\", \"though\", \"meanwhile\", \"unless\", \"whereas\", \"when\", \"while\",\n",
      "    \"before\", \"after\", \"until\", \"whether\", \"once\"\n",
      "]\n",
      "PRONOUNS = [\n",
      "    \"i\", \"me\", \"my\", \"mine\", \"myself\", \"we\", \"us\", \"our\", \"ours\", \"ourselves\",\n",
      "    \"you\", \"your\", \"yours\", \"yourself\", \"he\", \"him\", \"his\", \"himself\",\n",
      "    \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
      "    \"they\", \"them\", \"their\", \"theirs\", \"themselves\"\n",
      "]\n",
      "CONJUNCTIONS = [\n",
      "    \"and\", \"but\", \"or\", \"nor\", \"for\", \"yet\", \"so\", \"although\", \"because\",\n",
      "    \"since\", \"unless\", \"while\", \"whereas\", \"even though\", \"though\"\n",
      "]\n",
      "\n",
      "def sentence_split(text):\n",
      "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
      "    num_sentences = sum(1 for s in sentences if s.strip())\n",
      "    return num_sentences\n",
      "\n",
      "def count_markers(text, markers):\n",
      "    t = text.lower()\n",
      "    total = 0\n",
      "    for word in markers:\n",
      "        total += t.count(word)\n",
      "    return total\n",
      "\n",
      "def count_words(text, word_list):\n",
      "    tokens = re.findall(r\\w', text.lower())\n",
      "    return sum(1 for tok in tokens if tok in word_list)\n",
      "\n",
      "def compute_engineered_features(df_combined, language_tool=None):\n",
      "    print(\"Computing engineered features...\")\n",
      "    features = []\n",
      "    for idx, row in df_combined.iterrows():\n",
      "        essay = row['full_text']\n",
      "        feats = {}\n",
      "        feats['char_count'] = len(essay)\n",
      "        tokens = re.findall(r\\w', essay)\n",
      "        feats['token_count'] = len(tokens)\n",
      "        feats['sentence_count'] = sentence_split(essay)\n",
      "        feats['avg_token_length'] = np.mean([len(tok) for tok in tokens]) if tokens else 0\n",
      "        feats['avg_sentence_length_tokens'] = feats['token_count'] / feats['sentence_count'] if feats['sentence_count'] > 0 else 0\n",
      "        feats['discourse_marker_count'] = count_markers(essay, DISCOURSE_MARKERS)\n",
      "        feats['pronoun_count'] = count_words(essay, PRONOUNS)\n",
      "        feats['conjunction_count'] = count_words(essay, CONJUNCTIONS)\n",
      "        try:\n",
      "            feats['flesch_reading_ease'] = textstat.flesch_reading_ease(essay)\n",
      "        except Exception:\n",
      "            feats['flesch_reading_ease'] = 0.0\n",
      "        try:\n",
      "            feats['flesch_kincaid_grade'] = textstat.flesch_kincaid_grade(essay)\n",
      "        except Exception:\n",
      "            feats['flesch_kincaid_grade'] = 0.0\n",
      "        try:\n",
      "            feats['smog_index'] = textstat.smog_index(essay)\n",
      "        except Exception:\n",
      "            feats['smog_index'] = 0.0\n",
      "        try:\n",
      "            feats['readability_grade'] = textstat.text_standard(essay, float_output=True)\n",
      "        except Exception:\n",
      "            feats['readability_grade'] = 0.0\n",
      "        grammar_errors = 0\n",
      "        if language_tool is not None:\n",
      "            try:\n",
      "                matches = language_tool.check(essay)\n",
      "                grammar_errors = len(matches)\n",
      "            except Exception as e:\n",
      "                grammar_errors = 0\n",
      "        feats['grammar_error_count'] = grammar_errors\n",
      "        features.append(feats)\n",
      "        if idx % 2500 == 0 and idx != 0:\n",
      "            print(f\"   Processed {idx}/{len(df_combined)} essays.\")\n",
      "    feats_df = pd.DataFrame(features, index=df_combined.index)\n",
      "    return feats_df.astype(np.float32)\n",
      "\n",
      "def get_engineered_features(train, test, cache_path=ENGINEERED_FEATURES_CACHE):\n",
      "    if os.path.exists(cache_path):\n",
      "        print(\"Loading engineered features from cache...\")\n",
      "        all_feats = load_pickle(cache_path)\n",
      "        feats_train = all_feats.iloc[:len(train)].reset_index(drop=True)\n",
      "        feats_test = all_feats.iloc[len(train):].reset_index(drop=True)\n",
      "        return feats_train, feats_test\n",
      "    else:\n",
      "        lt_tool = None\n",
      "        if language_tool_available:\n",
      "            print(\"Instantiating LanguageTool for grammar checking...\")\n",
      "            try:\n",
      "                lt_tool = language_tool_python.LanguageTool('en-US')\n",
      "            except Exception as ex:\n",
      "                print(\"Failed to instantiate LanguageTool (Java may be missing). Skipping grammar feature.\")\n",
      "                lt_tool = None\n",
      "        dfs = [train[['full_text']].copy(), test[['full_text']].copy()]\n",
      "        combined = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
      "        feats_df = compute_engineered_features(combined, language_tool=lt_tool)\n",
      "        if lt_tool is not None:\n",
      "            lt_tool.close()\n",
      "        print(\"Applying z-score normalization on engineered features...\")\n",
      "        scaler = StandardScaler()\n",
      "        train_vals = feats_df.iloc[:len(train)].values\n",
      "        scaler.fit(train_vals)\n",
      "        normed = scaler.transform(feats_df.values)\n",
      "        feats_normed = pd.DataFrame(normed, columns=feats_df.columns, index=feats_df.index)\n",
      "        save_pickle(feats_normed, cache_path)\n",
      "        feats_train = feats_normed.iloc[:len(train)].reset_index(drop=True)\n",
      "        feats_test = feats_normed.iloc[len(train):].reset_index(drop=True)\n",
      "        return feats_train, feats_test\n",
      "\n",
      "eng_feats_train, eng_feats_test = get_engineered_features(train_df, test_df, ENGINEERED_FEATURES_CACHE)\n",
      "print(f\"Engineered feature shape (train): {eng_feats_train.shape}, test: {eng_feats_test.shape}\")\n",
      "\n",
      "# ============ TOKENIZATION ================\n",
      "TOKENIZER_NAME = \"microsoft/deberta-v3-base\"\n",
      "MAX_LEN = 512\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
      "\n",
      "# ============ DATASET CLASS ================\n",
      "class EssayDataset(Dataset):\n",
      "    def __init__(self, texts, engineered_feats, targets=None):\n",
      "        self.texts = texts.reset_index(drop=True)\n",
      "        self.engineered_feats = engineered_feats.reset_index(drop=True)\n",
      "        self.targets = targets.reset_index(drop=True) if targets is not None else None\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.texts)\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        text = self.texts[idx]\n",
      "        feats = torch.tensor(self.engineered_feats.iloc[idx].values, dtype=torch.float32)\n",
      "        item = tokenizer(\n",
      "            text,\n",
      "            padding='max_length',\n",
      "            truncation=True,\n",
      "            max_length=MAX_LEN,\n",
      "            return_tensors='pt'\n",
      "        )\n",
      "        for k in item.keys():\n",
      "            item[k] = item[k].squeeze(0)\n",
      "        item['engineered_feats'] = feats\n",
      "        if self.targets is not None:\n",
      "            item['labels'] = int(self.targets[idx])\n",
      "        return item\n",
      "\n",
      "# ================ 5-FOLD SPLIT & SAMPLING (DEBUG) ================\n",
      "N_CLASSES = 6\n",
      "K_THRESHOLDS = N_CLASSES - 1\n",
      "N_SPLITS = 5\n",
      "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
      "folds = np.zeros(len(train_df), dtype=int)\n",
      "for i, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['score'])):\n",
      "    folds[val_idx] = i\n",
      "\n",
      "fold_col = \"fold\"\n",
      "train_df[fold_col] = folds\n",
      "\n",
      "def get_debug_indices(idx_list, debug=False):\n",
      "    if not debug:\n",
      "        return idx_list\n",
      "    n = int(0.1 * len(idx_list))\n",
      "    n = max(n, 1)\n",
      "    np.random.seed(42)\n",
      "    idx_list = np.random.choice(idx_list, size=n, replace=False)\n",
      "    return np.sort(idx_list)\n",
      "\n",
      "def create_dataloaders(dataset, train_idx, valid_idx, args,\n",
      "                       batch_size=8):\n",
      "    train_indices = get_debug_indices(train_idx, args.debug)\n",
      "    valid_indices = get_debug_indices(valid_idx, args.debug)\n",
      "    print(f\"Training samples: {len(train_indices)}, Validation samples: {len(valid_indices)} (debug={args.debug})\")\n",
      "    train_ds = Subset(dataset, train_indices)\n",
      "    valid_ds = Subset(dataset, valid_indices)\n",
      "    drop_last = False\n",
      "    dl_args = dict(num_workers=2, pin_memory=True, drop_last=drop_last)\n",
      "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, **dl_args)\n",
      "    valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, **dl_args)\n",
      "    return train_loader, valid_loader, train_indices, valid_indices\n",
      "\n",
      "# ================ LABEL ORDINAL ENCODING FOR CORAL ===================\n",
      "def build_coral_targets(y, num_classes=N_CLASSES):\n",
      "    K = num_classes - 1\n",
      "    coral_targets = np.zeros((len(y), K), dtype=np.float32)\n",
      "    for i in range(K):\n",
      "        coral_targets[:, i] = (y > (i + 1)).astype(np.float32)\n",
      "    return torch.tensor(coral_targets, dtype=torch.float32)\n",
      "\n",
      "# ================ EFFECTIVE NUMBER CLASS WEIGHTS =================\n",
      "def effective_number_weights(y, num_classes=N_CLASSES, beta=0.999):\n",
      "    # y is 1-based in [1, num_classes]\n",
      "    # For CORAL, binary classifiers for thresholds k=1,...,K (K=num_classes-1)\n",
      "    K = num_classes - 1\n",
      "    threshold_pos_counts = [(y > (k+1)).sum() for k in range(K)]\n",
      "    threshold_neg_counts = [len(y) - threshold_pos_counts[k] for k in range(K)]\n",
      "    # Compute effective number for positive and negative samples for each threshold\n",
      "    eff_num_pos = 1.0 - np.power(beta, threshold_pos_counts)\n",
      "    eff_num_neg = 1.0 - np.power(beta, threshold_neg_counts)\n",
      "    weight_pos = (1.0 - beta) / (eff_num_pos + 1e-8)\n",
      "    weight_neg = (1.0 - beta) / (eff_num_neg + 1e-8)\n",
      "    # Classifier weight = average of pos/neg, then normalized across all K\n",
      "    weights = (weight_pos + weight_neg) / 2.0\n",
      "    weights = weights / weights.sum() * K\n",
      "    return weights.astype(np.float32)  # shape (K,)\n",
      "\n",
      "# =========== CORAL ORDINAL HEAD ============\n",
      "class CoralOrdinalHead(nn.Module):\n",
      "    def __init__(self, in_dim, engineered_dim, n_layers=1, hidden_dim=768, K=K_THRESHOLDS):\n",
      "        super().__init__()\n",
      "        self.fc_eng = nn.Linear(engineered_dim, hidden_dim)\n",
      "        self.head = nn.Linear(hidden_dim * 2, 512)\n",
      "        self.dropout = nn.Dropout(0.2)\n",
      "        self.out = nn.Linear(512, K)\n",
      "        self.bias = nn.Parameter(torch.zeros(K).float())\n",
      "\n",
      "    def forward(self, x_cls, feats):\n",
      "        eng_out = torch.relu(self.fc_eng(feats))\n",
      "        x = torch.cat([x_cls, eng_out], dim=-1)\n",
      "        x = self.head(x)\n",
      "        x = self.dropout(x)\n",
      "        logits = self.out(x) + torch.cumsum(self.bias, dim=0)\n",
      "        return logits\n",
      "\n",
      "# ============ MODEL ============\n",
      "class OrdinalEssayModel(nn.Module):\n",
      "    def __init__(self, engineered_dim, hidden_dim=768, K=K_THRESHOLDS):\n",
      "        super().__init__()\n",
      "        self.transformer = AutoModel.from_pretrained(TOKENIZER_NAME)\n",
      "        self.engineered_dim = engineered_dim\n",
      "        self.head = CoralOrdinalHead(in_dim=hidden_dim, engineered_dim=engineered_dim, hidden_dim=hidden_dim, K=K)\n",
      "        self.K = K\n",
      "\n",
      "    def forward(self, input_ids, attention_mask, engineered_feats):\n",
      "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
      "        last_hidden = outputs['last_hidden_state'][:, 0, :]\n",
      "        logits = self.head(last_hidden, engineered_feats)\n",
      "        return logits\n",
      "\n",
      "    def freeze_layers(self, n=6):\n",
      "        print(f\"Freezing bottom {n} transformer layers...\")\n",
      "        for name, param in self.transformer.named_parameters():\n",
      "            if 'encoder.layer' in name:\n",
      "                layer_num = int(name.split(\"encoder.layer.\")[1].split(\".\")[0])\n",
      "                if layer_num < n:\n",
      "                    param.requires_grad = False\n",
      "        for param in self.transformer.embeddings.parameters():\n",
      "            param.requires_grad = False\n",
      "\n",
      "    def unfreeze_all(self):\n",
      "        print(\"Unfreezing all transformer layers...\")\n",
      "        for param in self.transformer.parameters():\n",
      "            param.requires_grad = True\n",
      "\n",
      "# =========== CORAL LOSS (CB Focal) ==========\n",
      "class CBFocalLoss(nn.Module):\n",
      "    def __init__(self, class_weights, gamma=2.0):\n",
      "        super().__init__()\n",
      "        self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
      "        self.gamma = gamma\n",
      "\n",
      "    def forward(self, logits, targets):\n",
      "        # logits: (bs, K), targets: (bs, K), weights: (K,)\n",
      "        device = logits.device\n",
      "        weights = self.class_weights.to(device)\n",
      "        if logits.shape[-1] != weights.shape[0]:\n",
      "            raise RuntimeError(f\"CBFocalLoss: logits shape {logits.shape}, weights shape {weights.shape}\")\n",
      "        probas = torch.sigmoid(logits)\n",
      "        pt = torch.where(targets == 1, probas, 1 - probas)\n",
      "        ce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
      "        focal_weight = (1 - pt) ** self.gamma\n",
      "        loss = focal_weight * ce_loss\n",
      "        loss = loss * weights.view(1, -1)\n",
      "        return loss.mean()\n",
      "\n",
      "# ========== TRAIN/EVAL UTILITIES ============\n",
      "def train_one_epoch(model, train_loader, optimizer, scheduler, criterion, scaler,\n",
      "                    epoch, fp16):\n",
      "    model.train()\n",
      "    loss_meter = []\n",
      "    for batch in train_loader:\n",
      "        optimizer.zero_grad()\n",
      "        input_ids = batch['input_ids'].to(device)\n",
      "        attn_mask = batch['attention_mask'].to(device)\n",
      "        eng_feats = batch['engineered_feats'].to(device)\n",
      "        labels = batch['labels']\n",
      "        coral_labels = build_coral_targets(labels.cpu().numpy(), num_classes=N_CLASSES).to(device)\n",
      "        with autocast(enabled=fp16):\n",
      "            logits = model(input_ids, attn_mask, eng_feats)\n",
      "            loss = criterion(logits, coral_labels)\n",
      "        scaler.scale(loss).backward()\n",
      "        scaler.step(optimizer)\n",
      "        scaler.update()\n",
      "        scheduler.step()\n",
      "        loss_meter.append(loss.item())\n",
      "    return float(np.mean(loss_meter))\n",
      "\n",
      "@torch.no_grad()\n",
      "def eval_model(model, loader, fp16):\n",
      "    model.eval()\n",
      "    all_logits = []\n",
      "    all_labels = []\n",
      "    for batch in loader:\n",
      "        input_ids = batch['input_ids'].to(device)\n",
      "        attn_mask = batch['attention_mask'].to(device)\n",
      "        eng_feats = batch['engineered_feats'].to(device)\n",
      "        labels = batch['labels'].cpu().numpy()\n",
      "        with autocast(enabled=fp16):\n",
      "            logits = model(input_ids, attn_mask, eng_feats)\n",
      "        all_logits.append(logits.cpu())\n",
      "        all_labels.append(labels)\n",
      "    all_logits = torch.cat(all_logits, dim=0)\n",
      "    all_labels = np.concatenate(all_labels, axis=0)\n",
      "    return all_logits, all_labels\n",
      "\n",
      "def coral_logits_to_scores(logits):\n",
      "    probs = torch.sigmoid(logits)\n",
      "    score = 1 + (probs > 0.5).sum(dim=-1).cpu().numpy()\n",
      "    score = score.astype(np.int64)\n",
      "    return score\n",
      "\n",
      "def coral_logits_to_expected(logits):\n",
      "    probs = torch.sigmoid(logits)\n",
      "    expected = 1 + probs.sum(dim=1)\n",
      "    return expected.cpu().numpy()\n",
      "\n",
      "def save_best_checkpoint(model, best_path, only_model_head=False):\n",
      "    if only_model_head:\n",
      "        state_dict = {k: v for k, v in model.state_dict().items() if \"head\" in k}\n",
      "        torch.save(state_dict, best_path)\n",
      "    else:\n",
      "        torch.save(model.state_dict(), best_path)\n",
      "\n",
      "def load_best_checkpoint(model, best_path, strict=True):\n",
      "    model.load_state_dict(torch.load(best_path, map_location=device), strict=strict)\n",
      "\n",
      "# ========== THRESHOLD OPTIMIZATION ==============\n",
      "def optimize_thresholds(y_true, preds, num_classes=N_CLASSES):\n",
      "    from scipy.optimize import minimize\n",
      "\n",
      "    def apply_thresholds(values, thresholds):\n",
      "        cuts = np.sort(thresholds)\n",
      "        labels = [np.sum(v > cuts) + 1 for v in values]\n",
      "        return np.array(labels)\n",
      "\n",
      "    def neg_qwk(thresholds):\n",
      "        y_pred = apply_thresholds(preds, thresholds)\n",
      "        return -qwk_metric(y_true, y_pred)\n",
      "\n",
      "    initial = np.percentile(preds, np.linspace(5, 95, num_classes-1))\n",
      "    bounds = []\n",
      "    minv, maxv = preds.min(), preds.max()\n",
      "    for _ in range(num_classes-1):\n",
      "        bounds.append((minv, maxv))\n",
      "    best = minimize(neg_qwk, initial, method='Powell', bounds=bounds, options={'maxiter':1000})\n",
      "    thresholds = np.sort(best.x)\n",
      "    return thresholds\n",
      "\n",
      "def apply_thresholds_batch(values, thresholds):\n",
      "    cuts = np.sort(thresholds)\n",
      "    return (np.add.reduce([values > t for t in cuts], axis=0) + 1).astype(np.int64)\n",
      "\n",
      "# ========== TRAINING LOOP (SINGLE FOLD) =============\n",
      "def train_one_fold(fold, train_df, feats_train, args,\n",
      "                   batch_size=8, epochs=5,\n",
      "                   patience=2, K=K_THRESHOLDS, lr=2e-5, weight_decay=0.01,\n",
      "                   beta=0.999, gamma=2.0):\n",
      "    print(f\"Starting fold {fold}...\")\n",
      "    set_global_seed(42)\n",
      "    train_idx = np.where(train_df[fold_col] != fold)[0]\n",
      "    valid_idx = np.where(train_df[fold_col] == fold)[0]\n",
      "\n",
      "    train_targets = train_df.loc[train_idx, 'score'].reset_index(drop=True)\n",
      "    valid_targets = train_df.loc[valid_idx, 'score'].reset_index(drop=True)\n",
      "\n",
      "    full_dataset = EssayDataset(\n",
      "        texts=train_df['full_text'], \n",
      "        engineered_feats=feats_train, \n",
      "        targets=train_df['score']\n",
      "    )\n",
      "    train_loader, valid_loader, real_train_idx, real_valid_idx = create_dataloaders(\n",
      "        full_dataset, train_idx, valid_idx, args, batch_size=batch_size)\n",
      "\n",
      "    y_labels = train_targets.values\n",
      "    class_weights = effective_number_weights(y_labels, num_classes=N_CLASSES, beta=beta)\n",
      "    print(f\"CORAL (threshold) class weights: {class_weights} (len={len(class_weights)})\")\n",
      "\n",
      "    model = OrdinalEssayModel(\n",
      "        engineered_dim=feats_train.shape[1], hidden_dim=768, K=K\n",
      "    )\n",
      "    model = model.to(device)\n",
      "    model.freeze_layers(n=6)\n",
      "\n",
      "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
      "    criterion = CBFocalLoss(class_weights=class_weights, gamma=gamma)\n",
      "    steps_per_epoch = len(train_loader)\n",
      "    num_train_steps = steps_per_epoch * epochs\n",
      "    warmup_steps = int(num_train_steps * 0.1)\n",
      "    scheduler = get_linear_schedule_with_warmup(\n",
      "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_train_steps\n",
      "    )\n",
      "    scaler = GradScaler(enabled=FP16)\n",
      "\n",
      "    best_qwk = -float('inf')\n",
      "    best_epoch = -1\n",
      "    best_path = f\"best_fold{fold}.pt\"\n",
      "    patience_counter = 0\n",
      "    start_time = time.time()\n",
      "    real_epochs = (1 if args.debug else epochs)\n",
      "\n",
      "    for ep in range(real_epochs):\n",
      "        if ep == 1:\n",
      "            model.unfreeze_all()\n",
      "        print(f\"Epoch {ep+1}/{real_epochs}\")\n",
      "        ep_start = time.time()\n",
      "        train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, criterion, scaler, ep, FP16)\n",
      "        print(f\"Train loss: {train_loss:.4f}\")\n",
      "        logits, val_labels = eval_model(model, valid_loader, FP16)\n",
      "        pred_scores = coral_logits_to_scores(logits)\n",
      "        qwk = qwk_metric(val_labels, pred_scores)\n",
      "        print(f\"Epoch {ep+1} QWK: {qwk:.5f}\")\n",
      "        if qwk > best_qwk:\n",
      "            best_qwk = qwk\n",
      "            best_epoch = ep\n",
      "            save_best_checkpoint(model, best_path)\n",
      "            patience_counter = 0\n",
      "        else:\n",
      "            patience_counter += 1\n",
      "        if patience_counter >= patience and ep + 1 >= min(2, real_epochs):\n",
      "            print(\"Early stopping.\")\n",
      "            break\n",
      "        print(f\"Epoch time: {time.time()-ep_start:.1f}s\")\n",
      "        gc.collect()\n",
      "        torch.cuda.empty_cache()\n",
      "    print(f\"Best valid QWK: {best_qwk:.5f} at epoch {best_epoch+1}.\")\n",
      "    load_best_checkpoint(model, best_path)\n",
      "    return model, valid_loader, best_qwk, valid_idx, best_path\n",
      "\n",
      "# =============== THRESHOLD HOOK + EVALUATION/SCORES.CSV =============\n",
      "def save_scores_csv(model_name, qwk_value):\n",
      "    df = pd.DataFrame(\n",
      "        {'Quadratic Weighted Kappa': [qwk_value, qwk_value]},\n",
      "        index=[model_name, \"ensemble\"]\n",
      "    )\n",
      "    df.to_csv('scores.csv', encoding='utf-8')\n",
      "\n",
      "# =============== TEST SET PREDICTION AND SUBMISSION ===============\n",
      "def generate_submission(essay_ids, pred_scores):\n",
      "    df_sub = pd.DataFrame({'essay_id': essay_ids, 'score': pred_scores})\n",
      "    df_sub['score'] = df_sub['score'].astype(np.int64)\n",
      "    df_sub.to_csv('submission.csv', index=False, encoding='utf-8')\n",
      "    print(f\"submission.csv generated ({len(df_sub)} rows).\")\n",
      "\n",
      "# ============== MAIN EXECUTION ======================\n",
      "if __name__ == \"__main__\":\n",
      "    timer0 = time.time()\n",
      "    FOLD = 0\n",
      "    batch_size = 8\n",
      "    epochs = 5\n",
      "    patience = 2\n",
      "\n",
      "    model, valid_loader, best_qwk, valid_idx, best_path = train_one_fold(\n",
      "        FOLD, train_df, eng_feats_train, args=args, batch_size=batch_size, epochs=epochs, patience=patience)\n",
      "\n",
      "    print(\"Optimizing thresholds on validation set...\")\n",
      "    model.eval()\n",
      "    logits, val_labels = eval_model(model, valid_loader, FP16)\n",
      "    expected_vals = coral_logits_to_expected(logits)\n",
      "    opt_thresholds = optimize_thresholds(val_labels, expected_vals, num_classes=N_CLASSES)\n",
      "    print(f\"Optimized thresholds: {opt_thresholds}\")\n",
      "\n",
      "    preds_final = apply_thresholds_batch(expected_vals, opt_thresholds)\n",
      "    final_qwk = qwk_metric(val_labels, preds_final)\n",
      "    print(f\"Final validation QWK after threshold optimization: {final_qwk:.5f}\")\n",
      "\n",
      "    save_scores_csv(model_name=\"deberta_v3_coral\", qwk_value=final_qwk)\n",
      "\n",
      "    print(\"Predicting on test set...\")\n",
      "    test_dataset = EssayDataset(\n",
      "        texts=test_df['full_text'],\n",
      "        engineered_feats=eng_feats_test,\n",
      "        targets=None\n",
      "    )\n",
      "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
      "    all_logits = []\n",
      "    for batch in test_loader:\n",
      "        input_ids = batch['input_ids'].to(device)\n",
      "        attn_mask = batch['attention_mask'].to(device)\n",
      "        eng_feats = batch['engineered_feats'].to(device)\n",
      "        with torch.no_grad():\n",
      "            with autocast(enabled=FP16):\n",
      "                logits = model(input_ids, attn_mask, eng_feats)\n",
      "        all_logits.append(logits.cpu())\n",
      "    all_logits = torch.cat(all_logits, dim=0)\n",
      "    expected_test = coral_logits_to_expected(all_logits)\n",
      "    pred_scores = apply_thresholds_batch(expected_test, opt_thresholds)\n",
      "    generate_submission(test_df['essay_id'], pred_scores)\n",
      "    timer1 = time.time()\n",
      "\n",
      "    if args.debug:\n",
      "        debug_time = timer1 - timer0\n",
      "        est_epochs = 5\n",
      "        scale = (1.0/0.1) * (est_epochs/1)\n",
      "        estimated_time = debug_time * scale\n",
      "        print(\"=== Start of Debug Information ===\")\n",
      "        print(f\"debug_time: {debug_time:.2f}\")\n",
      "        print(f\"estimated_time: {estimated_time:.2f}\")\n",
      "        print(\"=== End of Debug Information ===\")\n",
      "```\n",
      "\n",
      "### Hypothesis for the experiment\n",
      "the experiment is designed based on hypothesis: Target Problem Name: Ordinal-aware modelling & thresholding\n",
      "Target Problem: Design an ordinal-aware modelling and inference pipeline (e.g., ordinal regression head or cumulative link loss) that outputs monotonic score probabilities and converts them to integer labels using validation-tuned thresholds.\n",
      "Chosen Component: Model\n",
      "Hypothesis: Fine-tune a pretrained DeBERTa-v3-base encoder with a CORAL (cumulative link) ordinal regression head that outputs five monotonic logits P(score>k). Convert these probabilities into the final 1-6 integer scores using validation-optimised thresholds that maximise QWK.\n",
      "Reason: Current baselines for essay scoring often treat the task as plain 6-way classification, ignoring the ordered nature of the labels, which can hurt QWK. An explicitly ordinal approach should align the optimisation objective with the evaluation metric.\n",
      "\n",
      "### Results\n",
      "\n",
      "Evaluated results on validation are:\n",
      "                  Quadratic Weighted Kappa\n",
      "deberta_v3_coral                  0.829588\n",
      "ensemble                          0.829588\n",
      "\n",
      "Submission format check result is:\n",
      "Submission is valid.\n",
      "Total running time: 1.638 seconds.\n",
      "\n",
      "Running time: 3314.4708676338196 seconds\n",
      "\n",
      "# Identified Challenges\n",
      "## 1. Capture full essay context beyond 512 tokens\n",
      "Enable the model to ingest the entire essay (or a substantially larger portion) using long-context transformers, hierarchical chunk aggregation, or sliding-window ensembling instead of simple 512-token truncation.\n",
      "\n",
      "## 2. Mitigate severe high-score class imbalance\n",
      "Design specialised strategies (e.g., focal-CB loss tuned per threshold, minority-focused sampling, ordinal margin penalties, or calibrated posterior mapping) to improve recall and calibration for the rare top-score essays.\n",
      "\n",
      "## 3. Inject argumentative & discourse quality features\n",
      "Incorporate domain-specific features such as argument-component detection, discourse relational counts, grammar/fluency error density, and vocabulary richness into the model, possibly via multi-task learning or feature fusion.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[32m2025-07-17 10:04:50.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrdagent.oai.backend.litellm\u001b[0m:\u001b[36m_create_chat_completion_inner_function\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1m\u001b[92mUsing chat model\u001b[0m o3\u001b[0m\n",
      "\u001b[32m2025-07-17 10:04:50.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrdagent.oai.backend.litellm\u001b[0m:\u001b[36m_create_chat_completion_inner_function\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1m\u001b[94massistant:\u001b[0m\u001b[0m\n",
      "\u001b[96m{\"\u001b[0m\u001b[96mded\u001b[0m\u001b[96muplic\u001b[0m\u001b[96mated\u001b[0m\u001b[96m_ch\u001b[0m\u001b[96mallenges\u001b[0m\u001b[96m\":[\"\u001b[0m\u001b[96mCapture\u001b[0m\u001b[96m full\u001b[0m\u001b[96m essay\u001b[0m\u001b[96m context\u001b[0m\u001b[96m beyond\u001b[0m\u001b[96m \u001b[0m\u001b[96m512\u001b[0m\u001b[96m tokens\u001b[0m\u001b[96m\",\"\u001b[0m\u001b[96mMit\u001b[0m\u001b[96migate\u001b[0m\u001b[96m severe\u001b[0m\u001b[96m high\u001b[0m\u001b[96m-score\u001b[0m\u001b[96m class\u001b[0m\u001b[96m imbalance\u001b[0m\u001b[96m\",\"\u001b[0m\u001b[96mInject\u001b[0m\u001b[96m argumentative\u001b[0m\u001b[96m &\u001b[0m\u001b[96m discourse\u001b[0m\u001b[96m quality\u001b[0m\u001b[96m features\u001b[0m\u001b[96m\",\"\u001b[0m\u001b[96mLe\u001b[0m\u001b[96mverage\u001b[0m\u001b[96m full\u001b[0m\u001b[96m \u001b[0m\u001b[96m5\u001b[0m\u001b[96m-\u001b[0m\u001b[96mfold\u001b[0m\u001b[96m cross\u001b[0m\u001b[96m-\u001b[0m\u001b[96mvalidation\u001b[0m\u001b[96m and\u001b[0m\u001b[96m model\u001b[0m\u001b[96m en\u001b[0m\u001b[96msembling\u001b[0m\u001b[96m\",\"\u001b[0m\u001b[96mExpl\u001b[0m\u001b[96moit\u001b[0m\u001b[96m domain\u001b[0m\u001b[96m-\u001b[0m\u001b[96madaptive\u001b[0m\u001b[96m pre\u001b[0m\u001b[96mtraining\u001b[0m\u001b[96m of\u001b[0m\u001b[96m the\u001b[0m\u001b[96m base\u001b[0m\u001b[96m transformer\u001b[0m\u001b[96m on\u001b[0m\u001b[96m the\u001b[0m\u001b[96m essay\u001b[0m\u001b[96m corpus\u001b[0m\u001b[96m\",\"\u001b[0m\u001b[96mImprove\u001b[0m\u001b[96m robustness\u001b[0m\u001b[96m of\u001b[0m\u001b[96m threshold\u001b[0m\u001b[96m mapping\u001b[0m\u001b[96m beyond\u001b[0m\u001b[96m single\u001b[0m\u001b[96m-\u001b[0m\u001b[96mfold\u001b[0m\u001b[96m optimisation\u001b[0m\u001b[96m\"],\u001b[0m\u001b[96m\"\u001b[0m\u001b[96mhyp\u001b[0m\u001b[96moth\u001b[0m\u001b[96meses\u001b[0m\u001b[96m\":[\u001b[0m\u001b[96m{\"\u001b[0m\u001b[96mcaption\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mCapture\u001b[0m\u001b[96m full\u001b[0m\u001b[96m essay\u001b[0m\u001b[96m context\u001b[0m\u001b[96m beyond\u001b[0m\u001b[96m \u001b[0m\u001b[96m512\u001b[0m\u001b[96m tokens\u001b[0m\u001b[96m\",\"\u001b[0m\u001b[96mchallenge\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mThe\u001b[0m\u001b[96m current\u001b[0m\u001b[96m model\u001b[0m\u001b[96m trunc\u001b[0m\u001b[96mates\u001b[0m\u001b[96m essays\u001b[0m\u001b[96m to\u001b[0m\u001b[96m \u001b[0m\u001b[96m512\u001b[0m\u001b[96m tokens\u001b[0m\u001b[96m,\u001b[0m\u001b[96m losing\u001b[0m\u001b[96m potentially\u001b[0m\u001b[96m important\u001b[0m\u001b[96m concluding\u001b[0m\u001b[96m or\u001b[0m\u001b[96m introductory\u001b[0m\u001b[96m arguments\u001b[0m\u001b[96m that\u001b[0m\u001b[96m may\u001b[0m\u001b[96m influence\u001b[0m\u001b[96m Q\u001b[0m\u001b[96mWK\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mhyp\u001b[0m\u001b[96mothesis\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mReplace\u001b[0m\u001b[96m the\u001b[0m\u001b[96m De\u001b[0m\u001b[96mB\u001b[0m\u001b[96mERT\u001b[0m\u001b[96ma\u001b[0m\u001b[96m-\u001b[0m\u001b[96mv\u001b[0m\u001b[96m3\u001b[0m\u001b[96m backbone\u001b[0m\u001b[96m with\u001b[0m\u001b[96m Long\u001b[0m\u001b[96mformer\u001b[0m\u001b[96m-\u001b[0m\u001b[96mbase\u001b[0m\u001b[96m and\u001b[0m\u001b[96m fine\u001b[0m\u001b[96m-\u001b[0m\u001b[96mt\u001b[0m\u001b[96mune\u001b[0m\u001b[96m it\u001b[0m\u001b[96m using\u001b[0m\u001b[96m the\u001b[0m\u001b[96m same\u001b[0m\u001b[96m COR\u001b[0m\u001b[96mAL\u001b[0m\u001b[96m ordinal\u001b[0m\u001b[96m head\u001b[0m\u001b[96m,\u001b[0m\u001b[96m allowing\u001b[0m\u001b[96m up\u001b[0m\u001b[96m to\u001b[0m\u001b[96m \u001b[0m\u001b[96m2\u001b[0m\u001b[96m \u001b[0m\u001b[96m048\u001b[0m\u001b[96m tokens\u001b[0m\u001b[96m so\u001b[0m\u001b[96m each\u001b[0m\u001b[96m essay\u001b[0m\u001b[96m is\u001b[0m\u001b[96m processed\u001b[0m\u001b[96m in\u001b[0m\u001b[96m a\u001b[0m\u001b[96m single\u001b[0m\u001b[96m pass\u001b[0m\u001b[96m without\u001b[0m\u001b[96m trunc\u001b[0m\u001b[96mation\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mmetric\u001b[0m\u001b[96m_\u001b[0m\u001b[96mimpact\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mLong\u001b[0m\u001b[96mer\u001b[0m\u001b[96m context\u001b[0m\u001b[96m should\u001b[0m\u001b[96m let\u001b[0m\u001b[96m the\u001b[0m\u001b[96m model\u001b[0m\u001b[96m exploit\u001b[0m\u001b[96m full\u001b[0m\u001b[96m-\u001b[0m\u001b[96messay\u001b[0m\u001b[96m cues\u001b[0m\u001b[96m,\u001b[0m\u001b[96m raising\u001b[0m\u001b[96m validation\u001b[0m\u001b[96m Q\u001b[0m\u001b[96mWK\u001b[0m\u001b[96m by\u001b[0m\u001b[96m capturing\u001b[0m\u001b[96m information\u001b[0m\u001b[96m previously\u001b[0m\u001b[96m discarded\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mcomponent\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mModel\u001b[0m\u001b[96m\",\"\u001b[0m\u001b[96mevaluation\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96malignment\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mDirect\u001b[0m\u001b[96mly\u001b[0m\u001b[96m removes\u001b[0m\u001b[96m the\u001b[0m\u001b[96m \u001b[0m\u001b[96m512\u001b[0m\u001b[96m-\u001b[0m\u001b[96mtoken\u001b[0m\u001b[96m limit\u001b[0m\u001b[96m called\u001b[0m\u001b[96m out\u001b[0m\u001b[96m in\u001b[0m\u001b[96m the\u001b[0m\u001b[96m challenge\u001b[0m\u001b[96m by\u001b[0m\u001b[96m switching\u001b[0m\u001b[96m to\u001b[0m\u001b[96m a\u001b[0m\u001b[96m long\u001b[0m\u001b[96m-\u001b[0m\u001b[96mcontext\u001b[0m\u001b[96m transformer\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m9\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mimpact\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mAccess\u001b[0m\u001b[96m to\u001b[0m\u001b[96m complete\u001b[0m\u001b[96m essays\u001b[0m\u001b[96m can\u001b[0m\u001b[96m meaning\u001b[0m\u001b[96mfully\u001b[0m\u001b[96m boost\u001b[0m\u001b[96m scoring\u001b[0m\u001b[96m accuracy\u001b[0m\u001b[96m;\u001b[0m\u001b[96m moderate\u001b[0m\u001b[96m–\u001b[0m\u001b[96mhigh\u001b[0m\u001b[96m gain\u001b[0m\u001b[96m expected\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m8\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mnov\u001b[0m\u001b[96mel\u001b[0m\u001b[96mty\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mLong\u001b[0m\u001b[96mformer\u001b[0m\u001b[96m has\u001b[0m\u001b[96m not\u001b[0m\u001b[96m yet\u001b[0m\u001b[96m been\u001b[0m\u001b[96m tried\u001b[0m\u001b[96m in\u001b[0m\u001b[96m prior\u001b[0m\u001b[96m experiments\u001b[0m\u001b[96m;\u001b[0m\u001b[96m this\u001b[0m\u001b[96m is\u001b[0m\u001b[96m a\u001b[0m\u001b[96m new\u001b[0m\u001b[96m architectural\u001b[0m\u001b[96m change\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m8\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mfe\u001b[0m\u001b[96mas\u001b[0m\u001b[96mibility\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mLong\u001b[0m\u001b[96mformer\u001b[0m\u001b[96m-\u001b[0m\u001b[96mbase\u001b[0m\u001b[96m is\u001b[0m\u001b[96m available\u001b[0m\u001b[96m on\u001b[0m\u001b[96m HF\u001b[0m\u001b[96m and\u001b[0m\u001b[96m fits\u001b[0m\u001b[96m into\u001b[0m\u001b[96m \u001b[0m\u001b[96m24\u001b[0m\u001b[96m \u001b[0m\u001b[96mGB\u001b[0m\u001b[96m GPUs\u001b[0m\u001b[96m with\u001b[0m\u001b[96m batch\u001b[0m\u001b[96m size\u001b[0m\u001b[96m adjustments\u001b[0m\u001b[96m;\u001b[0m\u001b[96m code\u001b[0m\u001b[96m changes\u001b[0m\u001b[96m are\u001b[0m\u001b[96m local\u001b[0m\u001b[96mised\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m6\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mrisk\u001b[0m\u001b[96m_reward\u001b[0m\u001b[96m_balance\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mMemory\u001b[0m\u001b[96m usage\u001b[0m\u001b[96m and\u001b[0m\u001b[96m slower\u001b[0m\u001b[96m training\u001b[0m\u001b[96m raise\u001b[0m\u001b[96m risk\u001b[0m\u001b[96m,\u001b[0m\u001b[96m but\u001b[0m\u001b[96m potential\u001b[0m\u001b[96m Q\u001b[0m\u001b[96mWK\u001b[0m\u001b[96m gain\u001b[0m\u001b[96m just\u001b[0m\u001b[96mifies\u001b[0m\u001b[96m it\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m7\u001b[0m\u001b[96m}}\u001b[0m\u001b[96m},{\"\u001b[0m\u001b[96mcaption\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mMit\u001b[0m\u001b[96migate\u001b[0m\u001b[96m severe\u001b[0m\u001b[96m high\u001b[0m\u001b[96m-score\u001b[0m\u001b[96m class\u001b[0m\u001b[96m imbalance\u001b[0m\u001b[96m\",\"\u001b[0m\u001b[96mchallenge\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mRare\u001b[0m\u001b[96m \u001b[0m\u001b[96m5\u001b[0m\u001b[96m/\u001b[0m\u001b[96m6\u001b[0m\u001b[96m scores\u001b[0m\u001b[96m receive\u001b[0m\u001b[96m little\u001b[0m\u001b[96m representation\u001b[0m\u001b[96m,\u001b[0m\u001b[96m leading\u001b[0m\u001b[96m to\u001b[0m\u001b[96m poor\u001b[0m\u001b[96m recall\u001b[0m\u001b[96m and\u001b[0m\u001b[96m calibration\u001b[0m\u001b[96m for\u001b[0m\u001b[96m top\u001b[0m\u001b[96m marks\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mhyp\u001b[0m\u001b[96mothesis\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mImplement\u001b[0m\u001b[96m class\u001b[0m\u001b[96m-\u001b[0m\u001b[96mbalanced\u001b[0m\u001b[96m focal\u001b[0m\u001b[96m COR\u001b[0m\u001b[96mAL\u001b[0m\u001b[96m loss\u001b[0m\u001b[96m with\u001b[0m\u001b[96m dynamic\u001b[0m\u001b[96m overs\u001b[0m\u001b[96mampling\u001b[0m\u001b[96m that\u001b[0m\u001b[96m duplicates\u001b[0m\u001b[96m essays\u001b[0m\u001b[96m of\u001b[0m\u001b[96m scores\u001b[0m\u001b[96m \u001b[0m\u001b[96m5\u001b[0m\u001b[96m and\u001b[0m\u001b[96m \u001b[0m\u001b[96m6\u001b[0m\u001b[96m at\u001b[0m\u001b[96m a\u001b[0m\u001b[96m \u001b[0m\u001b[96m3\u001b[0m\u001b[96m×\u001b[0m\u001b[96m rate\u001b[0m\u001b[96m in\u001b[0m\u001b[96m each\u001b[0m\u001b[96m mini\u001b[0m\u001b[96m-\u001b[0m\u001b[96mbatch\u001b[0m\u001b[96m to\u001b[0m\u001b[96m equal\u001b[0m\u001b[96mise\u001b[0m\u001b[96m positive\u001b[0m\u001b[96m/\u001b[0m\u001b[96mnegative\u001b[0m\u001b[96m threshold\u001b[0m\u001b[96m counts\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mmetric\u001b[0m\u001b[96m_\u001b[0m\u001b[96mimpact\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mBetter\u001b[0m\u001b[96m gradient\u001b[0m\u001b[96m signal\u001b[0m\u001b[96m for\u001b[0m\u001b[96m minority\u001b[0m\u001b[96m thresholds\u001b[0m\u001b[96m should\u001b[0m\u001b[96m improve\u001b[0m\u001b[96m recall\u001b[0m\u001b[96m for\u001b[0m\u001b[96m scores\u001b[0m\u001b[96m \u001b[0m\u001b[96m5\u001b[0m\u001b[96m–\u001b[0m\u001b[96m6\u001b[0m\u001b[96m,\u001b[0m\u001b[96m elev\u001b[0m\u001b[96mating\u001b[0m\u001b[96m overall\u001b[0m\u001b[96m Q\u001b[0m\u001b[96mWK\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mcomponent\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mModel\u001b[0m\u001b[96m\",\"\u001b[0m\u001b[96mevaluation\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96malignment\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mUses\u001b[0m\u001b[96m loss\u001b[0m\u001b[96m re\u001b[0m\u001b[96m-\u001b[0m\u001b[96mweight\u001b[0m\u001b[96ming\u001b[0m\u001b[96m and\u001b[0m\u001b[96m sampling\u001b[0m\u001b[96m exactly\u001b[0m\u001b[96m to\u001b[0m\u001b[96m counter\u001b[0m\u001b[96m class\u001b[0m\u001b[96m imbalance\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m9\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mimpact\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mImpro\u001b[0m\u001b[96mving\u001b[0m\u001b[96m extreme\u001b[0m\u001b[96m classes\u001b[0m\u001b[96m can\u001b[0m\u001b[96m noticeably\u001b[0m\u001b[96m affect\u001b[0m\u001b[96m Q\u001b[0m\u001b[96mWK\u001b[0m\u001b[96m;\u001b[0m\u001b[96m moderate\u001b[0m\u001b[96m impact\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m7\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mnov\u001b[0m\u001b[96mel\u001b[0m\u001b[96mty\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mWhile\u001b[0m\u001b[96m focal\u001b[0m\u001b[96m-\u001b[0m\u001b[96mCB\u001b[0m\u001b[96m loss\u001b[0m\u001b[96m exists\u001b[0m\u001b[96m,\u001b[0m\u001b[96m combining\u001b[0m\u001b[96m it\u001b[0m\u001b[96m with\u001b[0m\u001b[96m targeted\u001b[0m\u001b[96m batch\u001b[0m\u001b[96m-\u001b[0m\u001b[96mlevel\u001b[0m\u001b[96m overs\u001b[0m\u001b[96mampling\u001b[0m\u001b[96m is\u001b[0m\u001b[96m new\u001b[0m\u001b[96m to\u001b[0m\u001b[96m this\u001b[0m\u001b[96m pipeline\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m7\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mfe\u001b[0m\u001b[96mas\u001b[0m\u001b[96mibility\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mEasy\u001b[0m\u001b[96m to\u001b[0m\u001b[96m add\u001b[0m\u001b[96m a\u001b[0m\u001b[96m weighted\u001b[0m\u001b[96m sampler\u001b[0m\u001b[96m and\u001b[0m\u001b[96m modify\u001b[0m\u001b[96m existing\u001b[0m\u001b[96m loss\u001b[0m\u001b[96m;\u001b[0m\u001b[96m low\u001b[0m\u001b[96m engineering\u001b[0m\u001b[96m burden\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m8\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mrisk\u001b[0m\u001b[96m_reward\u001b[0m\u001b[96m_balance\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mLow\u001b[0m\u001b[96m risk\u001b[0m\u001b[96m of\u001b[0m\u001b[96m metric\u001b[0m\u001b[96m drop\u001b[0m\u001b[96m;\u001b[0m\u001b[96m overs\u001b[0m\u001b[96mampling\u001b[0m\u001b[96m may\u001b[0m\u001b[96m length\u001b[0m\u001b[96men\u001b[0m\u001b[96m training\u001b[0m\u001b[96m slightly\u001b[0m\u001b[96m but\u001b[0m\u001b[96m payoff\u001b[0m\u001b[96m likely\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m8\u001b[0m\u001b[96m}}\u001b[0m\u001b[96m},{\"\u001b[0m\u001b[96mcaption\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mInject\u001b[0m\u001b[96m argumentative\u001b[0m\u001b[96m &\u001b[0m\u001b[96m discourse\u001b[0m\u001b[96m quality\u001b[0m\u001b[96m features\u001b[0m\u001b[96m\",\"\u001b[0m\u001b[96mchallenge\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mCurrent\u001b[0m\u001b[96m engineered\u001b[0m\u001b[96m features\u001b[0m\u001b[96m are\u001b[0m\u001b[96m mostly\u001b[0m\u001b[96m surface\u001b[0m\u001b[96m-\u001b[0m\u001b[96mlevel\u001b[0m\u001b[96m;\u001b[0m\u001b[96m deeper\u001b[0m\u001b[96m discourse\u001b[0m\u001b[96m signals\u001b[0m\u001b[96m are\u001b[0m\u001b[96m missing\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mhyp\u001b[0m\u001b[96mothesis\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mAug\u001b[0m\u001b[96mment\u001b[0m\u001b[96m feature\u001b[0m\u001b[96m set\u001b[0m\u001b[96m with\u001b[0m\u001b[96m transformer\u001b[0m\u001b[96m-\u001b[0m\u001b[96mbased\u001b[0m\u001b[96m argument\u001b[0m\u001b[96m-\u001b[0m\u001b[96mcomponent\u001b[0m\u001b[96m counts\u001b[0m\u001b[96m (\u001b[0m\u001b[96mclaims\u001b[0m\u001b[96m,\u001b[0m\u001b[96m evidence\u001b[0m\u001b[96m,\u001b[0m\u001b[96m rebut\u001b[0m\u001b[96mt\u001b[0m\u001b[96mals\u001b[0m\u001b[96m)\u001b[0m\u001b[96m extracted\u001b[0m\u001b[96m via\u001b[0m\u001b[96m a\u001b[0m\u001b[96m pretrained\u001b[0m\u001b[96m Argument\u001b[0m\u001b[96m Mining\u001b[0m\u001b[96m model\u001b[0m\u001b[96m,\u001b[0m\u001b[96m and\u001b[0m\u001b[96m concatenate\u001b[0m\u001b[96m these\u001b[0m\u001b[96m normal\u001b[0m\u001b[96mised\u001b[0m\u001b[96m counts\u001b[0m\u001b[96m to\u001b[0m\u001b[96m the\u001b[0m\u001b[96m existing\u001b[0m\u001b[96m engineered\u001b[0m\u001b[96m vector\u001b[0m\u001b[96m before\u001b[0m\u001b[96m the\u001b[0m\u001b[96m ordinal\u001b[0m\u001b[96m head\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mmetric\u001b[0m\u001b[96m_\u001b[0m\u001b[96mimpact\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mR\u001b[0m\u001b[96micher\u001b[0m\u001b[96m discourse\u001b[0m\u001b[96m features\u001b[0m\u001b[96m should\u001b[0m\u001b[96m help\u001b[0m\u001b[96m the\u001b[0m\u001b[96m model\u001b[0m\u001b[96m distinguish\u001b[0m\u001b[96m well\u001b[0m\u001b[96m-\u001b[0m\u001b[96mstructured\u001b[0m\u001b[96m high\u001b[0m\u001b[96m-\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m essays\u001b[0m\u001b[96m,\u001b[0m\u001b[96m boosting\u001b[0m\u001b[96m Q\u001b[0m\u001b[96mWK\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mcomponent\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mFeature\u001b[0m\u001b[96mEng\u001b[0m\u001b[96m\",\"\u001b[0m\u001b[96mevaluation\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96malignment\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mDirect\u001b[0m\u001b[96mly\u001b[0m\u001b[96m answers\u001b[0m\u001b[96m the\u001b[0m\u001b[96m request\u001b[0m\u001b[96m for\u001b[0m\u001b[96m argumentative\u001b[0m\u001b[96m /\u001b[0m\u001b[96m discourse\u001b[0m\u001b[96m signals\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m9\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mimpact\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mIf\u001b[0m\u001b[96m argument\u001b[0m\u001b[96m mining\u001b[0m\u001b[96m features\u001b[0m\u001b[96m correlate\u001b[0m\u001b[96m with\u001b[0m\u001b[96m scores\u001b[0m\u001b[96m,\u001b[0m\u001b[96m could\u001b[0m\u001b[96m bring\u001b[0m\u001b[96m a\u001b[0m\u001b[96m meaningful\u001b[0m\u001b[96m bump\u001b[0m\u001b[96m;\u001b[0m\u001b[96m moderate\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m6\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mnov\u001b[0m\u001b[96mel\u001b[0m\u001b[96mty\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mThis\u001b[0m\u001b[96m type\u001b[0m\u001b[96m of\u001b[0m\u001b[96m feature\u001b[0m\u001b[96m has\u001b[0m\u001b[96m not\u001b[0m\u001b[96m appeared\u001b[0m\u001b[96m in\u001b[0m\u001b[96m prior\u001b[0m\u001b[96m runs\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m8\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mfe\u001b[0m\u001b[96mas\u001b[0m\u001b[96mibility\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mRequires\u001b[0m\u001b[96m downloading\u001b[0m\u001b[96m an\u001b[0m\u001b[96m open\u001b[0m\u001b[96m-\u001b[0m\u001b[96msource\u001b[0m\u001b[96m argument\u001b[0m\u001b[96m mining\u001b[0m\u001b[96m tag\u001b[0m\u001b[96mger\u001b[0m\u001b[96m and\u001b[0m\u001b[96m running\u001b[0m\u001b[96m inference\u001b[0m\u001b[96m;\u001b[0m\u001b[96m feasible\u001b[0m\u001b[96m but\u001b[0m\u001b[96m adds\u001b[0m\u001b[96m preprocessing\u001b[0m\u001b[96m time\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m5\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mrisk\u001b[0m\u001b[96m_reward\u001b[0m\u001b[96m_balance\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mModer\u001b[0m\u001b[96mate\u001b[0m\u001b[96m engineering\u001b[0m\u001b[96m effort\u001b[0m\u001b[96m,\u001b[0m\u001b[96m medium\u001b[0m\u001b[96m upside\u001b[0m\u001b[96m;\u001b[0m\u001b[96m limited\u001b[0m\u001b[96m downside\u001b[0m\u001b[96m as\u001b[0m\u001b[96m features\u001b[0m\u001b[96m can\u001b[0m\u001b[96m be\u001b[0m\u001b[96m cached\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m7\u001b[0m\u001b[96m}}\u001b[0m\u001b[96m},{\"\u001b[0m\u001b[96mcaption\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mLe\u001b[0m\u001b[96mverage\u001b[0m\u001b[96m full\u001b[0m\u001b[96m \u001b[0m\u001b[96m5\u001b[0m\u001b[96m-\u001b[0m\u001b[96mfold\u001b[0m\u001b[96m cross\u001b[0m\u001b[96m-\u001b[0m\u001b[96mvalidation\u001b[0m\u001b[96m and\u001b[0m\u001b[96m model\u001b[0m\u001b[96m en\u001b[0m\u001b[96msembling\u001b[0m\u001b[96m\",\"\u001b[0m\u001b[96mchallenge\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mOnly\u001b[0m\u001b[96m fold\u001b[0m\u001b[96m-\u001b[0m\u001b[96m0\u001b[0m\u001b[96m is\u001b[0m\u001b[96m trained\u001b[0m\u001b[96m;\u001b[0m\u001b[96m general\u001b[0m\u001b[96misation\u001b[0m\u001b[96m is\u001b[0m\u001b[96m limited\u001b[0m\u001b[96m and\u001b[0m\u001b[96m unused\u001b[0m\u001b[96m data\u001b[0m\u001b[96m hurts\u001b[0m\u001b[96m performance\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mhyp\u001b[0m\u001b[96mothesis\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mTrain\u001b[0m\u001b[96m the\u001b[0m\u001b[96m COR\u001b[0m\u001b[96mAL\u001b[0m\u001b[96m model\u001b[0m\u001b[96m on\u001b[0m\u001b[96m all\u001b[0m\u001b[96m \u001b[0m\u001b[96m5\u001b[0m\u001b[96m folds\u001b[0m\u001b[96m,\u001b[0m\u001b[96m save\u001b[0m\u001b[96m each\u001b[0m\u001b[96m checkpoint\u001b[0m\u001b[96m,\u001b[0m\u001b[96m and\u001b[0m\u001b[96m at\u001b[0m\u001b[96m inference\u001b[0m\u001b[96m average\u001b[0m\u001b[96m the\u001b[0m\u001b[96m per\u001b[0m\u001b[96m-\u001b[0m\u001b[96messay\u001b[0m\u001b[96m expected\u001b[0m\u001b[96m scores\u001b[0m\u001b[96m from\u001b[0m\u001b[96m the\u001b[0m\u001b[96m five\u001b[0m\u001b[96m models\u001b[0m\u001b[96m before\u001b[0m\u001b[96m threshold\u001b[0m\u001b[96m mapping\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mmetric\u001b[0m\u001b[96m_\u001b[0m\u001b[96mimpact\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mUsing\u001b[0m\u001b[96m \u001b[0m\u001b[96m100\u001b[0m\u001b[96m \u001b[0m\u001b[96m%\u001b[0m\u001b[96m of\u001b[0m\u001b[96m training\u001b[0m\u001b[96m data\u001b[0m\u001b[96m and\u001b[0m\u001b[96m en\u001b[0m\u001b[96msembling\u001b[0m\u001b[96m typically\u001b[0m\u001b[96m yields\u001b[0m\u001b[96m a\u001b[0m\u001b[96m solid\u001b[0m\u001b[96m Q\u001b[0m\u001b[96mWK\u001b[0m\u001b[96m boost\u001b[0m\u001b[96m (\u001b[0m\u001b[96m≈\u001b[0m\u001b[96m0\u001b[0m\u001b[96m.\u001b[0m\u001b[96m01\u001b[0m\u001b[96m–\u001b[0m\u001b[96m0\u001b[0m\u001b[96m.\u001b[0m\u001b[96m02\u001b[0m\u001b[96m).\u001b[0m\u001b[96m\",\"\u001b[0m\u001b[96mcomponent\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mEn\u001b[0m\u001b[96msemble\u001b[0m\u001b[96m\",\"\u001b[0m\u001b[96mevaluation\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96malignment\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mAddresses\u001b[0m\u001b[96m the\u001b[0m\u001b[96m single\u001b[0m\u001b[96m-\u001b[0m\u001b[96mfold\u001b[0m\u001b[96m limitation\u001b[0m\u001b[96m precisely\u001b[0m\u001b[96m by\u001b[0m\u001b[96m introducing\u001b[0m\u001b[96m \u001b[0m\u001b[96m5\u001b[0m\u001b[96m-\u001b[0m\u001b[96mfold\u001b[0m\u001b[96m ensemble\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m10\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mimpact\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mCross\u001b[0m\u001b[96m-\u001b[0m\u001b[96mfold\u001b[0m\u001b[96m ensembles\u001b[0m\u001b[96m are\u001b[0m\u001b[96m proven\u001b[0m\u001b[96m to\u001b[0m\u001b[96m improve\u001b[0m\u001b[96m leaderboard\u001b[0m\u001b[96m metrics\u001b[0m\u001b[96m;\u001b[0m\u001b[96m good\u001b[0m\u001b[96m expected\u001b[0m\u001b[96m gain\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m8\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mnov\u001b[0m\u001b[96mel\u001b[0m\u001b[96mty\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mNo\u001b[0m\u001b[96m ensemble\u001b[0m\u001b[96m across\u001b[0m\u001b[96m folds\u001b[0m\u001b[96m exists\u001b[0m\u001b[96m yet\u001b[0m\u001b[96m;\u001b[0m\u001b[96m but\u001b[0m\u001b[96m idea\u001b[0m\u001b[96m is\u001b[0m\u001b[96m standard\u001b[0m\u001b[96m in\u001b[0m\u001b[96m competitions\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m6\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mfe\u001b[0m\u001b[96mas\u001b[0m\u001b[96mibility\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mStraight\u001b[0m\u001b[96mforward\u001b[0m\u001b[96m extension\u001b[0m\u001b[96m of\u001b[0m\u001b[96m existing\u001b[0m\u001b[96m training\u001b[0m\u001b[96m loop\u001b[0m\u001b[96m;\u001b[0m\u001b[96m only\u001b[0m\u001b[96m increases\u001b[0m\u001b[96m compute\u001b[0m\u001b[96m line\u001b[0m\u001b[96marly\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m7\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mrisk\u001b[0m\u001b[96m_reward\u001b[0m\u001b[96m_balance\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mCompute\u001b[0m\u001b[96m cost\u001b[0m\u001b[96m grows\u001b[0m\u001b[96m,\u001b[0m\u001b[96m but\u001b[0m\u001b[96m risk\u001b[0m\u001b[96m of\u001b[0m\u001b[96m worse\u001b[0m\u001b[96m performance\u001b[0m\u001b[96m is\u001b[0m\u001b[96m minimal\u001b[0m\u001b[96m;\u001b[0m\u001b[96m strong\u001b[0m\u001b[96m reward\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m8\u001b[0m\u001b[96m}}\u001b[0m\u001b[96m},{\"\u001b[0m\u001b[96mcaption\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mExpl\u001b[0m\u001b[96moit\u001b[0m\u001b[96m domain\u001b[0m\u001b[96m-\u001b[0m\u001b[96madaptive\u001b[0m\u001b[96m pre\u001b[0m\u001b[96mtraining\u001b[0m\u001b[96m of\u001b[0m\u001b[96m the\u001b[0m\u001b[96m base\u001b[0m\u001b[96m transformer\u001b[0m\u001b[96m on\u001b[0m\u001b[96m the\u001b[0m\u001b[96m essay\u001b[0m\u001b[96m corpus\u001b[0m\u001b[96m\",\"\u001b[0m\u001b[96mchallenge\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mThe\u001b[0m\u001b[96m generic\u001b[0m\u001b[96m De\u001b[0m\u001b[96mB\u001b[0m\u001b[96mERT\u001b[0m\u001b[96ma\u001b[0m\u001b[96m encoder\u001b[0m\u001b[96m may\u001b[0m\u001b[96m not\u001b[0m\u001b[96m capture\u001b[0m\u001b[96m essay\u001b[0m\u001b[96m-\u001b[0m\u001b[96mspecific\u001b[0m\u001b[96m linguistic\u001b[0m\u001b[96m patterns\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mhyp\u001b[0m\u001b[96mothesis\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mPerform\u001b[0m\u001b[96m an\u001b[0m\u001b[96m additional\u001b[0m\u001b[96m \u001b[0m\u001b[96m10\u001b[0m\u001b[96m \u001b[0m\u001b[96m000\u001b[0m\u001b[96m-\u001b[0m\u001b[96mstep\u001b[0m\u001b[96m Mask\u001b[0m\u001b[96med\u001b[0m\u001b[96m Language\u001b[0m\u001b[96m Mod\u001b[0m\u001b[96melling\u001b[0m\u001b[96m pre\u001b[0m\u001b[96mtraining\u001b[0m\u001b[96m of\u001b[0m\u001b[96m De\u001b[0m\u001b[96mB\u001b[0m\u001b[96mERT\u001b[0m\u001b[96ma\u001b[0m\u001b[96m-\u001b[0m\u001b[96mv\u001b[0m\u001b[96m3\u001b[0m\u001b[96m-\u001b[0m\u001b[96mbase\u001b[0m\u001b[96m on\u001b[0m\u001b[96m the\u001b[0m\u001b[96m concaten\u001b[0m\u001b[96mated\u001b[0m\u001b[96m train\u001b[0m\u001b[96m \u001b[0m\u001b[96m+\u001b[0m\u001b[96m \u001b[0m\u001b[96mtest\u001b[0m\u001b[96m essays\u001b[0m\u001b[96m (\u001b[0m\u001b[96mno\u001b[0m\u001b[96m labels\u001b[0m\u001b[96m)\u001b[0m\u001b[96m before\u001b[0m\u001b[96m fine\u001b[0m\u001b[96m-\u001b[0m\u001b[96mt\u001b[0m\u001b[96muning\u001b[0m\u001b[96m with\u001b[0m\u001b[96m the\u001b[0m\u001b[96m COR\u001b[0m\u001b[96mAL\u001b[0m\u001b[96m head\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mmetric\u001b[0m\u001b[96m_\u001b[0m\u001b[96mimpact\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mDomain\u001b[0m\u001b[96m adaptation\u001b[0m\u001b[96m should\u001b[0m\u001b[96m yield\u001b[0m\u001b[96m better\u001b[0m\u001b[96m text\u001b[0m\u001b[96m representations\u001b[0m\u001b[96m,\u001b[0m\u001b[96m likely\u001b[0m\u001b[96m improving\u001b[0m\u001b[96m Q\u001b[0m\u001b[96mWK\u001b[0m\u001b[96m by\u001b[0m\u001b[96m several\u001b[0m\u001b[96m points\u001b[0m\u001b[96m of\u001b[0m\u001b[96m the\u001b[0m\u001b[96m third\u001b[0m\u001b[96m decimal\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mcomponent\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mModel\u001b[0m\u001b[96m\",\"\u001b[0m\u001b[96mevaluation\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96malignment\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mDirect\u001b[0m\u001b[96mly\u001b[0m\u001b[96m adap\u001b[0m\u001b[96mts\u001b[0m\u001b[96m the\u001b[0m\u001b[96m backbone\u001b[0m\u001b[96m to\u001b[0m\u001b[96m the\u001b[0m\u001b[96m essay\u001b[0m\u001b[96m domain\u001b[0m\u001b[96m as\u001b[0m\u001b[96m requested\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m8\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mimpact\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mDomain\u001b[0m\u001b[96m-\u001b[0m\u001b[96madaptive\u001b[0m\u001b[96m pre\u001b[0m\u001b[96mtraining\u001b[0m\u001b[96m has\u001b[0m\u001b[96m shown\u001b[0m\u001b[96m noticeable\u001b[0m\u001b[96m gains\u001b[0m\u001b[96m in\u001b[0m\u001b[96m many\u001b[0m\u001b[96m NLP\u001b[0m\u001b[96m tasks\u001b[0m\u001b[96m;\u001b[0m\u001b[96m moderate\u001b[0m\u001b[96m-\u001b[0m\u001b[96mhigh\u001b[0m\u001b[96m impact\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m8\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mnov\u001b[0m\u001b[96mel\u001b[0m\u001b[96mty\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mNo\u001b[0m\u001b[96m prior\u001b[0m\u001b[96m experiment\u001b[0m\u001b[96m performed\u001b[0m\u001b[96m D\u001b[0m\u001b[96mAPT\u001b[0m\u001b[96m;\u001b[0m\u001b[96m innovative\u001b[0m\u001b[96m for\u001b[0m\u001b[96m this\u001b[0m\u001b[96m pipeline\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m9\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mfe\u001b[0m\u001b[96mas\u001b[0m\u001b[96mibility\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mRequires\u001b[0m\u001b[96m additional\u001b[0m\u001b[96m GPU\u001b[0m\u001b[96m hours\u001b[0m\u001b[96m but\u001b[0m\u001b[96m is\u001b[0m\u001b[96m routine\u001b[0m\u001b[96m with\u001b[0m\u001b[96m HF\u001b[0m\u001b[96m Trainer\u001b[0m\u001b[96m;\u001b[0m\u001b[96m within\u001b[0m\u001b[96m \u001b[0m\u001b[96m5\u001b[0m\u001b[96m-\u001b[0m\u001b[96mhour\u001b[0m\u001b[96m limit\u001b[0m\u001b[96m if\u001b[0m\u001b[96m batch\u001b[0m\u001b[96m size\u001b[0m\u001b[96m tuned\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m6\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mrisk\u001b[0m\u001b[96m_reward\u001b[0m\u001b[96m_balance\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mHigh\u001b[0m\u001b[96m compute\u001b[0m\u001b[96m cost\u001b[0m\u001b[96m introduces\u001b[0m\u001b[96m risk\u001b[0m\u001b[96m,\u001b[0m\u001b[96m but\u001b[0m\u001b[96m potential\u001b[0m\u001b[96m reward\u001b[0m\u001b[96m is\u001b[0m\u001b[96m significant\u001b[0m\u001b[96m;\u001b[0m\u001b[96m balanced\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m7\u001b[0m\u001b[96m}}\u001b[0m\u001b[96m},{\"\u001b[0m\u001b[96mcaption\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mImprove\u001b[0m\u001b[96m robustness\u001b[0m\u001b[96m of\u001b[0m\u001b[96m threshold\u001b[0m\u001b[96m mapping\u001b[0m\u001b[96m beyond\u001b[0m\u001b[96m single\u001b[0m\u001b[96m-\u001b[0m\u001b[96mfold\u001b[0m\u001b[96m optimisation\u001b[0m\u001b[96m\",\"\u001b[0m\u001b[96mchallenge\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mThreshold\u001b[0m\u001b[96ms\u001b[0m\u001b[96m optim\u001b[0m\u001b[96mised\u001b[0m\u001b[96m on\u001b[0m\u001b[96m one\u001b[0m\u001b[96m fold\u001b[0m\u001b[96m may\u001b[0m\u001b[96m over\u001b[0m\u001b[96m-\u001b[0m\u001b[96mfit\u001b[0m\u001b[96m and\u001b[0m\u001b[96m not\u001b[0m\u001b[96m general\u001b[0m\u001b[96mise\u001b[0m\u001b[96m to\u001b[0m\u001b[96m leaderboard\u001b[0m\u001b[96m data\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mhyp\u001b[0m\u001b[96mothesis\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mAfter\u001b[0m\u001b[96m training\u001b[0m\u001b[96m the\u001b[0m\u001b[96m \u001b[0m\u001b[96m5\u001b[0m\u001b[96m-\u001b[0m\u001b[96mfold\u001b[0m\u001b[96m ensemble\u001b[0m\u001b[96m,\u001b[0m\u001b[96m learn\u001b[0m\u001b[96m global\u001b[0m\u001b[96m thresholds\u001b[0m\u001b[96m by\u001b[0m\u001b[96m minim\u001b[0m\u001b[96mising\u001b[0m\u001b[96m negative\u001b[0m\u001b[96m Q\u001b[0m\u001b[96mWK\u001b[0m\u001b[96m on\u001b[0m\u001b[96m the\u001b[0m\u001b[96m concaten\u001b[0m\u001b[96mated\u001b[0m\u001b[96m validation\u001b[0m\u001b[96m predictions\u001b[0m\u001b[96m from\u001b[0m\u001b[96m all\u001b[0m\u001b[96m folds\u001b[0m\u001b[96m,\u001b[0m\u001b[96m then\u001b[0m\u001b[96m apply\u001b[0m\u001b[96m the\u001b[0m\u001b[96m same\u001b[0m\u001b[96m thresholds\u001b[0m\u001b[96m to\u001b[0m\u001b[96m ensemble\u001b[0m\u001b[96m predictions\u001b[0m\u001b[96m on\u001b[0m\u001b[96m the\u001b[0m\u001b[96m test\u001b[0m\u001b[96m set\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mmetric\u001b[0m\u001b[96m_\u001b[0m\u001b[96mimpact\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mUsing\u001b[0m\u001b[96m more\u001b[0m\u001b[96m validation\u001b[0m\u001b[96m data\u001b[0m\u001b[96m for\u001b[0m\u001b[96m threshold\u001b[0m\u001b[96m tuning\u001b[0m\u001b[96m should\u001b[0m\u001b[96m general\u001b[0m\u001b[96mise\u001b[0m\u001b[96m better\u001b[0m\u001b[96m,\u001b[0m\u001b[96m increasing\u001b[0m\u001b[96m public\u001b[0m\u001b[96m &\u001b[0m\u001b[96m private\u001b[0m\u001b[96m Q\u001b[0m\u001b[96mWK\u001b[0m\u001b[96m stability\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mcomponent\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mWorkflow\u001b[0m\u001b[96m\",\"\u001b[0m\u001b[96mevaluation\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96malignment\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mPrecis\u001b[0m\u001b[96mely\u001b[0m\u001b[96m tackles\u001b[0m\u001b[96m the\u001b[0m\u001b[96m over\u001b[0m\u001b[96m-\u001b[0m\u001b[96mf\u001b[0m\u001b[96mitting\u001b[0m\u001b[96m risk\u001b[0m\u001b[96m of\u001b[0m\u001b[96m single\u001b[0m\u001b[96m-\u001b[0m\u001b[96mfold\u001b[0m\u001b[96m threshold\u001b[0m\u001b[96m optimisation\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m9\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mimpact\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mBetter\u001b[0m\u001b[96m-\u001b[0m\u001b[96mcal\u001b[0m\u001b[96mibr\u001b[0m\u001b[96mated\u001b[0m\u001b[96m thresholds\u001b[0m\u001b[96m can\u001b[0m\u001b[96m yield\u001b[0m\u001b[96m small\u001b[0m\u001b[96m but\u001b[0m\u001b[96m leaderboard\u001b[0m\u001b[96m-\u001b[0m\u001b[96mre\u001b[0m\u001b[96mlevant\u001b[0m\u001b[96m gains\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m6\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mnov\u001b[0m\u001b[96mel\u001b[0m\u001b[96mty\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mGlobal\u001b[0m\u001b[96m threshold\u001b[0m\u001b[96ming\u001b[0m\u001b[96m across\u001b[0m\u001b[96m folds\u001b[0m\u001b[96m hasn\u001b[0m\u001b[96m’t\u001b[0m\u001b[96m been\u001b[0m\u001b[96m attempted\u001b[0m\u001b[96m yet\u001b[0m\u001b[96m;\u001b[0m\u001b[96m moderately\u001b[0m\u001b[96m novel\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m7\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mfe\u001b[0m\u001b[96mas\u001b[0m\u001b[96mibility\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mImplementation\u001b[0m\u001b[96m is\u001b[0m\u001b[96m a\u001b[0m\u001b[96m simple\u001b[0m\u001b[96m change\u001b[0m\u001b[96m to\u001b[0m\u001b[96m the\u001b[0m\u001b[96m existing\u001b[0m\u001b[96m optimisation\u001b[0m\u001b[96m function\u001b[0m\u001b[96m;\u001b[0m\u001b[96m trivial\u001b[0m\u001b[96m cost\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m9\u001b[0m\u001b[96m},\"\u001b[0m\u001b[96mrisk\u001b[0m\u001b[96m_reward\u001b[0m\u001b[96m_balance\u001b[0m\u001b[96m\":{\"\u001b[0m\u001b[96mreason\u001b[0m\u001b[96ming\u001b[0m\u001b[96m\":\"\u001b[0m\u001b[96mVery\u001b[0m\u001b[96m low\u001b[0m\u001b[96m risk\u001b[0m\u001b[96m and\u001b[0m\u001b[96m effort\u001b[0m\u001b[96m for\u001b[0m\u001b[96m potential\u001b[0m\u001b[96m incremental\u001b[0m\u001b[96m gain\u001b[0m\u001b[96m;\u001b[0m\u001b[96m excellent\u001b[0m\u001b[96m balance\u001b[0m\u001b[96m.\",\"\u001b[0m\u001b[96mscore\u001b[0m\u001b[96m\":\u001b[0m\u001b[96m9\u001b[0m\u001b[96m}}\u001b[0m\u001b[96m}\u001b[0m\u001b[96m]}\u001b[0m\u001b[96m\u001b[0m/data/userdata/v-lijingyuan/anaconda3/envs/rdagent/lib/python3.10/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='{\"dedupl...er_specific_fields=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\n",
      "\u001b[32m2025-07-17 10:05:13.584\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrdagent.oai.backend.litellm\u001b[0m:\u001b[36m_create_chat_completion_inner_function\u001b[0m:\u001b[36m181\u001b[0m - \u001b[1mCurrent Cost: $0.0345340000; Accumulated Cost: $0.0345340000; finish_reason='stop'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "supports_response_schema = True\n",
    "diclist =[]\n",
    "for i in range(1):\n",
    "    response = APIBackend().build_messages_and_create_chat_completion(\n",
    "                    user_prompt=user_prompt,\n",
    "                    system_prompt=sys_prompt,\n",
    "                    response_format=HypothesisList if supports_response_schema else {\"type\": \"json_object\"},\n",
    "                    json_target_type=(\n",
    "                        Dict[str, Dict[str, str | Dict[str, str | int]]] if not supports_response_schema else None\n",
    "                    ),\n",
    "                )\n",
    "    \n",
    "    hypotheses = HypothesisList(**json.loads(response))\n",
    "    resp_dict = {\n",
    "                h.caption: {\n",
    "                    \"reason\": h.challenge,\n",
    "                    \"component\": h.component,\n",
    "                    \"hypothesis\": h.hypothesis,\n",
    "                    \"evaluation\": {\n",
    "                        \"alignment_score\": h.evaluation.alignment.score,\n",
    "                        \"impact_score\": h.evaluation.impact.score,\n",
    "                        \"novelty_score\": h.evaluation.novelty.score,\n",
    "                        \"feasibility_score\": h.evaluation.feasibility.score,\n",
    "                        \"risk_reward_balance_score\": h.evaluation.risk_reward_balance.score,\n",
    "                    },\n",
    "                }\n",
    "                for h in hypotheses.hypotheses\n",
    "            }\n",
    "    diclist.append(resp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2db1bb75",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pickled_problem_name, new_hypothesis \u001b[38;5;241m=\u001b[39m \u001b[43mhypothesis_rank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhypothesis_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresp_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproblem_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 82\u001b[0m, in \u001b[0;36mhypothesis_rank\u001b[0;34m(hypothesis_dict, problem_dict, selected_idx, scen_prob_multiplier)\u001b[0m\n\u001b[1;32m     80\u001b[0m scores_sorted \u001b[38;5;241m=\u001b[39m compute_top_scores(hypothesis_dict)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     selected_idx \u001b[38;5;241m=\u001b[39m \u001b[43mselect_hypothesis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscores_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscores_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhypothesis_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhypothesis_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproblem_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproblem_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscen_prob_multiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscen_prob_multiplier\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m max_score_problem_name \u001b[38;5;241m=\u001b[39m scores_sorted\u001b[38;5;241m.\u001b[39mindex[selected_idx]\n\u001b[1;32m     90\u001b[0m problem_info \u001b[38;5;241m=\u001b[39m problem_dict\u001b[38;5;241m.\u001b[39mget(max_score_problem_name, {})\n",
      "Cell \u001b[0;32mIn[5], line 57\u001b[0m, in \u001b[0;36mselect_hypothesis\u001b[0;34m(scores_sorted, hypothesis_dict, problem_dict, scen_prob_multiplier)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hypothesis_dict[problem_name]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minspired\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     56\u001b[0m     index_to_pick_pool_list\u001b[38;5;241m.\u001b[39mextend([j] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mproblem_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mproblem_name\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCENARIO_PROBLEM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     58\u001b[0m     index_to_pick_pool_list\u001b[38;5;241m.\u001b[39mextend([j] \u001b[38;5;241m*\u001b[39m scen_prob_multiplier)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m problem_dict[problem_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFEEDBACK_PROBLEM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "pickled_problem_name, new_hypothesis = hypothesis_rank(\n",
    "            hypothesis_dict=resp_dict,\n",
    "            problem_dict=  None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802e39d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Capture full essay context beyond 512 tokens': {'reason': 'The current model truncates essays to 512 tokens, losing potentially important concluding or introductory arguments that may influence QWK.',\n",
       "  'component': <HypothesisComponent.Model: 'Model'>,\n",
       "  'hypothesis': 'Replace the DeBERTa-v3 backbone with Longformer-base and fine-tune it using the same CORAL ordinal head, allowing up to 2 048 tokens so each essay is processed in a single pass without truncation.',\n",
       "  'evaluation': {'alignment_score': 9.0,\n",
       "   'impact_score': 8.0,\n",
       "   'novelty_score': 8.0,\n",
       "   'feasibility_score': 6.0,\n",
       "   'risk_reward_balance_score': 7.0}},\n",
       " 'Mitigate severe high-score class imbalance': {'reason': 'Rare 5/6 scores receive little representation, leading to poor recall and calibration for top marks.',\n",
       "  'component': <HypothesisComponent.Model: 'Model'>,\n",
       "  'hypothesis': 'Implement class-balanced focal CORAL loss with dynamic oversampling that duplicates essays of scores 5 and 6 at a 3× rate in each mini-batch to equalise positive/negative threshold counts.',\n",
       "  'evaluation': {'alignment_score': 9.0,\n",
       "   'impact_score': 7.0,\n",
       "   'novelty_score': 7.0,\n",
       "   'feasibility_score': 8.0,\n",
       "   'risk_reward_balance_score': 8.0}},\n",
       " 'Inject argumentative & discourse quality features': {'reason': 'Current engineered features are mostly surface-level; deeper discourse signals are missing.',\n",
       "  'component': <HypothesisComponent.FeatureEng: 'FeatureEng'>,\n",
       "  'hypothesis': 'Augment feature set with transformer-based argument-component counts (claims, evidence, rebuttals) extracted via a pretrained Argument Mining model, and concatenate these normalised counts to the existing engineered vector before the ordinal head.',\n",
       "  'evaluation': {'alignment_score': 9.0,\n",
       "   'impact_score': 6.0,\n",
       "   'novelty_score': 8.0,\n",
       "   'feasibility_score': 5.0,\n",
       "   'risk_reward_balance_score': 7.0}},\n",
       " 'Leverage full 5-fold cross-validation and model ensembling': {'reason': 'Only fold-0 is trained; generalisation is limited and unused data hurts performance.',\n",
       "  'component': <HypothesisComponent.Ensemble: 'Ensemble'>,\n",
       "  'hypothesis': 'Train the CORAL model on all 5 folds, save each checkpoint, and at inference average the per-essay expected scores from the five models before threshold mapping.',\n",
       "  'evaluation': {'alignment_score': 10.0,\n",
       "   'impact_score': 8.0,\n",
       "   'novelty_score': 6.0,\n",
       "   'feasibility_score': 7.0,\n",
       "   'risk_reward_balance_score': 8.0}},\n",
       " 'Exploit domain-adaptive pretraining of the base transformer on the essay corpus': {'reason': 'The generic DeBERTa encoder may not capture essay-specific linguistic patterns.',\n",
       "  'component': <HypothesisComponent.Model: 'Model'>,\n",
       "  'hypothesis': 'Perform an additional 10 000-step Masked Language Modelling pretraining of DeBERTa-v3-base on the concatenated train + test essays (no labels) before fine-tuning with the CORAL head.',\n",
       "  'evaluation': {'alignment_score': 8.0,\n",
       "   'impact_score': 8.0,\n",
       "   'novelty_score': 9.0,\n",
       "   'feasibility_score': 6.0,\n",
       "   'risk_reward_balance_score': 7.0}},\n",
       " 'Improve robustness of threshold mapping beyond single-fold optimisation': {'reason': 'Thresholds optimised on one fold may over-fit and not generalise to leaderboard data.',\n",
       "  'component': <HypothesisComponent.Workflow: 'Workflow'>,\n",
       "  'hypothesis': 'After training the 5-fold ensemble, learn global thresholds by minimising negative QWK on the concatenated validation predictions from all folds, then apply the same thresholds to ensemble predictions on the test set.',\n",
       "  'evaluation': {'alignment_score': 9.0,\n",
       "   'impact_score': 6.0,\n",
       "   'novelty_score': 7.0,\n",
       "   'feasibility_score': 9.0,\n",
       "   'risk_reward_balance_score': 9.0}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "resp_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "588ad51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_candidates =  str(json.dumps(resp_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57b548e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"Capture full essay context beyond 512 tokens\": {\\n    \"reason\": \"The current model truncates essays to 512 tokens, losing potentially important concluding or introductory arguments that may influence QWK.\",\\n    \"component\": \"Model\",\\n    \"hypothesis\": \"Replace the DeBERTa-v3 backbone with Longformer-base and fine-tune it using the same CORAL ordinal head, allowing up to 2 048 tokens so each essay is processed in a single pass without truncation.\",\\n    \"evaluation\": {\\n      \"alignment_score\": 9.0,\\n      \"impact_score\": 8.0,\\n      \"novelty_score\": 8.0,\\n      \"feasibility_score\": 6.0,\\n      \"risk_reward_balance_score\": 7.0\\n    }\\n  },\\n  \"Mitigate severe high-score class imbalance\": {\\n    \"reason\": \"Rare 5/6 scores receive little representation, leading to poor recall and calibration for top marks.\",\\n    \"component\": \"Model\",\\n    \"hypothesis\": \"Implement class-balanced focal CORAL loss with dynamic oversampling that duplicates essays of scores 5 and 6 at a 3\\\\u00d7 rate in each mini-batch to equalise positive/negative threshold counts.\",\\n    \"evaluation\": {\\n      \"alignment_score\": 9.0,\\n      \"impact_score\": 7.0,\\n      \"novelty_score\": 7.0,\\n      \"feasibility_score\": 8.0,\\n      \"risk_reward_balance_score\": 8.0\\n    }\\n  },\\n  \"Inject argumentative & discourse quality features\": {\\n    \"reason\": \"Current engineered features are mostly surface-level; deeper discourse signals are missing.\",\\n    \"component\": \"FeatureEng\",\\n    \"hypothesis\": \"Augment feature set with transformer-based argument-component counts (claims, evidence, rebuttals) extracted via a pretrained Argument Mining model, and concatenate these normalised counts to the existing engineered vector before the ordinal head.\",\\n    \"evaluation\": {\\n      \"alignment_score\": 9.0,\\n      \"impact_score\": 6.0,\\n      \"novelty_score\": 8.0,\\n      \"feasibility_score\": 5.0,\\n      \"risk_reward_balance_score\": 7.0\\n    }\\n  },\\n  \"Leverage full 5-fold cross-validation and model ensembling\": {\\n    \"reason\": \"Only fold-0 is trained; generalisation is limited and unused data hurts performance.\",\\n    \"component\": \"Ensemble\",\\n    \"hypothesis\": \"Train the CORAL model on all 5 folds, save each checkpoint, and at inference average the per-essay expected scores from the five models before threshold mapping.\",\\n    \"evaluation\": {\\n      \"alignment_score\": 10.0,\\n      \"impact_score\": 8.0,\\n      \"novelty_score\": 6.0,\\n      \"feasibility_score\": 7.0,\\n      \"risk_reward_balance_score\": 8.0\\n    }\\n  },\\n  \"Exploit domain-adaptive pretraining of the base transformer on the essay corpus\": {\\n    \"reason\": \"The generic DeBERTa encoder may not capture essay-specific linguistic patterns.\",\\n    \"component\": \"Model\",\\n    \"hypothesis\": \"Perform an additional 10 000-step Masked Language Modelling pretraining of DeBERTa-v3-base on the concatenated train + test essays (no labels) before fine-tuning with the CORAL head.\",\\n    \"evaluation\": {\\n      \"alignment_score\": 8.0,\\n      \"impact_score\": 8.0,\\n      \"novelty_score\": 9.0,\\n      \"feasibility_score\": 6.0,\\n      \"risk_reward_balance_score\": 7.0\\n    }\\n  },\\n  \"Improve robustness of threshold mapping beyond single-fold optimisation\": {\\n    \"reason\": \"Thresholds optimised on one fold may over-fit and not generalise to leaderboard data.\",\\n    \"component\": \"Workflow\",\\n    \"hypothesis\": \"After training the 5-fold ensemble, learn global thresholds by minimising negative QWK on the concatenated validation predictions from all folds, then apply the same thresholds to ensemble predictions on the test set.\",\\n    \"evaluation\": {\\n      \"alignment_score\": 9.0,\\n      \"impact_score\": 6.0,\\n      \"novelty_score\": 7.0,\\n      \"feasibility_score\": 9.0,\\n      \"risk_reward_balance_score\": 9.0\\n    }\\n  }\\n}'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd6da9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Capture full essay context beyond 512 tokens': {'reason': 'The current model truncates essays to 512 tokens, losing potentially important concluding or introductory arguments that may influence QWK.',\n",
       "  'component': <HypothesisComponent.Model: 'Model'>,\n",
       "  'hypothesis': 'Replace the DeBERTa-v3 backbone with Longformer-base and fine-tune it using the same CORAL ordinal head, allowing up to 2 048 tokens so each essay is processed in a single pass without truncation.',\n",
       "  'evaluation': {'alignment_score': 9.0,\n",
       "   'impact_score': 8.0,\n",
       "   'novelty_score': 8.0,\n",
       "   'feasibility_score': 6.0,\n",
       "   'risk_reward_balance_score': 7.0}},\n",
       " 'Mitigate severe high-score class imbalance': {'reason': 'Rare 5/6 scores receive little representation, leading to poor recall and calibration for top marks.',\n",
       "  'component': <HypothesisComponent.Model: 'Model'>,\n",
       "  'hypothesis': 'Implement class-balanced focal CORAL loss with dynamic oversampling that duplicates essays of scores 5 and 6 at a 3× rate in each mini-batch to equalise positive/negative threshold counts.',\n",
       "  'evaluation': {'alignment_score': 9.0,\n",
       "   'impact_score': 7.0,\n",
       "   'novelty_score': 7.0,\n",
       "   'feasibility_score': 8.0,\n",
       "   'risk_reward_balance_score': 8.0}},\n",
       " 'Inject argumentative & discourse quality features': {'reason': 'Current engineered features are mostly surface-level; deeper discourse signals are missing.',\n",
       "  'component': <HypothesisComponent.FeatureEng: 'FeatureEng'>,\n",
       "  'hypothesis': 'Augment feature set with transformer-based argument-component counts (claims, evidence, rebuttals) extracted via a pretrained Argument Mining model, and concatenate these normalised counts to the existing engineered vector before the ordinal head.',\n",
       "  'evaluation': {'alignment_score': 9.0,\n",
       "   'impact_score': 6.0,\n",
       "   'novelty_score': 8.0,\n",
       "   'feasibility_score': 5.0,\n",
       "   'risk_reward_balance_score': 7.0}},\n",
       " 'Leverage full 5-fold cross-validation and model ensembling': {'reason': 'Only fold-0 is trained; generalisation is limited and unused data hurts performance.',\n",
       "  'component': <HypothesisComponent.Ensemble: 'Ensemble'>,\n",
       "  'hypothesis': 'Train the CORAL model on all 5 folds, save each checkpoint, and at inference average the per-essay expected scores from the five models before threshold mapping.',\n",
       "  'evaluation': {'alignment_score': 10.0,\n",
       "   'impact_score': 8.0,\n",
       "   'novelty_score': 6.0,\n",
       "   'feasibility_score': 7.0,\n",
       "   'risk_reward_balance_score': 8.0}},\n",
       " 'Exploit domain-adaptive pretraining of the base transformer on the essay corpus': {'reason': 'The generic DeBERTa encoder may not capture essay-specific linguistic patterns.',\n",
       "  'component': <HypothesisComponent.Model: 'Model'>,\n",
       "  'hypothesis': 'Perform an additional 10 000-step Masked Language Modelling pretraining of DeBERTa-v3-base on the concatenated train + test essays (no labels) before fine-tuning with the CORAL head.',\n",
       "  'evaluation': {'alignment_score': 8.0,\n",
       "   'impact_score': 8.0,\n",
       "   'novelty_score': 9.0,\n",
       "   'feasibility_score': 6.0,\n",
       "   'risk_reward_balance_score': 7.0}},\n",
       " 'Improve robustness of threshold mapping beyond single-fold optimisation': {'reason': 'Thresholds optimised on one fold may over-fit and not generalise to leaderboard data.',\n",
       "  'component': <HypothesisComponent.Workflow: 'Workflow'>,\n",
       "  'hypothesis': 'After training the 5-fold ensemble, learn global thresholds by minimising negative QWK on the concatenated validation predictions from all folds, then apply the same thresholds to ensemble predictions on the test set.',\n",
       "  'evaluation': {'alignment_score': 9.0,\n",
       "   'impact_score': 6.0,\n",
       "   'novelty_score': 7.0,\n",
       "   'feasibility_score': 9.0,\n",
       "   'risk_reward_balance_score': 9.0}}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00fbff83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Capture full essay context beyond 512 tokens'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(resp_dict.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "293761aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reason': 'The current model truncates essays to 512 tokens, losing potentially important concluding or introductory arguments that may influence QWK.',\n",
       " 'component': <HypothesisComponent.Model: 'Model'>,\n",
       " 'hypothesis': 'Replace the DeBERTa-v3 backbone with Longformer-base and fine-tune it using the same CORAL ordinal head, allowing up to 2 048 tokens so each essay is processed in a single pass without truncation.',\n",
       " 'evaluation': {'alignment_score': 9.0,\n",
       "  'impact_score': 8.0,\n",
       "  'novelty_score': 8.0,\n",
       "  'feasibility_score': 6.0,\n",
       "  'risk_reward_balance_score': 7.0}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(resp_dict.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa7e6989",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_sorted = compute_top_scores(resp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38071e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Exploit domain-adaptive pretraining of the base transformer on the essay corpus    7.9\n",
       "Leverage full 5-fold cross-validation and model ensembling                         7.9\n",
       "Capture full essay context beyond 512 tokens                                       7.9\n",
       "Mitigate severe high-score class imbalance                                         7.6\n",
       "Improve robustness of threshold mapping beyond single-fold optimisation            7.4\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ae35cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Capture full essay context beyond 512 tokens'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_score_problem_name = scores_sorted.index[2]\n",
    "max_score_problem_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64383d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hypothesis = DSHypothesis(component=resp_dict[max_score_problem_name].get(\"component\", \"Model\"),hypothesis=resp_dict[max_score_problem_name].get(\"hypothesis\", \"Hypothesis not provided\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6a3461a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HypothesisComponent.Ensemble: 'Ensemble'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_dict[max_score_problem_name].get(\"component\", \"Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1551349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HypothesisComponent.Ensemble: 'Ensemble'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HypothesisComponent.Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c1f793c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train the CORAL model on all 5 folds, save each checkpoint, and at inference average the per-essay expected scores from the five models before threshold mapping.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_dict[max_score_problem_name].get(\"hypothesis\", \"Hypothesis not provided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90ee28ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reason': 'The current model truncates essays to 512 tokens, losing potentially important concluding or introductory arguments that may influence QWK.',\n",
       " 'component': <HypothesisComponent.Model: 'Model'>,\n",
       " 'hypothesis': 'Replace the DeBERTa-v3 backbone with Longformer-base and fine-tune it using the same CORAL ordinal head, allowing up to 2 048 tokens so each essay is processed in a single pass without truncation.',\n",
       " 'evaluation': {'alignment_score': 9.0,\n",
       "  'impact_score': 8.0,\n",
       "  'novelty_score': 8.0,\n",
       "  'feasibility_score': 6.0,\n",
       "  'risk_reward_balance_score': 7.0}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_dict[max_score_problem_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bb3ced2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reason': 'Only fold-0 is trained; generalisation is limited and unused data hurts performance.',\n",
       " 'component': <HypothesisComponent.Ensemble: 'Ensemble'>,\n",
       " 'hypothesis': 'Train the CORAL model on all 5 folds, save each checkpoint, and at inference average the per-essay expected scores from the five models before threshold mapping.',\n",
       " 'evaluation': {'alignment_score': 10.0,\n",
       "  'impact_score': 8.0,\n",
       "  'novelty_score': 6.0,\n",
       "  'feasibility_score': 7.0,\n",
       "  'risk_reward_balance_score': 8.0}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_dict[max_score_problem_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13ea0243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdagent.scenarios.data_science.proposal.exp_gen.base.DSHypothesis"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DSHypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cb1cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses=([new_hypothesis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0449f602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<rdagent.scenarios.data_science.proposal.exp_gen.base.DSHypothesis at 0x7fc334f89600>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9c00c78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_score(eval_dict: dict,\n",
    "                        weights: dict | None = None,\n",
    "                        normalize: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    根据 evaluation 子字段计算总评分。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eval_dict : dict\n",
    "        形如 {\n",
    "            \"alignment_score\": 9.0,\n",
    "            \"impact_score\": 7.0,\n",
    "            ...\n",
    "        }\n",
    "    weights : dict | None\n",
    "        每个维度的权重，例如：\n",
    "            {\n",
    "                \"alignment_score\": 1.0,\n",
    "                \"impact_score\": 1.5,\n",
    "                \"novelty_score\": 1.0,\n",
    "                \"feasibility_score\": 1.2,\n",
    "                \"risk_reward_balance_score\": 1.0,\n",
    "            }\n",
    "        若为 None，则各维度权重均为 1。\n",
    "    normalize : bool\n",
    "        若为 True，则先把各分值映射到 [0,1] 再加权求和\n",
    "        （在不同模型输出量纲不一致时有用）。\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        加权总分\n",
    "    \"\"\"\n",
    "    # 默认权重：所有维度等权\n",
    "    if weights is None:\n",
    "        weights = {k: 1.0 for k in eval_dict.keys()}\n",
    "\n",
    "    # 可选：归一化到 [0,1]\n",
    "    if normalize:\n",
    "        # 这里假设原始分数区间为 [1,10]\n",
    "        norm_dict = {k: (v - 1) / 9 for k, v in eval_dict.items()}\n",
    "    else:\n",
    "        norm_dict = eval_dict\n",
    "\n",
    "    total = 0.0\n",
    "    for k, v in norm_dict.items():\n",
    "        w = weights.get(k, 1.0)  # 若缺某字段权重则视为 1\n",
    "        total += w * v\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f42f23a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "top_k_per_component = 2\n",
    "grouped = defaultdict(list)\n",
    "for caption, h in resp_dict.items():\n",
    "    score = compute_total_score(h[\"evaluation\"])\n",
    "    grouped[h[\"component\"]].append((caption, h, score))\n",
    "\n",
    "selected = []\n",
    "for comp, lst in grouped.items():\n",
    "    lst = sorted(lst, key=lambda x: x[2], reverse=True)\n",
    "    selected += lst[:top_k_per_component]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f2a21dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Capture full essay context beyond 512 tokens',\n",
       "  {'reason': 'The current SOTA truncates each essay to 512 word-piece tokens, discarding up to 75 % of the text for long essays. Important evidence for high-level argument quality may lie in later paragraphs, so QWK is capped by this information loss.',\n",
       "   'component': <HypothesisComponent.Model: 'Model'>,\n",
       "   'hypothesis': 'Replace the single-pass 512-token input with a sliding-window inference strategy: split each essay into overlapping 350-token windows (stride = 300); pass every window through the existing DeBERTa-v3+CORAL model; average the per-threshold logits across windows, then apply the same learned thresholds to obtain the final 1-6 score.',\n",
       "   'evaluation': {'alignment_score': 9.0,\n",
       "    'impact_score': 7.0,\n",
       "    'novelty_score': 6.0,\n",
       "    'feasibility_score': 8.0,\n",
       "    'risk_reward_balance_score': 8.0}},\n",
       "  38.0),\n",
       " ('Mitigate severe high-score class imbalance',\n",
       "  {'reason': 'Classes 5 and 6 represent < 6 % of the data, leading to poor recall and calibration for top scores despite focal-CB loss.',\n",
       "   'component': <HypothesisComponent.Model: 'Model'>,\n",
       "   'hypothesis': 'Augment the training data with minority-focused up-sampling: duplicate essays with scores 5 and 6 four times in each epoch’s sampling pool, and add a ordinal-margin regularisation term that penalises logits where P(score>k) is within 0.05 of the decision boundary for k≥4.',\n",
       "   'evaluation': {'alignment_score': 9.0,\n",
       "    'impact_score': 6.0,\n",
       "    'novelty_score': 7.0,\n",
       "    'feasibility_score': 8.0,\n",
       "    'risk_reward_balance_score': 8.0}},\n",
       "  38.0),\n",
       " ('Inject argumentative & discourse quality features',\n",
       "  {'reason': 'The feature set includes readability and simple counts but lacks argumentative structure signals known to correlate with holistic scores.',\n",
       "   'component': <HypothesisComponent.FeatureEng: 'FeatureEng'>,\n",
       "   'hypothesis': 'Add a pre-trained Argument Mining classifier (e.g., ‘args-me’ model) to detect counts of claims, premises, and major discourse relations per essay, z-score them, and concatenate to the engineered feature vector before the CORAL head.',\n",
       "   'evaluation': {'alignment_score': 8.0,\n",
       "    'impact_score': 6.0,\n",
       "    'novelty_score': 8.0,\n",
       "    'feasibility_score': 7.0,\n",
       "    'risk_reward_balance_score': 7.0}},\n",
       "  36.0),\n",
       " ('Single-fold training limits generalisation; need multi-fold ensembling',\n",
       "  {'reason': 'Current pipeline trains only on fold 0, leaving 80 % of data unused during training and no ensemble variance reduction, limiting validation unbiasedness.',\n",
       "   'component': <HypothesisComponent.Ensemble: 'Ensemble'>,\n",
       "   'hypothesis': 'Train the existing model on all five stratified folds (same hyper-params), save each best checkpoint, and average their threshold-logits on test before applying the global optimised thresholds to produce final scores.',\n",
       "   'evaluation': {'alignment_score': 9.0,\n",
       "    'impact_score': 8.0,\n",
       "    'novelty_score': 5.0,\n",
       "    'feasibility_score': 7.0,\n",
       "    'risk_reward_balance_score': 8.0}},\n",
       "  37.0),\n",
       " ('Slow engineered-feature pipeline (grammar check) inflates runtime and is brittle when Java is missing',\n",
       "  {'reason': 'LanguageTool grammar feature adds ≥1 h and fails on some systems, slowing iteration and risking broken runs.',\n",
       "   'component': <HypothesisComponent.Workflow: 'Workflow'>,\n",
       "   'hypothesis': 'Remove LanguageTool dependency and replace ‘grammar_error_count’ with the faster spaCy rule-based grammar heuristic (e.g., noun-verb agreement detector) implemented locally, cutting feature extraction time by >80 %.',\n",
       "   'evaluation': {'alignment_score': 9.0,\n",
       "    'impact_score': 5.0,\n",
       "    'novelty_score': 6.0,\n",
       "    'feasibility_score': 9.0,\n",
       "    'risk_reward_balance_score': 8.0}},\n",
       "  37.0)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5701529f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Capture full essay context beyond 512 tokens': {'reason': 'The current SOTA truncates each essay to 512 word-piece tokens, discarding up to 75 % of the text for long essays. Important evidence for high-level argument quality may lie in later paragraphs, so QWK is capped by this information loss.',\n",
       "  'component': <HypothesisComponent.Model: 'Model'>,\n",
       "  'hypothesis': 'Replace the single-pass 512-token input with a sliding-window inference strategy: split each essay into overlapping 350-token windows (stride = 300); pass every window through the existing DeBERTa-v3+CORAL model; average the per-threshold logits across windows, then apply the same learned thresholds to obtain the final 1-6 score.',\n",
       "  'evaluation': {'alignment_score': 9.0,\n",
       "   'impact_score': 7.0,\n",
       "   'novelty_score': 6.0,\n",
       "   'feasibility_score': 8.0,\n",
       "   'risk_reward_balance_score': 8.0}},\n",
       " 'Mitigate severe high-score class imbalance': {'reason': 'Classes 5 and 6 represent < 6 % of the data, leading to poor recall and calibration for top scores despite focal-CB loss.',\n",
       "  'component': <HypothesisComponent.Model: 'Model'>,\n",
       "  'hypothesis': 'Augment the training data with minority-focused up-sampling: duplicate essays with scores 5 and 6 four times in each epoch’s sampling pool, and add a ordinal-margin regularisation term that penalises logits where P(score>k) is within 0.05 of the decision boundary for k≥4.',\n",
       "  'evaluation': {'alignment_score': 9.0,\n",
       "   'impact_score': 6.0,\n",
       "   'novelty_score': 7.0,\n",
       "   'feasibility_score': 8.0,\n",
       "   'risk_reward_balance_score': 8.0}},\n",
       " 'Inject argumentative & discourse quality features': {'reason': 'The feature set includes readability and simple counts but lacks argumentative structure signals known to correlate with holistic scores.',\n",
       "  'component': <HypothesisComponent.FeatureEng: 'FeatureEng'>,\n",
       "  'hypothesis': 'Add a pre-trained Argument Mining classifier (e.g., ‘args-me’ model) to detect counts of claims, premises, and major discourse relations per essay, z-score them, and concatenate to the engineered feature vector before the CORAL head.',\n",
       "  'evaluation': {'alignment_score': 8.0,\n",
       "   'impact_score': 6.0,\n",
       "   'novelty_score': 8.0,\n",
       "   'feasibility_score': 7.0,\n",
       "   'risk_reward_balance_score': 7.0}},\n",
       " 'Single-fold training limits generalisation; need multi-fold ensembling': {'reason': 'Current pipeline trains only on fold 0, leaving 80 % of data unused during training and no ensemble variance reduction, limiting validation unbiasedness.',\n",
       "  'component': <HypothesisComponent.Ensemble: 'Ensemble'>,\n",
       "  'hypothesis': 'Train the existing model on all five stratified folds (same hyper-params), save each best checkpoint, and average their threshold-logits on test before applying the global optimised thresholds to produce final scores.',\n",
       "  'evaluation': {'alignment_score': 9.0,\n",
       "   'impact_score': 8.0,\n",
       "   'novelty_score': 5.0,\n",
       "   'feasibility_score': 7.0,\n",
       "   'risk_reward_balance_score': 8.0}},\n",
       " 'Leverage larger or domain-specialised language models to better capture essay nuances': {'reason': 'DeBERTa-v3-base (86 M params) might underfit nuanced writing quality signals; larger encoders often improve textual understanding.',\n",
       "  'component': <HypothesisComponent.Model: 'Model'>,\n",
       "  'hypothesis': 'Swap the backbone to DeBERTa-v3-large (×2 parameters) while keeping identical CORAL head and training regime, but freeze bottom 9 layers for the first epoch to fit VRAM within 16 GB.',\n",
       "  'evaluation': {'alignment_score': 8.0,\n",
       "   'impact_score': 7.0,\n",
       "   'novelty_score': 6.0,\n",
       "   'feasibility_score': 6.0,\n",
       "   'risk_reward_balance_score': 6.0}},\n",
       " 'Slow engineered-feature pipeline (grammar check) inflates runtime and is brittle when Java is missing': {'reason': 'LanguageTool grammar feature adds ≥1 h and fails on some systems, slowing iteration and risking broken runs.',\n",
       "  'component': <HypothesisComponent.Workflow: 'Workflow'>,\n",
       "  'hypothesis': 'Remove LanguageTool dependency and replace ‘grammar_error_count’ with the faster spaCy rule-based grammar heuristic (e.g., noun-verb agreement detector) implemented locally, cutting feature extraction time by >80 %.',\n",
       "  'evaluation': {'alignment_score': 9.0,\n",
       "   'impact_score': 5.0,\n",
       "   'novelty_score': 6.0,\n",
       "   'feasibility_score': 9.0,\n",
       "   'risk_reward_balance_score': 8.0}}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be96755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_hypotheses(hypotheses_dict, top_k_per_component=2):\n",
    "    grouped = defaultdict(list)\n",
    "    for caption, h in hypotheses_dict.items():\n",
    "        score = compute_total_score(h[\"evaluation\"])\n",
    "        grouped[h[\"component\"]].append((caption, h, score))\n",
    "\n",
    "    selected = []\n",
    "    for comp, lst in grouped.items():\n",
    "        lst = sorted(lst, key=lambda x: x[2], reverse=True)\n",
    "        selected += lst[:top_k_per_component]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282a7717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_hypotheses(hypotheses_dict, top_k_per_component=2):\n",
    "    # 1. 每个 component 选 top-K\n",
    "    grouped = defaultdict(list)\n",
    "    for caption, h in hypotheses_dict.items():\n",
    "        score = compute_total_score(h[\"evaluation\"])\n",
    "        grouped[h[\"component\"]].append((caption, h, score))\n",
    "\n",
    "    selected = []\n",
    "    for comp, lst in grouped.items():\n",
    "        lst = sorted(lst, key=lambda x: x[2], reverse=True)\n",
    "        selected += lst[:top_k_per_component]\n",
    "\n",
    "    # 2. 聚类去重（假设你有 sentence embedding 模型）\n",
    "    texts = [h[\"hypothesis\"] for _, h, _ in selected]\n",
    "    embeddings = embed(texts)  # shape (N, D)\n",
    "    clusters = cluster_embeddings(embeddings)  # 输出 cluster_id for each\n",
    "\n",
    "    clustered = defaultdict(list)\n",
    "    for (caption, h, score), cluster_id in zip(selected, clusters):\n",
    "        clustered[cluster_id].append((caption, h, score))\n",
    "\n",
    "    # 每簇取得分最高的\n",
    "    deduplicated = [max(clustered[cluster], key=lambda x: x[2]) for cluster in clustered]\n",
    "\n",
    "    # 3. Pareto 最优筛选\n",
    "    vectors = [\n",
    "        (\n",
    "            h[\"evaluation\"][\"alignment_score\"],\n",
    "            h[\"evaluation\"][\"impact_score\"],\n",
    "            h[\"evaluation\"][\"novelty_score\"],\n",
    "            h[\"evaluation\"][\"feasibility_score\"],\n",
    "            h[\"evaluation\"][\"risk_reward_balance_score\"],\n",
    "        )\n",
    "        for _, h, _ in deduplicated\n",
    "    ]\n",
    "    pareto_mask = pareto_front_mask(vectors)\n",
    "    pareto_hypotheses = [deduplicated[i] for i, ok in enumerate(pareto_mask) if ok]\n",
    "\n",
    "    final = sorted(pareto_hypotheses, key=lambda x: x[2], reverse=True)\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2276ad6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Capture full essay context beyond 512 tokens': {'reason': 'The current SOTA truncates each essay to 512 word-piece tokens, discarding up to 75 % of the text for long essays. Important evidence for high-level argument quality may lie in later paragraphs, so QWK is capped by this information loss.',\n",
       "  'component': <HypothesisComponent.Model: 'Model'>,\n",
       "  'hypothesis': 'Replace the single-pass 512-token input with a sliding-window inference strategy: split each essay into overlapping 350-token windows (stride = 300); pass every window through the existing DeBERTa-v3+CORAL model; average the per-threshold logits across windows, then apply the same learned thresholds to obtain the final 1-6 score.',\n",
       "  'evaluation': {'alignment_score': 9.0,\n",
       "   'impact_score': 7.0,\n",
       "   'novelty_score': 6.0,\n",
       "   'feasibility_score': 8.0,\n",
       "   'risk_reward_balance_score': 8.0}},\n",
       " 'Mitigate severe high-score class imbalance': {'reason': 'Classes 5 and 6 represent < 6 % of the data, leading to poor recall and calibration for top scores despite focal-CB loss.',\n",
       "  'component': <HypothesisComponent.Model: 'Model'>,\n",
       "  'hypothesis': 'Augment the training data with minority-focused up-sampling: duplicate essays with scores 5 and 6 four times in each epoch’s sampling pool, and add a ordinal-margin regularisation term that penalises logits where P(score>k) is within 0.05 of the decision boundary for k≥4.',\n",
       "  'evaluation': {'alignment_score': 9.0,\n",
       "   'impact_score': 6.0,\n",
       "   'novelty_score': 7.0,\n",
       "   'feasibility_score': 8.0,\n",
       "   'risk_reward_balance_score': 8.0}},\n",
       " 'Inject argumentative & discourse quality features': {'reason': 'The feature set includes readability and simple counts but lacks argumentative structure signals known to correlate with holistic scores.',\n",
       "  'component': <HypothesisComponent.FeatureEng: 'FeatureEng'>,\n",
       "  'hypothesis': 'Add a pre-trained Argument Mining classifier (e.g., ‘args-me’ model) to detect counts of claims, premises, and major discourse relations per essay, z-score them, and concatenate to the engineered feature vector before the CORAL head.',\n",
       "  'evaluation': {'alignment_score': 8.0,\n",
       "   'impact_score': 6.0,\n",
       "   'novelty_score': 8.0,\n",
       "   'feasibility_score': 7.0,\n",
       "   'risk_reward_balance_score': 7.0}},\n",
       " 'Single-fold training limits generalisation; need multi-fold ensembling': {'reason': 'Current pipeline trains only on fold 0, leaving 80 % of data unused during training and no ensemble variance reduction, limiting validation unbiasedness.',\n",
       "  'component': <HypothesisComponent.Ensemble: 'Ensemble'>,\n",
       "  'hypothesis': 'Train the existing model on all five stratified folds (same hyper-params), save each best checkpoint, and average their threshold-logits on test before applying the global optimised thresholds to produce final scores.',\n",
       "  'evaluation': {'alignment_score': 9.0,\n",
       "   'impact_score': 8.0,\n",
       "   'novelty_score': 5.0,\n",
       "   'feasibility_score': 7.0,\n",
       "   'risk_reward_balance_score': 8.0}},\n",
       " 'Leverage larger or domain-specialised language models to better capture essay nuances': {'reason': 'DeBERTa-v3-base (86 M params) might underfit nuanced writing quality signals; larger encoders often improve textual understanding.',\n",
       "  'component': <HypothesisComponent.Model: 'Model'>,\n",
       "  'hypothesis': 'Swap the backbone to DeBERTa-v3-large (×2 parameters) while keeping identical CORAL head and training regime, but freeze bottom 9 layers for the first epoch to fit VRAM within 16 GB.',\n",
       "  'evaluation': {'alignment_score': 8.0,\n",
       "   'impact_score': 7.0,\n",
       "   'novelty_score': 6.0,\n",
       "   'feasibility_score': 6.0,\n",
       "   'risk_reward_balance_score': 6.0}},\n",
       " 'Slow engineered-feature pipeline (grammar check) inflates runtime and is brittle when Java is missing': {'reason': 'LanguageTool grammar feature adds ≥1 h and fails on some systems, slowing iteration and risking broken runs.',\n",
       "  'component': <HypothesisComponent.Workflow: 'Workflow'>,\n",
       "  'hypothesis': 'Remove LanguageTool dependency and replace ‘grammar_error_count’ with the faster spaCy rule-based grammar heuristic (e.g., noun-verb agreement detector) implemented locally, cutting feature extraction time by >80 %.',\n",
       "  'evaluation': {'alignment_score': 9.0,\n",
       "   'impact_score': 5.0,\n",
       "   'novelty_score': 6.0,\n",
       "   'feasibility_score': 9.0,\n",
       "   'risk_reward_balance_score': 8.0}}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7c1f941e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Capture full essay context beyond 512 tokens', 'Mitigate severe high-score class imbalance', 'Inject argumentative & discourse quality features', 'Single-fold training limits generalisation; need multi-fold ensembling', 'Leverage larger or domain-specialised language models to better capture essay nuances', 'Slow engineered-feature pipeline (grammar check) inflates runtime and is brittle when Java is missing'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2e2f1dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Overfitting on some validation folds': {'reason': 'Cross-validation scores show noticeably higher errors on particular folds, indicating that the LightGBM model memorises fold-specific noise despite the existing basic regularisation settings. This overfitting prevents further gains and compromises leaderboard generalisation. Past experiments have tuned features and added rudimentary stacking but have not systematically optimised LightGBM’s regularisation knobs. Reducing variance without hurting bias is essential to make any future ensembling worthwhile.',\n",
       "  'component': <HypothesisComponent.Model: 'Model'>,\n",
       "  'hypothesis': 'Run an Optuna hyperparameter optimisation that specifically targets LightGBM’s regularisation controls (min_child_samples ≥ 30, lambda_l1 in [0,2], lambda_l2 in [0,5], feature_fraction ∈ [0.6,0.9], bagging_fraction ∈ [0.6,0.9], bagging_freq ≥ 1) with early stopping on out-of-fold RMSE, and retrain the best-found configuration to curb fold-level overfitting.',\n",
       "  'evaluation': {'alignment_score': 8.0,\n",
       "   'impact_score': 7.0,\n",
       "   'novelty_score': 6.0,\n",
       "   'feasibility_score': 8.0,\n",
       "   'risk_reward_balance_score': 7.0}},\n",
       " \"High cardinality in 'neighborhood' categorical variable\": {'reason': 'The ‘neighborhood’ column contains many unique categories, leading LightGBM to create sparse one-hot splits that add noise and increase model complexity. This can inflate variance and obscure informative signal, contributing to the mild overfitting observed. No specialised encoding has been applied so far.',\n",
       "  'component': <HypothesisComponent.FeatureEng: 'FeatureEng'>,\n",
       "  'hypothesis': 'Replace the raw ‘neighborhood’ categorical feature with leave-one-out target encoding (adding Gaussian noise during training to prevent leakage) so the model uses a smoothed numerical representation instead of high-cardinality splits.',\n",
       "  'evaluation': {'alignment_score': 9.0,\n",
       "   'impact_score': 6.0,\n",
       "   'novelty_score': 7.0,\n",
       "   'feasibility_score': 8.0,\n",
       "   'risk_reward_balance_score': 7.0}},\n",
       " 'Model performance saturates with simple stacking': {'reason': 'Adding a Ridge meta-learner on top of LightGBM only marginally improved CV RMSE and introduced overfitting, implying limited model diversity. Further improvements likely require a stronger, complementary model rather than more linear stacking.',\n",
       "  'component': <HypothesisComponent.Ensemble: 'Ensemble'>,\n",
       "  'hypothesis': 'Train a CatBoost regressor (handling categorical variables natively) with tuned depth and learning_rate, then create a simple mean-ensemble of CatBoost and the tuned LightGBM predictions to increase model diversity.',\n",
       "  'evaluation': {'alignment_score': 8.0,\n",
       "   'impact_score': 7.0,\n",
       "   'novelty_score': 7.0,\n",
       "   'feasibility_score': 6.0,\n",
       "   'risk_reward_balance_score': 7.0}}}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diclist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0327c359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-16 09:13:42.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrdagent.oai.backend.litellm\u001b[0m:\u001b[36m_create_chat_completion_inner_function\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96msystem\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96mYou are a senior machine learning researcher working on improving Kaggle competition pipelines. \n",
      "Your job is to propose and evaluate testable hypotheses that could help improve the current SOTA implementation.\n",
      "Be specific, concise, and use competition insight to ground your ideas.\u001b[0m\n",
      "\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96muser\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96m\n",
      "# Scenario Description\n",
      "The competition is a tabular regression problem predicting house prices. The metric is RMSE. \n",
      "The dataset includes structured features like location, size, number of rooms, and age.\n",
      "\n",
      "# Previous Experiments and Feedbacks\n",
      "1. Used LightGBM with basic features — RMSE: 0.145\n",
      "2. Added polynomial features — RMSE: 0.142\n",
      "3. Tried stacking with Ridge — RMSE: 0.141 but overfitted on fold 3\n",
      "\n",
      "# Current SOTA Implementation\n",
      "The current pipeline uses LightGBM and some feature engineering. It performs decently but still overfits slightly.\n",
      "\n",
      "# Identified Challenges\n",
      "1. Overfitting on some validation folds\n",
      "2. High cardinality in 'neighborhood' categorical variable\n",
      "3. Model performance saturates with simple stacking\n",
      "\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-16 09:13:43.180\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m518\u001b[0m - \u001b[33m\u001b[1mlitellm.UnsupportedParamsError: openai does not support parameters: ['top_p'], for model=o3. To drop these, set `litellm.drop_params=True` or for proxy:\n",
      "\n",
      "`litellm_settings:\n",
      " drop_params: true`\n",
      ". \n",
      " If you want to use these params dynamically send allowed_openai_params=['top_p'] in your request.\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:43.201\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mRetrying 1th time...\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:43.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrdagent.oai.backend.litellm\u001b[0m:\u001b[36m_create_chat_completion_inner_function\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96msystem\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96mYou are a senior machine learning researcher working on improving Kaggle competition pipelines. \n",
      "Your job is to propose and evaluate testable hypotheses that could help improve the current SOTA implementation.\n",
      "Be specific, concise, and use competition insight to ground your ideas.\u001b[0m\n",
      "\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96muser\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96m\n",
      "# Scenario Description\n",
      "The competition is a tabular regression problem predicting house prices. The metric is RMSE. \n",
      "The dataset includes structured features like location, size, number of rooms, and age.\n",
      "\n",
      "# Previous Experiments and Feedbacks\n",
      "1. Used LightGBM with basic features — RMSE: 0.145\n",
      "2. Added polynomial features — RMSE: 0.142\n",
      "3. Tried stacking with Ridge — RMSE: 0.141 but overfitted on fold 3\n",
      "\n",
      "# Current SOTA Implementation\n",
      "The current pipeline uses LightGBM and some feature engineering. It performs decently but still overfits slightly.\n",
      "\n",
      "# Identified Challenges\n",
      "1. Overfitting on some validation folds\n",
      "2. High cardinality in 'neighborhood' categorical variable\n",
      "3. Model performance saturates with simple stacking\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:44.246\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m518\u001b[0m - \u001b[33m\u001b[1mlitellm.UnsupportedParamsError: openai does not support parameters: ['top_p'], for model=o3. To drop these, set `litellm.drop_params=True` or for proxy:\n",
      "\n",
      "`litellm_settings:\n",
      " drop_params: true`\n",
      ". \n",
      " If you want to use these params dynamically send allowed_openai_params=['top_p'] in your request.\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:44.265\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mRetrying 2th time...\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:44.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrdagent.oai.backend.litellm\u001b[0m:\u001b[36m_create_chat_completion_inner_function\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96msystem\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96mYou are a senior machine learning researcher working on improving Kaggle competition pipelines. \n",
      "Your job is to propose and evaluate testable hypotheses that could help improve the current SOTA implementation.\n",
      "Be specific, concise, and use competition insight to ground your ideas.\u001b[0m\n",
      "\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96muser\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96m\n",
      "# Scenario Description\n",
      "The competition is a tabular regression problem predicting house prices. The metric is RMSE. \n",
      "The dataset includes structured features like location, size, number of rooms, and age.\n",
      "\n",
      "# Previous Experiments and Feedbacks\n",
      "1. Used LightGBM with basic features — RMSE: 0.145\n",
      "2. Added polynomial features — RMSE: 0.142\n",
      "3. Tried stacking with Ridge — RMSE: 0.141 but overfitted on fold 3\n",
      "\n",
      "# Current SOTA Implementation\n",
      "The current pipeline uses LightGBM and some feature engineering. It performs decently but still overfits slightly.\n",
      "\n",
      "# Identified Challenges\n",
      "1. Overfitting on some validation folds\n",
      "2. High cardinality in 'neighborhood' categorical variable\n",
      "3. Model performance saturates with simple stacking\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:45.311\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m518\u001b[0m - \u001b[33m\u001b[1mlitellm.UnsupportedParamsError: openai does not support parameters: ['top_p'], for model=o3. To drop these, set `litellm.drop_params=True` or for proxy:\n",
      "\n",
      "`litellm_settings:\n",
      " drop_params: true`\n",
      ". \n",
      " If you want to use these params dynamically send allowed_openai_params=['top_p'] in your request.\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:45.331\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mRetrying 3th time...\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:45.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrdagent.oai.backend.litellm\u001b[0m:\u001b[36m_create_chat_completion_inner_function\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96msystem\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96mYou are a senior machine learning researcher working on improving Kaggle competition pipelines. \n",
      "Your job is to propose and evaluate testable hypotheses that could help improve the current SOTA implementation.\n",
      "Be specific, concise, and use competition insight to ground your ideas.\u001b[0m\n",
      "\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96muser\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96m\n",
      "# Scenario Description\n",
      "The competition is a tabular regression problem predicting house prices. The metric is RMSE. \n",
      "The dataset includes structured features like location, size, number of rooms, and age.\n",
      "\n",
      "# Previous Experiments and Feedbacks\n",
      "1. Used LightGBM with basic features — RMSE: 0.145\n",
      "2. Added polynomial features — RMSE: 0.142\n",
      "3. Tried stacking with Ridge — RMSE: 0.141 but overfitted on fold 3\n",
      "\n",
      "# Current SOTA Implementation\n",
      "The current pipeline uses LightGBM and some feature engineering. It performs decently but still overfits slightly.\n",
      "\n",
      "# Identified Challenges\n",
      "1. Overfitting on some validation folds\n",
      "2. High cardinality in 'neighborhood' categorical variable\n",
      "3. Model performance saturates with simple stacking\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:46.376\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m518\u001b[0m - \u001b[33m\u001b[1mlitellm.UnsupportedParamsError: openai does not support parameters: ['top_p'], for model=o3. To drop these, set `litellm.drop_params=True` or for proxy:\n",
      "\n",
      "`litellm_settings:\n",
      " drop_params: true`\n",
      ". \n",
      " If you want to use these params dynamically send allowed_openai_params=['top_p'] in your request.\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:46.395\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mRetrying 4th time...\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:46.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrdagent.oai.backend.litellm\u001b[0m:\u001b[36m_create_chat_completion_inner_function\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96msystem\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96mYou are a senior machine learning researcher working on improving Kaggle competition pipelines. \n",
      "Your job is to propose and evaluate testable hypotheses that could help improve the current SOTA implementation.\n",
      "Be specific, concise, and use competition insight to ground your ideas.\u001b[0m\n",
      "\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96muser\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96m\n",
      "# Scenario Description\n",
      "The competition is a tabular regression problem predicting house prices. The metric is RMSE. \n",
      "The dataset includes structured features like location, size, number of rooms, and age.\n",
      "\n",
      "# Previous Experiments and Feedbacks\n",
      "1. Used LightGBM with basic features — RMSE: 0.145\n",
      "2. Added polynomial features — RMSE: 0.142\n",
      "3. Tried stacking with Ridge — RMSE: 0.141 but overfitted on fold 3\n",
      "\n",
      "# Current SOTA Implementation\n",
      "The current pipeline uses LightGBM and some feature engineering. It performs decently but still overfits slightly.\n",
      "\n",
      "# Identified Challenges\n",
      "1. Overfitting on some validation folds\n",
      "2. High cardinality in 'neighborhood' categorical variable\n",
      "3. Model performance saturates with simple stacking\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:47.441\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m518\u001b[0m - \u001b[33m\u001b[1mlitellm.UnsupportedParamsError: openai does not support parameters: ['top_p'], for model=o3. To drop these, set `litellm.drop_params=True` or for proxy:\n",
      "\n",
      "`litellm_settings:\n",
      " drop_params: true`\n",
      ". \n",
      " If you want to use these params dynamically send allowed_openai_params=['top_p'] in your request.\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:47.461\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mRetrying 5th time...\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:47.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrdagent.oai.backend.litellm\u001b[0m:\u001b[36m_create_chat_completion_inner_function\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96msystem\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96mYou are a senior machine learning researcher working on improving Kaggle competition pipelines. \n",
      "Your job is to propose and evaluate testable hypotheses that could help improve the current SOTA implementation.\n",
      "Be specific, concise, and use competition insight to ground your ideas.\u001b[0m\n",
      "\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96muser\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96m\n",
      "# Scenario Description\n",
      "The competition is a tabular regression problem predicting house prices. The metric is RMSE. \n",
      "The dataset includes structured features like location, size, number of rooms, and age.\n",
      "\n",
      "# Previous Experiments and Feedbacks\n",
      "1. Used LightGBM with basic features — RMSE: 0.145\n",
      "2. Added polynomial features — RMSE: 0.142\n",
      "3. Tried stacking with Ridge — RMSE: 0.141 but overfitted on fold 3\n",
      "\n",
      "# Current SOTA Implementation\n",
      "The current pipeline uses LightGBM and some feature engineering. It performs decently but still overfits slightly.\n",
      "\n",
      "# Identified Challenges\n",
      "1. Overfitting on some validation folds\n",
      "2. High cardinality in 'neighborhood' categorical variable\n",
      "3. Model performance saturates with simple stacking\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:48.507\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m518\u001b[0m - \u001b[33m\u001b[1mlitellm.UnsupportedParamsError: openai does not support parameters: ['top_p'], for model=o3. To drop these, set `litellm.drop_params=True` or for proxy:\n",
      "\n",
      "`litellm_settings:\n",
      " drop_params: true`\n",
      ". \n",
      " If you want to use these params dynamically send allowed_openai_params=['top_p'] in your request.\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:48.526\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mRetrying 6th time...\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:48.546\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrdagent.oai.backend.litellm\u001b[0m:\u001b[36m_create_chat_completion_inner_function\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96msystem\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96mYou are a senior machine learning researcher working on improving Kaggle competition pipelines. \n",
      "Your job is to propose and evaluate testable hypotheses that could help improve the current SOTA implementation.\n",
      "Be specific, concise, and use competition insight to ground your ideas.\u001b[0m\n",
      "\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96muser\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96m\n",
      "# Scenario Description\n",
      "The competition is a tabular regression problem predicting house prices. The metric is RMSE. \n",
      "The dataset includes structured features like location, size, number of rooms, and age.\n",
      "\n",
      "# Previous Experiments and Feedbacks\n",
      "1. Used LightGBM with basic features — RMSE: 0.145\n",
      "2. Added polynomial features — RMSE: 0.142\n",
      "3. Tried stacking with Ridge — RMSE: 0.141 but overfitted on fold 3\n",
      "\n",
      "# Current SOTA Implementation\n",
      "The current pipeline uses LightGBM and some feature engineering. It performs decently but still overfits slightly.\n",
      "\n",
      "# Identified Challenges\n",
      "1. Overfitting on some validation folds\n",
      "2. High cardinality in 'neighborhood' categorical variable\n",
      "3. Model performance saturates with simple stacking\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:49.571\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m518\u001b[0m - \u001b[33m\u001b[1mlitellm.UnsupportedParamsError: openai does not support parameters: ['top_p'], for model=o3. To drop these, set `litellm.drop_params=True` or for proxy:\n",
      "\n",
      "`litellm_settings:\n",
      " drop_params: true`\n",
      ". \n",
      " If you want to use these params dynamically send allowed_openai_params=['top_p'] in your request.\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:49.591\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mRetrying 7th time...\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:49.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrdagent.oai.backend.litellm\u001b[0m:\u001b[36m_create_chat_completion_inner_function\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96msystem\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96mYou are a senior machine learning researcher working on improving Kaggle competition pipelines. \n",
      "Your job is to propose and evaluate testable hypotheses that could help improve the current SOTA implementation.\n",
      "Be specific, concise, and use competition insight to ground your ideas.\u001b[0m\n",
      "\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96muser\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96m\n",
      "# Scenario Description\n",
      "The competition is a tabular regression problem predicting house prices. The metric is RMSE. \n",
      "The dataset includes structured features like location, size, number of rooms, and age.\n",
      "\n",
      "# Previous Experiments and Feedbacks\n",
      "1. Used LightGBM with basic features — RMSE: 0.145\n",
      "2. Added polynomial features — RMSE: 0.142\n",
      "3. Tried stacking with Ridge — RMSE: 0.141 but overfitted on fold 3\n",
      "\n",
      "# Current SOTA Implementation\n",
      "The current pipeline uses LightGBM and some feature engineering. It performs decently but still overfits slightly.\n",
      "\n",
      "# Identified Challenges\n",
      "1. Overfitting on some validation folds\n",
      "2. High cardinality in 'neighborhood' categorical variable\n",
      "3. Model performance saturates with simple stacking\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:50.636\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m518\u001b[0m - \u001b[33m\u001b[1mlitellm.UnsupportedParamsError: openai does not support parameters: ['top_p'], for model=o3. To drop these, set `litellm.drop_params=True` or for proxy:\n",
      "\n",
      "`litellm_settings:\n",
      " drop_params: true`\n",
      ". \n",
      " If you want to use these params dynamically send allowed_openai_params=['top_p'] in your request.\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:50.656\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mRetrying 8th time...\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:50.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrdagent.oai.backend.litellm\u001b[0m:\u001b[36m_create_chat_completion_inner_function\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96msystem\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96mYou are a senior machine learning researcher working on improving Kaggle competition pipelines. \n",
      "Your job is to propose and evaluate testable hypotheses that could help improve the current SOTA implementation.\n",
      "Be specific, concise, and use competition insight to ground your ideas.\u001b[0m\n",
      "\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96muser\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96m\n",
      "# Scenario Description\n",
      "The competition is a tabular regression problem predicting house prices. The metric is RMSE. \n",
      "The dataset includes structured features like location, size, number of rooms, and age.\n",
      "\n",
      "# Previous Experiments and Feedbacks\n",
      "1. Used LightGBM with basic features — RMSE: 0.145\n",
      "2. Added polynomial features — RMSE: 0.142\n",
      "3. Tried stacking with Ridge — RMSE: 0.141 but overfitted on fold 3\n",
      "\n",
      "# Current SOTA Implementation\n",
      "The current pipeline uses LightGBM and some feature engineering. It performs decently but still overfits slightly.\n",
      "\n",
      "# Identified Challenges\n",
      "1. Overfitting on some validation folds\n",
      "2. High cardinality in 'neighborhood' categorical variable\n",
      "3. Model performance saturates with simple stacking\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:51.702\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m518\u001b[0m - \u001b[33m\u001b[1mlitellm.UnsupportedParamsError: openai does not support parameters: ['top_p'], for model=o3. To drop these, set `litellm.drop_params=True` or for proxy:\n",
      "\n",
      "`litellm_settings:\n",
      " drop_params: true`\n",
      ". \n",
      " If you want to use these params dynamically send allowed_openai_params=['top_p'] in your request.\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:51.721\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mRetrying 9th time...\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:51.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrdagent.oai.backend.litellm\u001b[0m:\u001b[36m_create_chat_completion_inner_function\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1m\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96msystem\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96mYou are a senior machine learning researcher working on improving Kaggle competition pipelines. \n",
      "Your job is to propose and evaluate testable hypotheses that could help improve the current SOTA implementation.\n",
      "Be specific, concise, and use competition insight to ground your ideas.\u001b[0m\n",
      "\n",
      "\u001b[95m\u001b[1mRole:\u001b[0m\u001b[96muser\u001b[0m\n",
      "\u001b[95m\u001b[1mContent:\u001b[0m \u001b[96m\n",
      "# Scenario Description\n",
      "The competition is a tabular regression problem predicting house prices. The metric is RMSE. \n",
      "The dataset includes structured features like location, size, number of rooms, and age.\n",
      "\n",
      "# Previous Experiments and Feedbacks\n",
      "1. Used LightGBM with basic features — RMSE: 0.145\n",
      "2. Added polynomial features — RMSE: 0.142\n",
      "3. Tried stacking with Ridge — RMSE: 0.141 but overfitted on fold 3\n",
      "\n",
      "# Current SOTA Implementation\n",
      "The current pipeline uses LightGBM and some feature engineering. It performs decently but still overfits slightly.\n",
      "\n",
      "# Identified Challenges\n",
      "1. Overfitting on some validation folds\n",
      "2. High cardinality in 'neighborhood' categorical variable\n",
      "3. Model performance saturates with simple stacking\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:52.765\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m518\u001b[0m - \u001b[33m\u001b[1mlitellm.UnsupportedParamsError: openai does not support parameters: ['top_p'], for model=o3. To drop these, set `litellm.drop_params=True` or for proxy:\n",
      "\n",
      "`litellm_settings:\n",
      " drop_params: true`\n",
      ". \n",
      " If you want to use these params dynamically send allowed_openai_params=['top_p'] in your request.\u001b[0m\n",
      "\u001b[32m2025-07-16 09:13:52.784\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrdagent.oai.backend.base\u001b[0m:\u001b[36m_try_create_chat_completion_or_embedding\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mRetrying 10th time...\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to create chat completion after 10 retries.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 23\u001b[0m\n\u001b[1;32m      1\u001b[0m system_prompt1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are a senior machine learning researcher working on improving Kaggle competition pipelines. \u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mYour job is to propose and evaluate testable hypotheses that could help improve the current SOTA implementation.\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mBe specific, concise, and use competition insight to ground your ideas.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m user_prompt1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m# Scenario Description\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mThe competition is a tabular regression problem predicting house prices. The metric is RMSE. \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m3. Model performance saturates with simple stacking\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 23\u001b[0m response2 \u001b[38;5;241m=\u001b[39m \u001b[43mAPIBackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_messages_and_create_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_prompt1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_prompt1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson_object\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson_target_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/userdata/v-lijingyuan/RD-Agent-ensemble/RD-Agent/rdagent/oai/backend/base.py:408\u001b[0m, in \u001b[0;36mAPIBackend.build_messages_and_create_chat_completion\u001b[0;34m(self, user_prompt, system_prompt, former_messages, chat_cache_prefix, shrink_multiple_break, *args, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m     former_messages \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    401\u001b[0m messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_messages(\n\u001b[1;32m    402\u001b[0m     user_prompt,\n\u001b[1;32m    403\u001b[0m     system_prompt,\n\u001b[1;32m    404\u001b[0m     former_messages,\n\u001b[1;32m    405\u001b[0m     shrink_multiple_break\u001b[38;5;241m=\u001b[39mshrink_multiple_break,\n\u001b[1;32m    406\u001b[0m )\n\u001b[0;32m--> 408\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_create_chat_completion_or_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[misc]\u001b[39;49;00m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchat_cache_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_cache_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe response of _try_create_chat_completion_or_embedding should be a string.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/userdata/v-lijingyuan/RD-Agent-ensemble/RD-Agent/rdagent/oai/backend/base.py:521\u001b[0m, in \u001b[0;36mAPIBackend._try_create_chat_completion_or_embedding\u001b[0;34m(self, max_retry, chat_completion, embedding, *args, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mth time...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    520\u001b[0m error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to create chat completion after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retry\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m retries.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 521\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_message)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to create chat completion after 10 retries."
     ]
    }
   ],
   "source": [
    "\n",
    "system_prompt1 = \"\"\"You are a senior machine learning researcher working on improving Kaggle competition pipelines. \n",
    "Your job is to propose and evaluate testable hypotheses that could help improve the current SOTA implementation.\n",
    "Be specific, concise, and use competition insight to ground your ideas.\"\"\"\n",
    "user_prompt1 = \"\"\"\n",
    "# Scenario Description\n",
    "The competition is a tabular regression problem predicting house prices. The metric is RMSE. \n",
    "The dataset includes structured features like location, size, number of rooms, and age.\n",
    "\n",
    "# Previous Experiments and Feedbacks\n",
    "1. Used LightGBM with basic features — RMSE: 0.145\n",
    "2. Added polynomial features — RMSE: 0.142\n",
    "3. Tried stacking with Ridge — RMSE: 0.141 but overfitted on fold 3\n",
    "\n",
    "# Current SOTA Implementation\n",
    "The current pipeline uses LightGBM and some feature engineering. It performs decently but still overfits slightly.\n",
    "\n",
    "# Identified Challenges\n",
    "1. Overfitting on some validation folds\n",
    "2. High cardinality in 'neighborhood' categorical variable\n",
    "3. Model performance saturates with simple stacking\n",
    "\"\"\"\n",
    "    \"temperature\": 0.7,\n",
    "\n",
    "response2 = APIBackend().build_messages_and_create_chat_completion(\n",
    "    user_prompt=user_prompt1,\n",
    "    system_prompt=system_prompt1,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    json_target_type=None,\n",
    "    top_p= 0.9,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8464be93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-BtsgKT45BJVbuBtX5606mW0vXgSzl', created=1752657936, model='azure/o3-2025-04-16', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='length', index=0, message=Message(content='', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=100, prompt_tokens=25, total_tokens=125, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=100, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier=None)\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"请帮我写一句问候语。\"},\n",
    "]\n",
    "\n",
    "response = completion(\n",
    "    model=\"o3\",\n",
    "    messages=messages,\n",
    "    stream=False,\n",
    "    temperature=1,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "edd0b217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8293ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = HypothesisList(**json.loads(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "646a99e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HypothesisList(deduplicated_challenges=['Capture full essay context beyond 512 tokens', 'Mitigate severe high-score class imbalance', 'Inject argumentative & discourse quality features'], hypotheses=[HypothesisDetail(caption='Capture full essay context beyond 512 tokens', challenge='The current DeBERTa-v3-base model is limited to 512 tokens, forcing truncation that can drop important sections (introductions, conclusions) of the ~2 k-character essays and potentially harming QWK. We need a way to feed the full essay text or its salient parts into the ordinal head without exceeding memory limits.', hypothesis='Substitute the DeBERTa encoder with the Longformer-base model (4 k token window) and fine-tune it using the same CORAL ordinal head; keep window size at 2 k tokens so every essay fits, and preserve the engineered-feature fusion and threshold tuning pipeline unchanged.', metric_impact='Longformer can attend to complete essays, which should improve semantic coverage and is expected to raise validation QWK by 1–3 pp.', component=<HypothesisComponent.Model: 'Model'>, evaluation=HypothesisEvaluation(alignment=HypothesisEvaluationReasoningScore(reasoning='Directly replaces the 512-token-limited encoder with a long-context transformer, tackling the missing-context issue head-on.', score=9.0), impact=HypothesisEvaluationReasoningScore(reasoning='Full-context understanding often yields noticeable gains; a modest but meaningful QWK lift is plausible.', score=8.0), novelty=HypothesisEvaluationReasoningScore(reasoning='Long-context transformers have not been tried in prior traces, so this is a fresh architectural change.', score=7.0), feasibility=HypothesisEvaluationReasoningScore(reasoning='Longformer-base is heavier (approx. 148 M params) but still trainable on a modern GPU within the 5-hour limit; adapting the existing head is straightforward.', score=6.0), risk_reward_balance=HypothesisEvaluationReasoningScore(reasoning='Higher memory and slower training introduce risk, but expected gains justify a trial.', score=6.0))), HypothesisDetail(caption='Mitigate severe high-score class imbalance', challenge='Scores 5–6 constitute <7 % of the training set, so the model under-represents these rare yet leaderboard-critical classes, leading to poor recall and possible threshold mis-calibration.', hypothesis='Introduce class-balanced batch sampling that oversamples essays with scores ≥5 to achieve a per-batch uniform score distribution, and pair it with per-threshold focal loss where γ is increased to 4.0 for the top two CORAL thresholds to emphasise minority errors.', metric_impact='Better exposure and sharper loss on rare classes should raise recall for scores 5–6, potentially improving overall QWK by 0.5–1 pp.', component=<HypothesisComponent.Model: 'Model'>, evaluation=HypothesisEvaluation(alignment=HypothesisEvaluationReasoningScore(reasoning='Directly targets the imbalance by altering both sampling and loss weighting on the offending classes.', score=8.0), impact=HypothesisEvaluationReasoningScore(reasoning='Correcting misclassified top scores can materially lift QWK but effect size is moderate.', score=7.0), novelty=HypothesisEvaluationReasoningScore(reasoning='Current pipeline already uses effective-number weighting but not targeted oversampling or adaptive γ; hence partly new.', score=6.0), feasibility=HypothesisEvaluationReasoningScore(reasoning='Implementing a custom PyTorch sampler and adjusting γ are low-effort code changes.', score=8.0), risk_reward_balance=HypothesisEvaluationReasoningScore(reasoning='Minor compute overhead with little downside; good chance of incremental gain.', score=7.0))), HypothesisDetail(caption='Inject argumentative & discourse quality features', challenge='Current engineered features capture surface readability and simple counts but ignore argumentative structure (claims, evidence, counter-arguments) and discourse coherence—all factors graders consider.', hypothesis='Augment the engineered feature set by running a pretrained argumentative-component detector (e.g., spaCy discourse-parser model) to count claims, premises, rebuttals, and compute ratios (e.g., claim/premise density, average argument length); z-score these features and concatenate them with existing engineered vectors before training the same CORAL model.', metric_impact='Richer argumentative signals should help the head distinguish high-quality essays, potentially improving QWK by 0.3–0.8 pp.', component=<HypothesisComponent.FeatureEng: 'FeatureEng'>, evaluation=HypothesisEvaluation(alignment=HypothesisEvaluationReasoningScore(reasoning='Adds precisely the missing argumentative and discourse cues highlighted in the challenge.', score=8.0), impact=HypothesisEvaluationReasoningScore(reasoning='Hand-crafted discourse features historically yield small but consistent gains in essay scoring.', score=6.0), novelty=HypothesisEvaluationReasoningScore(reasoning='Argument-component counts have not been used yet, providing a new information source.', score=7.0), feasibility=HypothesisEvaluationReasoningScore(reasoning='spaCy’s en_core_web_trf with discourse parser is installable; feature extraction is offline and cacheable though slower (~1 h).', score=6.0), risk_reward_balance=HypothesisEvaluationReasoningScore(reasoning='Low risk aside from extra preprocessing time; gains may be modest but additive.', score=6.0)))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
