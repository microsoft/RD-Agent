{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from typing import List\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nimport logging\nimport os\nimport shutil\nimport json\nimport yaml\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\nfrom transformers import DataCollatorWithPadding\nfrom datasets import Dataset,load_dataset, load_from_disk\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import load_metric, disable_progress_bar\nfrom sklearn.metrics import mean_squared_error\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch import nn\nfrom torch.nn.parameter import Parameter\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom tqdm import tqdm\nimport re\n\nimport random\nimport nltk\nfrom text_unidecode import unidecode\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom collections import Counter\nimport spacy\nimport re\nimport lightgbm as lgb\nfrom pathlib import Path\nimport gc\nimport glob\nfrom typing import Tuple\nimport codecs\n\n# logging setting\n\nwarnings.simplefilter(\"ignore\")\nlogging.disable(logging.ERROR)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\ndisable_progress_bar()\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:12:45.030582Z","iopub.execute_input":"2023-09-23T06:12:45.03172Z","iopub.status.idle":"2023-09-23T06:12:54.923328Z","shell.execute_reply.started":"2023-09-23T06:12:45.031667Z","shell.execute_reply":"2023-09-23T06:12:54.922423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:12:54.925631Z","iopub.execute_input":"2023-09-23T06:12:54.925889Z","iopub.status.idle":"2023-09-23T06:12:54.935405Z","shell.execute_reply.started":"2023-09-23T06:12:54.925858Z","shell.execute_reply":"2023-09-23T06:12:54.934548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## c1","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\nfrom tqdm.auto import tqdm\nimport json, yaml\nimport random\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorWithPadding\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nfrom text_unidecode import unidecode\nimport codecs\nfrom sklearn.model_selection import StratifiedKFold\nfrom typing import Tuple\nimport os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:12:54.936819Z","iopub.execute_input":"2023-09-23T06:12:54.937498Z","iopub.status.idle":"2023-09-23T06:12:54.945664Z","shell.execute_reply.started":"2023-09-23T06:12:54.937465Z","shell.execute_reply":"2023-09-23T06:12:54.944704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn.parameter import Parameter\nimport numpy as np\n\n\ndef get_last_hidden_state(backbone_outputs):\n    last_hidden_state = backbone_outputs[0]\n    return last_hidden_state\n\n\ndef get_all_hidden_states(backbone_outputs):\n    all_hidden_states = torch.stack(backbone_outputs[1])\n    return all_hidden_states\n\n\ndef get_input_ids(inputs):\n    return inputs[\"input_ids\"]\n\n\ndef get_attention_mask(inputs):\n    return inputs[\"attention_mask\"]\n\n\nclass MeanPooling(nn.Module):\n    def __init__(self, backbone_config):\n        super(MeanPooling, self).__init__()\n        self.output_dim = backbone_config.hidden_size\n\n    def forward(self, inputs, backbone_outputs):\n        attention_mask = get_attention_mask(inputs)\n        last_hidden_state = get_last_hidden_state(backbone_outputs)\n\n        input_mask_expanded = (\n            attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        )\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\\\n\nclass GeMText(nn.Module):\n    def __init__(self, backbone_config, cfg):\n        super(GeMText, self).__init__()\n\n        self.dim = cfg[\"pooling\"][\"dim\"]\n        self.eps = cfg[\"pooling\"][\"eps\"]\n        self.feat_mult = 1\n\n        self.p = Parameter(torch.ones(1) * cfg[\"pooling\"][\"p\"])\n\n        self.output_dim = backbone_config.hidden_size\n\n    def forward(self, inputs, backbone_output):\n        attention_mask = get_attention_mask(inputs)\n        last_hidden_state = get_last_hidden_state(backbone_output)\n\n        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.shape)\n        x = (last_hidden_state.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n        ret = ret.pow(1 / self.p)\n        return ret\n\nclass CLSPooling(nn.Module):\n    def __init__(self, backbone_config):\n        super(CLSPooling, self).__init__()\n        self.output_dim = backbone_config.hidden_size\n\n    def forward(self, inputs, backbone_outputs):\n        last_hidden_state = get_last_hidden_state(backbone_outputs)\n        return last_hidden_state[:, 0]\n\n\ndef get_pooling_layer(cfg, backbone_config):\n    if cfg[\"pooling\"][\"type\"] == \"mean\":\n        return MeanPooling(backbone_config)\n    elif cfg[\"pooling\"][\"type\"] == \"gem\":\n        return GeMText(backbone_config, cfg)\n    else:\n        raise NotImplementedError","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:12:54.94858Z","iopub.execute_input":"2023-09-23T06:12:54.948843Z","iopub.status.idle":"2023-09-23T06:12:54.968511Z","shell.execute_reply.started":"2023-09-23T06:12:54.948812Z","shell.execute_reply":"2023-09-23T06:12:54.967587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self, cfg, backbone_cfg=None):\n        super().__init__()\n        self.cfg = cfg\n        self.backbone_cfg = backbone_cfg\n        if backbone_cfg is not None:\n            self.backbone = AutoModel.from_config(config=self.backbone_cfg)\n            \n        else:\n            self.backbone_cfg = AutoConfig.from_pretrained(cfg[\"model\"][\"backbone_path\"])\n            self.backbone = AutoModel.from_pretrained(cfg[\"model\"][\"backbone_path\"])\n\n        self.backbone.resize_token_embeddings(len(cfg[\"tokenizer\"]))\n        self.pool = get_pooling_layer(self.cfg, self.backbone_cfg)\n        self.fc = nn.Linear(self.pool.output_dim, len(cfg[\"general\"][\"target_columns\"]))\n\n    def forward(self, input):\n        outputs = self.backbone(**input)\n        feature = self.pool(input, outputs)\n        output = self.fc(feature)\n        return output\n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.backbone_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.backbone_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n\ndef freeze(module):\n    for param in module.parameters():\n        param.requires_grad = False\n\n\ndef get_model(cfg, check_point_path=None, backbone_cfg_path=None, train=True):\n    backbone_cfg = get_backbone_config(cfg) if backbone_cfg_path is None else torch.load(backbone_cfg_path)\n    model = CustomModel(cfg, backbone_cfg=backbone_cfg)\n\n    if check_point_path is not None:\n        state = torch.load(check_point_path, map_location=\"cpu\")\n        if 'model.embeddings.position_ids' in state['model'].keys():\n            state = update_old_state(state)  \n        model.load_state_dict(state[\"model\"], False)\n\n    if cfg[\"model\"][\"gradient_checkpointing\"]:\n        if model.backbone.supports_gradient_checkpointing:\n            model.backbone.gradient_checkpointing_enable()\n        else:\n            print(\"Gradient checkpointing is not supported by the model\")\n\n    if train:\n        if cfg[\"model\"][\"freeze_embeddings\"]:\n            freeze(model.backbone.embeddings)\n        if cfg[\"model\"][\"freeze_n_layers\"] > 0:\n            freeze(model.backbone.encoder.layer[: cfg[\"model\"][\"freeze_n_layers\"]])\n        if cfg['model']['reinitialize_n_layers'] > 0:\n            for module in model.backbone.encoder.layer[-cfg['model']['reinitialize_n_layers']:]:\n                model._init_weights(module)\n\n    return model\n\ndef get_backbone_config(config):\n    if config[\"model\"][\"backbone_config_path\"] == '':\n        backbone_config = AutoConfig.from_pretrained(config[\"model\"][\"backbone_path\"], output_hidden_states=True)\n\n    else:\n        backbone_config = torch.load(config[\"model\"][\"backbone_config_path\"])\n    return backbone_config\n\ndef update_old_state(state):\n    new_state = {}\n    for key, value in state['model'].items():\n        new_key = key\n        if key.startswith('model.'):\n            new_key = key.replace('model', 'backbone')\n        new_state[new_key] = value\n\n    updated_state = {'model': new_state, 'predictions': state['predictions']}\n    return updated_state","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:12:54.972279Z","iopub.execute_input":"2023-09-23T06:12:54.972625Z","iopub.status.idle":"2023-09-23T06:12:54.993256Z","shell.execute_reply.started":"2023-09-23T06:12:54.9726Z","shell.execute_reply":"2023-09-23T06:12:54.992311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:12:54.998146Z","iopub.execute_input":"2023-09-23T06:12:54.998438Z","iopub.status.idle":"2023-09-23T06:12:55.007018Z","shell.execute_reply.started":"2023-09-23T06:12:54.998381Z","shell.execute_reply":"2023-09-23T06:12:55.006047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_additional_special_tokens() -> dict:\n    special_tokens_replacement = {\n        \"\\n\": \"[BR]\",\n        \"\\r\\n\": \"[BR]\"\n    }\n    return special_tokens_replacement\n\n\ndef replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n\n\ndef replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n\n\ncodecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\ncodecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n\n\ndef resolve_encodings_and_normalize(text: str) -> str:\n    text = (\n        text.encode(\"raw_unicode_escape\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n    )\n    text = unidecode(text)\n    return text\n\n\ndef replace_special_tokens(text: str) -> str:\n    special_tokens_replacement = get_additional_special_tokens()\n    for k, v in special_tokens_replacement.items():\n        text = text.replace(k, v)\n    return text\n\n\ndef preprocess_text(text):\n    text = resolve_encodings_and_normalize(text)\n    text = replace_special_tokens(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:12:55.008325Z","iopub.execute_input":"2023-09-23T06:12:55.008693Z","iopub.status.idle":"2023-09-23T06:12:55.019405Z","shell.execute_reply.started":"2023-09-23T06:12:55.008659Z","shell.execute_reply":"2023-09-23T06:12:55.018432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommonLitDataset(Dataset):\n    def __init__(self, df, cfg):\n        self.df = df\n        self.cfg = cfg\n        self.text = df[\"full_text\"].values\n        self.labels = None\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        text = self.text[item]\n        input = self.cfg[\"tokenizer\"].encode_plus(\n            text,\n            return_tensors = None,\n            add_special_tokens=True,\n            max_length=1024,\n            padding=\"max_length\",\n            truncation=True,\n        )\n        for k, v in input.items():\n            input[k] = torch.tensor(v, dtype=torch.long)\n        if self.labels:\n            label = torch.tensor(self.labels[item], dtype=torch.float)\n            return input, label\n        return input\n\ndef get_test_dataloader(cfg, df):\n    dataset = CommonLitDataset(df, cfg)\n    dataloader = DataLoader(\n        dataset,\n        batch_size=4,\n        num_workers=1,\n        shuffle=False,\n        pin_memory=True,\n        drop_last=False,\n    )\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:12:55.020683Z","iopub.execute_input":"2023-09-23T06:12:55.021073Z","iopub.status.idle":"2023-09-23T06:12:55.033605Z","shell.execute_reply.started":"2023-09-23T06:12:55.021041Z","shell.execute_reply":"2023-09-23T06:12:55.032649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate(inputs):\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n    for k, _ in inputs.items():\n        inputs[k] = inputs[k][:, :mask_len]\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:12:55.034712Z","iopub.execute_input":"2023-09-23T06:12:55.035709Z","iopub.status.idle":"2023-09-23T06:12:55.046657Z","shell.execute_reply.started":"2023-09-23T06:12:55.035677Z","shell.execute_reply":"2023-09-23T06:12:55.045784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_config(path: str) -> dict:\n    \"\"\"\n    Load config from yaml file\n    \"\"\"\n    with open(path, \"r\") as f:\n        config: dict = yaml.safe_load(f)\n\n    return config","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:12:55.052317Z","iopub.execute_input":"2023-09-23T06:12:55.052556Z","iopub.status.idle":"2023-09-23T06:12:55.059397Z","shell.execute_reply.started":"2023-09-23T06:12:55.052523Z","shell.execute_reply":"2023-09-23T06:12:55.05836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        inputs = collate(inputs)\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:12:55.060818Z","iopub.execute_input":"2023-09-23T06:12:55.061188Z","iopub.status.idle":"2023-09-23T06:12:55.069282Z","shell.execute_reply.started":"2023-09-23T06:12:55.061156Z","shell.execute_reply":"2023-09-23T06:12:55.068264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts_test = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv')\nsummary_test = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv')\n\nsnorf_df = prompts_test.merge(summary_test, on=\"prompt_id\")\npq = snorf_df[\"prompt_question\"].values\ntext = snorf_df[\"text\"].values\nfull_text = pq + \"[SEP]\" + text\nsnorf_df[\"full_text\"] = full_text\nsnorf_df","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:12:55.070774Z","iopub.execute_input":"2023-09-23T06:12:55.071087Z","iopub.status.idle":"2023-09-23T06:12:55.117357Z","shell.execute_reply.started":"2023-09-23T06:12:55.071056Z","shell.execute_reply":"2023-09-23T06:12:55.116434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nconfig = get_config(\"/kaggle/input/exp1-content-model/config.yaml\")\nseed_everything()\n\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/exp1-content-model\")\nconfig[\"tokenizer\"] = tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:12:55.118785Z","iopub.execute_input":"2023-09-23T06:12:55.119008Z","iopub.status.idle":"2023-09-23T06:12:55.541718Z","shell.execute_reply.started":"2023-09-23T06:12:55.118979Z","shell.execute_reply":"2023-09-23T06:12:55.540704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"snorf_df[\"full_text\"] = snorf_df[\"full_text\"].apply(preprocess_text)\n\ntarget_columns = [\"content\"]\n\npredictions = []\n\ntest_dataloader = get_test_dataloader(config, snorf_df)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:12:55.543147Z","iopub.execute_input":"2023-09-23T06:12:55.543437Z","iopub.status.idle":"2023-09-23T06:12:55.552028Z","shell.execute_reply.started":"2023-09-23T06:12:55.543394Z","shell.execute_reply":"2023-09-23T06:12:55.550913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(4):\n    model = get_model(config,\n                      check_point_path=f\"/kaggle/input/exp1-content-model/fold{i}_best.pt\", \n                      backbone_cfg_path=\"/kaggle/input/exp1-content-model/config.pt\",\n                      train=False)\n    prediction = inference_fn(test_dataloader, model, device)\n    gc.collect()\n    torch.cuda.empty_cache()\n    predictions.append(prediction)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:12:55.553578Z","iopub.execute_input":"2023-09-23T06:12:55.553886Z","iopub.status.idle":"2023-09-23T06:15:13.736254Z","shell.execute_reply.started":"2023-09-23T06:12:55.553853Z","shell.execute_reply":"2023-09-23T06:15:13.735085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"content_c1 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:15:13.738867Z","iopub.execute_input":"2023-09-23T06:15:13.739553Z","iopub.status.idle":"2023-09-23T06:15:13.749462Z","shell.execute_reply.started":"2023-09-23T06:15:13.739515Z","shell.execute_reply":"2023-09-23T06:15:13.748503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"content_c1","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:15:13.753517Z","iopub.execute_input":"2023-09-23T06:15:13.75412Z","iopub.status.idle":"2023-09-23T06:15:13.767026Z","shell.execute_reply.started":"2023-09-23T06:15:13.75408Z","shell.execute_reply":"2023-09-23T06:15:13.766088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## w2","metadata":{}},{"cell_type":"code","source":"class CFG:\n    competition = \"CommonLit\"\n    num_workers = 2\n    tokenizer = \"/kaggle/input/microsoft-deberta-v3-large\"\n    model = \"microsoft/deberta-v3-large\"\n    ckpt_name = \"microsoft/deberta-v3-large\"\n    betas = (0.9, 0.999)\n    batch_size = 12\n    infer_batch_size = 8\n    max_len = 512\n    max_grad_norm = 1000\n    gradient_checkpointing = True\n    mlm_ratio = False\n    layer_reinitialize_n = 0\n    freeze_n_layers = 0\n    target_cols = [\n        \"wording\",\n    ]\n    seed = 42\n    n_fold = 4\n    trn_fold = [0, 1, 2, 3]\n    multi_sample_dropouts = None","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:15:13.768999Z","iopub.execute_input":"2023-09-23T06:15:13.770168Z","iopub.status.idle":"2023-09-23T06:15:13.785418Z","shell.execute_reply.started":"2023-09-23T06:15:13.770132Z","shell.execute_reply":"2023-09-23T06:15:13.784486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(text: str) -> dict[str, torch.Tensor]:\n    inputs = CFG.tokenizer.encode_plus(\n        text,\n        return_tensors=None,\n        add_special_tokens=True,\n        max_length=CFG.max_len,\n        pad_to_max_length=True,\n        truncation=True,\n    )\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, df: pd.DataFrame):\n        self.cfg = CFG\n        self.texts = df[\"full_text\"].values\n\n    def __len__(self) -> int:\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = self.texts[item]\n        inputs = prepare_input(text)\n        return inputs\n\n\ndef collate(inputs: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n    \"\"\"dynamic padding\"\"\"\n    if CFG.debug:\n        return inputs\n\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n    for k, v in inputs.items():\n        inputs[k] = v[:, :mask_len]\n    return inputs\n\n# ====================================================\n# Model\n# ====================================================\ndef freeze(module):\n    \"\"\"\n    Freezes module's parameters.\n    \"\"\"\n    for parameter in module.parameters():\n        parameter.requires_grad = False\n\n\ndef get_freezed_parameters(module):\n    \"\"\"\n    Returns names of freezed parameters of the given module.\n    \"\"\"\n    freezed_parameters = []\n    for name, parameter in module.named_parameters():\n        if not parameter.requires_grad:\n            freezed_parameters.append(name)\n    return freezed_parameters\n\n\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n\n    def forward(\n        self, last_hidden_state: torch.Tensor, attention_mask: torch.Tensor\n    ) -> torch.Tensor:\n        input_mask_expanded = (\n            attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        )\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\n\nclass CustomModel(nn.Module):\n    def __init__(\n        self, cfg: CFG, config_path: Path | None = None, pretrained: bool = False\n    ):\n        super().__init__()\n        self.cfg = cfg\n        self.n_target = len(cfg.target_cols)\n\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(\n                cfg.model, output_hidden_states=True\n            )\n            self.config.hidden_dropout = 0.0\n            self.config.hidden_dropout_prob = 0.0\n            self.config.attention_dropout = 0.0\n            self.config.attention_probs_dropout_prob = 0.0\n            LOGGER.info(self.config)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.model.resize_token_embeddings(len(cfg.tokenizer))\n        \n        if self.cfg.gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n\n        if cfg.multi_sample_dropouts is not None:\n            self.dropouts = nn.ModuleList(\n                [nn.Dropout(p) for p in cfg.multi_sample_dropouts]\n            )\n        else:\n            self.dropouts = None\n\n        self.fc = nn.Linear(self.config.hidden_size, self.n_target) \n\n\n        self._re_init_layers(self.cfg.layer_reinitialize_n)\n        if 1 <= self.cfg.freeze_n_layers:\n            freeze(self.model.embeddings)\n            freeze(\n                self.model.encoder.layer[: self.cfg.freeze_n_layers]\n            ) \n\n    def _re_init_layers(self, n_layers: int):\n        if n_layers >= 1:\n            for layer in self.model.encoder.layer[-n_layers:]:\n                # deverta-v3\n                if hasattr(layer, \"modules\"):\n                    for module in layer.modules():\n                        for name, child in module.named_children():\n                            init_type_name = self._init_weights(child)\n                            if init_type_name is not None:\n                                print(\n                                    f\"{name} is re-initialized, type: {init_type_name}, {module.__class__}\"\n                                )\n\n    def _init_weights(self, module: nn.Module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n            return \"nn.Linear\"\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n            return \"nn.Embedding\"\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n            return \"nn.LayerNorm\"\n        return None\n\n\n    def forward(self, inputs: dict[str, torch.Tensor]) -> torch.Tensor:\n        outputs = self.model(**inputs)\n        cls_hidden_state = outputs.last_hidden_state[:, 0, :] \n        output = self.fc(cls_hidden_state)\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:15:13.787189Z","iopub.execute_input":"2023-09-23T06:15:13.789922Z","iopub.status.idle":"2023-09-23T06:15:13.840638Z","shell.execute_reply.started":"2023-09-23T06:15:13.789883Z","shell.execute_reply":"2023-09-23T06:15:13.839661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.inference_mode()\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    for inputs in tqdm(test_loader, total=len(test_loader)):\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        y_preds = model(inputs)\n        preds.append(y_preds.to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:15:13.845774Z","iopub.execute_input":"2023-09-23T06:15:13.848249Z","iopub.status.idle":"2023-09-23T06:15:13.857911Z","shell.execute_reply.started":"2023-09-23T06:15:13.848213Z","shell.execute_reply":"2023-09-23T06:15:13.857027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_short_quotes(text):\n    \"\"\"Removes quotes shorter than 30 chars\"\"\"\n    pattern = r'\\[QUOTE\\](.*?)\\[ENDQUOTE\\]'\n\n    def replace_quotes(match):\n        content = match.group(1)\n        if len(content) <= 30:\n            return content\n        else:\n            return match.group()\n\n    processed_text = re.sub(pattern, replace_quotes, text)\n    return processed_text\n\n\ndef replace_quotes_with_passage(text):\n    \"\"\"Replaces content inside [QUOTE]...[ENDQUOTE] with either [PASSAGE] or [PLAGIARISM] depending on proximity of quotations\"\"\"\n\n    pattern = r'\\[QUOTE\\](.*?)\\[ENDQUOTE\\]'\n\n    def determine_and_count_tokens(match):\n        content = match.group(1)  # Get content inside [QUOTE]...[ENDQUOTE]\n        quote_index = match.start(0)  # Get start index of [QUOTE]\n        endquote_index = match.end(0)  # Get end index of [ENDQUOTE]\n        content_length = len(content)\n        passage_count = content_length // 50\n\n        if '\"' in content or \\\n            '\"' in text[max(0, quote_index - 10):min(len(text), endquote_index + 10)]:\n            return ' [PASSAGE] ' * passage_count\n        else:\n            return ' [PASSAGE] ' * passage_count\n\n    # Replace matches based on context and content length\n    cleaned_text = re.sub(pattern, determine_and_count_tokens, text)\n    return cleaned_text\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\ndef plagiarism_checker(input_text, source_text, n=3, similarity_threshold=0.85):\n    try:\n        # Generate n-grams\n        def generate_ngrams(text, n):\n            words = text.split()\n            ngrams = []\n            for i in range(len(words) - n + 1):\n                ngram = ' '.join(words[i:i + n])\n                ngrams.append(ngram)\n            return ngrams\n\n        source_ngrams = generate_ngrams(source_text, n)\n        input_ngrams = generate_ngrams(input_text, n)\n\n        source_ngram_strings = [' '.join(ngram.split()) for ngram in source_ngrams]\n        input_ngram_strings = [' '.join(ngram.split()) for ngram in input_ngrams]\n\n        # Calculate TF-IDF vectors\n        vectorizer = TfidfVectorizer()\n        tfidf_matrix = vectorizer.fit_transform(source_ngram_strings + input_ngram_strings)\n        source_tfidf = tfidf_matrix[:len(source_ngram_strings)]\n        input_tfidf = tfidf_matrix[len(source_ngram_strings):]\n        # Calculate similarity\n        similarity_matrix = cosine_similarity(input_tfidf, source_tfidf)\n \n        # Find segments in the input text\n        plagiarized_indices = []\n        for i, input_ngram in enumerate(input_ngrams):\n            most_similar_index = np.argmax(similarity_matrix[i])\n            cosine_sim = similarity_matrix[i, most_similar_index]\n\n            if cosine_sim >= similarity_threshold:\n                plagiarized_indices.append(i)\n\n        # Mark plagiarized segments with [QUOTE] and [ENDQUOTE]\n        marked_input_text = input_text\n        for index in plagiarized_indices:\n            input_ngram = input_ngrams[index]\n            source_ngram = source_ngrams[np.argmax(similarity_matrix[index])]\n            # Replace only whole words\n            marked_input_text = marked_input_text.replace(f\" {input_ngram} \", f\" [QUOTE]{input_ngram}[ENDQUOTE] \")\n        marked_input_text = marked_input_text.replace(\"[ENDQUOTE] [QUOTE]\", \" \")\n\n        return  remove_short_quotes(marked_input_text)\n    except Exception:\n        return input_text","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:15:13.861468Z","iopub.execute_input":"2023-09-23T06:15:13.861703Z","iopub.status.idle":"2023-09-23T06:15:13.879447Z","shell.execute_reply.started":"2023-09-23T06:15:13.861678Z","shell.execute_reply":"2023-09-23T06:15:13.877621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts_test = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv')\nsummary_test = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv')\n\ntest = prompts_test.merge(summary_test, on=\"prompt_id\")\ntest[\"full_text\"] = [replace_quotes_with_passage(plagiarism_checker(text, prompt_text, n =3)) for text, prompt_text in zip(test[\"text\"].values,test[\"prompt_text\"].values)]\ntest","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:15:13.88136Z","iopub.execute_input":"2023-09-23T06:15:13.882072Z","iopub.status.idle":"2023-09-23T06:15:13.937136Z","shell.execute_reply.started":"2023-09-23T06:15:13.882039Z","shell.execute_reply":"2023-09-23T06:15:13.935362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer)\ntokenizer.add_tokens(['[PASSAGE]','[PLAGIARISM]'])\nCFG.tokenizer = tokenizer\nCFG.tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:15:13.938494Z","iopub.execute_input":"2023-09-23T06:15:13.938831Z","iopub.status.idle":"2023-09-23T06:15:15.169934Z","shell.execute_reply.started":"2023-09-23T06:15:13.938787Z","shell.execute_reply":"2023-09-23T06:15:15.168816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(test)\ntest_loader = DataLoader(test_dataset,\n  batch_size=CFG.batch_size,\n  shuffle=False,\n  collate_fn=DataCollatorWithPadding(tokenizer=CFG.tokenizer, padding='longest'),\n  num_workers=CFG.num_workers, pin_memory=True, drop_last=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:15:15.17146Z","iopub.execute_input":"2023-09-23T06:15:15.171912Z","iopub.status.idle":"2023-09-23T06:15:15.17842Z","shell.execute_reply.started":"2023-09-23T06:15:15.171876Z","shell.execute_reply":"2023-09-23T06:15:15.177492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:15:15.180063Z","iopub.execute_input":"2023-09-23T06:15:15.18063Z","iopub.status.idle":"2023-09-23T06:15:15.192035Z","shell.execute_reply.started":"2023-09-23T06:15:15.180594Z","shell.execute_reply":"2023-09-23T06:15:15.191035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wording_w2 = []\nfor fold in CFG.trn_fold:\n    model = CustomModel(CFG, config_path='/kaggle/input/exp3-wording-model/config.pth', pretrained=False)\n    state = torch.load(f'/kaggle/input/exp3-wording-model/microsoft_deberta-v3-large_fold{fold}_best.pth',\n                      map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    wording_w2.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\nwording_w2 = np.mean(wording_w2, axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:15:15.193882Z","iopub.execute_input":"2023-09-23T06:15:15.194122Z","iopub.status.idle":"2023-09-23T06:17:29.380187Z","shell.execute_reply.started":"2023-09-23T06:15:15.194092Z","shell.execute_reply":"2023-09-23T06:17:29.379109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## w4","metadata":{}},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\nprompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\nsummaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\ntest = prompts_test.merge(summaries_test, on=\"prompt_id\")\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:17:29.386294Z","iopub.execute_input":"2023-09-23T06:17:29.388798Z","iopub.status.idle":"2023-09-23T06:17:29.440296Z","shell.execute_reply.started":"2023-09-23T06:17:29.388755Z","shell.execute_reply":"2023-09-23T06:17:29.439437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_short_quotes(text):\n    \"\"\"Removes quotes shorter than 30 chars\"\"\"\n    pattern = r'\\[QUOTE\\](.*?)\\[ENDQUOTE\\]'\n\n    def replace_quotes(match):\n        content = match.group(1)\n        if len(content) <= 30:\n            return content\n        else:\n            return match.group()\n\n    processed_text = re.sub(pattern, replace_quotes, text)\n    return processed_text\n\n\ndef replace_quotes_with_passage(text):\n    \"\"\"Replaces content inside [QUOTE]...[ENDQUOTE] with either [PASSAGE] or [PLAGIARISM] depending on proximity of quotations\"\"\"\n\n    pattern = r'\\[QUOTE\\](.*?)\\[ENDQUOTE\\]'\n\n    def determine_and_count_tokens(match):\n        content = match.group(1)  # Get content inside [QUOTE]...[ENDQUOTE]\n        quote_index = match.start(0)  # Get start index of [QUOTE]\n        endquote_index = match.end(0)  # Get end index of [ENDQUOTE]\n        content_length = len(content)\n        passage_count = content_length // 50\n\n        if '\"' in content or \\\n            '\"' in text[max(0, quote_index - 10):min(len(text), endquote_index + 10)]:\n            return ' [PASSAGE] '*len(match.group(1).split())\n        else:\n            return ' [PASSAGE] '*len(match.group(1).split())\n\n    # Replace matches based on context and content length\n    cleaned_text = re.sub(pattern, determine_and_count_tokens, text)\n    return cleaned_text\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\ndef plagiarism_checker(input_text, source_text, n=3, similarity_threshold=0.9):\n    try:\n        # Generate n-grams\n        def generate_ngrams(text, n):\n            words = text.split()\n            ngrams = []\n            for i in range(len(words) - n + 1):\n                ngram = ' '.join(words[i:i + n])\n                ngrams.append(ngram)\n            return ngrams\n\n        source_ngrams = generate_ngrams(source_text, n)\n        input_ngrams = generate_ngrams(input_text, n)\n\n        source_ngram_strings = [' '.join(ngram.split()) for ngram in source_ngrams]\n        input_ngram_strings = [' '.join(ngram.split()) for ngram in input_ngrams]\n\n        # Calculate TF-IDF vectors\n        vectorizer = TfidfVectorizer()\n        tfidf_matrix = vectorizer.fit_transform(source_ngram_strings + input_ngram_strings)\n        source_tfidf = tfidf_matrix[:len(source_ngram_strings)]\n        input_tfidf = tfidf_matrix[len(source_ngram_strings):]\n        # Calculate similarity\n        similarity_matrix = cosine_similarity(input_tfidf, source_tfidf)\n \n        # Find segments in the input text\n        plagiarized_indices = []\n        for i, input_ngram in enumerate(input_ngrams):\n            most_similar_index = np.argmax(similarity_matrix[i])\n            cosine_sim = similarity_matrix[i, most_similar_index]\n\n            if cosine_sim >= similarity_threshold:\n                plagiarized_indices.append(i)\n\n        # Mark plagiarized segments with [QUOTE] and [ENDQUOTE]\n        marked_input_text = input_text\n        for index in plagiarized_indices:\n            input_ngram = input_ngrams[index]\n            source_ngram = source_ngrams[np.argmax(similarity_matrix[index])]\n            # Replace only whole words\n            marked_input_text = marked_input_text.replace(f\" {input_ngram} \", f\" [QUOTE]{input_ngram}[ENDQUOTE] \")\n        marked_input_text = marked_input_text.replace(\"[ENDQUOTE] [QUOTE]\", \" \")\n\n        return  remove_short_quotes(marked_input_text)\n    except Exception:\n        return input_text","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:17:29.457019Z","iopub.execute_input":"2023-09-23T06:17:29.457292Z","iopub.status.idle":"2023-09-23T06:17:29.487276Z","shell.execute_reply.started":"2023-09-23T06:17:29.457257Z","shell.execute_reply":"2023-09-23T06:17:29.486326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"fixed_text\"] = [replace_quotes_with_passage(plagiarism_checker(text, prompt_text, n=3)) for text, prompt_text in zip(test[\"text\"].values,test[\"prompt_text\"].values)]","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:17:29.49094Z","iopub.execute_input":"2023-09-23T06:17:29.491231Z","iopub.status.idle":"2023-09-23T06:17:29.513875Z","shell.execute_reply.started":"2023-09-23T06:17:29.491199Z","shell.execute_reply":"2023-09-23T06:17:29.512826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"full_text\"] = test[\"fixed_text\"]\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:17:29.51545Z","iopub.execute_input":"2023-09-23T06:17:29.515895Z","iopub.status.idle":"2023-09-23T06:17:29.529216Z","shell.execute_reply.started":"2023-09-23T06:17:29.515861Z","shell.execute_reply":"2023-09-23T06:17:29.528129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:17:29.53119Z","iopub.execute_input":"2023-09-23T06:17:29.531886Z","iopub.status.idle":"2023-09-23T06:17:29.538324Z","shell.execute_reply.started":"2023-09-23T06:17:29.531849Z","shell.execute_reply":"2023-09-23T06:17:29.537321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_additional_special_tokens() -> dict:\n    special_tokens_replacement = {\n        \"\\n\": \"[BR]\",\n        \"\\r\\n\": \"[BR]\",\n        \"[PASSAGE]\": \"[PASSAGE]\"\n    }\n    return special_tokens_replacement\n\n\ndef replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n\n\ndef replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n\n\ncodecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\ncodecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n\n\ndef resolve_encodings_and_normalize(text: str) -> str:\n    text = (\n        text.encode(\"raw_unicode_escape\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n    )\n    text = unidecode(text)\n    return text\n\n\ndef replace_special_tokens(text: str) -> str:\n    special_tokens_replacement = get_additional_special_tokens()\n    for k, v in special_tokens_replacement.items():\n        text = text.replace(k, v)\n    return text\n\n\ndef preprocess_text(text):\n    text = resolve_encodings_and_normalize(text)\n    text = replace_special_tokens(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:17:29.540129Z","iopub.execute_input":"2023-09-23T06:17:29.540771Z","iopub.status.idle":"2023-09-23T06:17:29.551775Z","shell.execute_reply.started":"2023-09-23T06:17:29.540737Z","shell.execute_reply":"2023-09-23T06:17:29.550896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_last_hidden_state(backbone_outputs):\n    last_hidden_state = backbone_outputs[0]\n    return last_hidden_state\n\n\ndef get_all_hidden_states(backbone_outputs):\n    all_hidden_states = torch.stack(backbone_outputs[1])\n    return all_hidden_states\n\n\ndef get_input_ids(inputs):\n    return inputs[\"input_ids\"]\n\n\ndef get_attention_mask(inputs):\n    return inputs[\"attention_mask\"]\n\n\nclass MeanPooling(nn.Module):\n    def __init__(self, backbone_config):\n        super(MeanPooling, self).__init__()\n        self.output_dim = backbone_config.hidden_size\n\n    def forward(self, inputs, backbone_outputs):\n        attention_mask = get_attention_mask(inputs)\n        last_hidden_state = get_last_hidden_state(backbone_outputs)\n\n        input_mask_expanded = (\n            attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        )\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\\\n\nclass GeMText(nn.Module):\n    def __init__(self, backbone_config, cfg):\n        super(GeMText, self).__init__()\n\n        self.dim = cfg[\"pooling\"][\"dim\"]\n        self.eps = cfg[\"pooling\"][\"eps\"]\n        self.feat_mult = 1\n\n        self.p = Parameter(torch.ones(1) * cfg[\"pooling\"][\"p\"])\n\n        self.output_dim = backbone_config.hidden_size\n\n    def forward(self, inputs, backbone_output):\n        attention_mask = get_attention_mask(inputs)\n        last_hidden_state = get_last_hidden_state(backbone_output)\n\n        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.shape)\n        x = (last_hidden_state.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n        ret = ret.pow(1 / self.p)\n        return ret\n\ndef get_pooling_layer(cfg, backbone_config):\n    if cfg[\"pooling\"][\"type\"] == \"mean\":\n        return MeanPooling(backbone_config)\n    elif cfg[\"pooling\"][\"type\"] == \"gem\":\n        return GeMText(backbone_config, cfg)\n    else:\n        raise NotImplementedError","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:17:29.553551Z","iopub.execute_input":"2023-09-23T06:17:29.553811Z","iopub.status.idle":"2023-09-23T06:17:29.569526Z","shell.execute_reply.started":"2023-09-23T06:17:29.55378Z","shell.execute_reply":"2023-09-23T06:17:29.568631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self, cfg, backbone_cfg=None):\n        super().__init__()\n        self.cfg = cfg\n        self.backbone_cfg = backbone_cfg\n        if backbone_cfg is not None:\n            self.backbone = AutoModel.from_config(config=self.backbone_cfg)\n            \n        else:\n            self.backbone_cfg = AutoConfig.from_pretrained(cfg[\"model\"][\"backbone_path\"])\n            self.backbone = AutoModel.from_pretrained(cfg[\"model\"][\"backbone_path\"])\n\n        self.backbone.resize_token_embeddings(len(cfg[\"tokenizer\"]))\n        self.pool = get_pooling_layer(self.cfg, self.backbone_cfg)\n        self.fc = nn.Linear(self.pool.output_dim, len(cfg[\"general\"][\"target_columns\"]))\n\n    def forward(self, input):\n        outputs = self.backbone(**input)\n        feature = self.pool(input, outputs)\n        output = self.fc(feature)\n        return output\n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.backbone_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.backbone_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n\ndef freeze(module):\n    for param in module.parameters():\n        param.requires_grad = False\n\n\ndef get_model(cfg, check_point_path=None, backbone_cfg_path=None, train=True):\n    backbone_cfg = get_backbone_config(cfg) if backbone_cfg_path is None else torch.load(backbone_cfg_path)\n    model = CustomModel(cfg, backbone_cfg=backbone_cfg)\n\n    if check_point_path is not None:\n        state = torch.load(check_point_path, map_location=\"cpu\")\n        if 'model.embeddings.position_ids' in state['model'].keys():\n            state = update_old_state(state)  \n        model.load_state_dict(state[\"model\"])\n\n    if cfg[\"model\"][\"gradient_checkpointing\"]:\n        if model.backbone.supports_gradient_checkpointing:\n            model.backbone.gradient_checkpointing_enable()\n        else:\n            print(\"Gradient checkpointing is not supported by the model\")\n\n    if train:\n        if cfg[\"model\"][\"freeze_embeddings\"]:\n            freeze(model.backbone.embeddings)\n        if cfg[\"model\"][\"freeze_n_layers\"] > 0:\n            freeze(model.backbone.encoder.layer[: cfg[\"model\"][\"freeze_n_layers\"]])\n        if cfg['model']['reinitialize_n_layers'] > 0:\n            for module in model.backbone.encoder.layer[-cfg['model']['reinitialize_n_layers']:]:\n                model._init_weights(module)\n\n    return model\n\ndef get_backbone_config(config):\n    if config[\"model\"][\"backbone_config_path\"] == '':\n        backbone_config = AutoConfig.from_pretrained(config[\"model\"][\"backbone_path\"], output_hidden_states=True)\n\n    else:\n        backbone_config = torch.load(config[\"model\"][\"backbone_config_path\"])\n    return backbone_config\n\ndef update_old_state(state):\n    new_state = {}\n    for key, value in state['model'].items():\n        new_key = key\n        if key.startswith('model.'):\n            new_key = key.replace('model', 'backbone')\n        new_state[new_key] = value\n\n    updated_state = {'model': new_state, 'predictions': state['predictions']}\n    return updated_state","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:17:29.571494Z","iopub.execute_input":"2023-09-23T06:17:29.571829Z","iopub.status.idle":"2023-09-23T06:17:29.59356Z","shell.execute_reply.started":"2023-09-23T06:17:29.571792Z","shell.execute_reply":"2023-09-23T06:17:29.592917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommonLitDataset(Dataset):\n    def __init__(self, df, cfg):\n        self.df = df\n        self.cfg = cfg\n        self.text = df[\"full_text\"].values\n        self.labels = None\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        text = self.text[item]\n        input = self.cfg[\"tokenizer\"].encode_plus(\n            text,\n            return_tensors = None,\n            add_special_tokens=True,\n            max_length=768,\n            padding=\"max_length\",\n            truncation=True,\n        )\n        for k, v in input.items():\n            input[k] = torch.tensor(v, dtype=torch.long)\n        if self.labels:\n            label = torch.tensor(self.labels[item], dtype=torch.float)\n            return input, label\n        return input\n\ndef get_test_dataloader(cfg, df):\n    dataset = CommonLitDataset(df, cfg)\n    dataloader = DataLoader(\n        dataset,\n        batch_size=4,\n        num_workers=2,\n        shuffle=False,\n        pin_memory=True,\n        drop_last=False,\n    )\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:17:29.595017Z","iopub.execute_input":"2023-09-23T06:17:29.595551Z","iopub.status.idle":"2023-09-23T06:17:29.607315Z","shell.execute_reply.started":"2023-09-23T06:17:29.595517Z","shell.execute_reply":"2023-09-23T06:17:29.606684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate(inputs):\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n    for k, _ in inputs.items():\n        inputs[k] = inputs[k][:, :mask_len]\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:17:29.609897Z","iopub.execute_input":"2023-09-23T06:17:29.610111Z","iopub.status.idle":"2023-09-23T06:17:29.620295Z","shell.execute_reply.started":"2023-09-23T06:17:29.610088Z","shell.execute_reply":"2023-09-23T06:17:29.619462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_config(path: str) -> dict:\n    \"\"\"\n    Load config from yaml file\n    \"\"\"\n    with open(path, \"r\") as f:\n        config: dict = yaml.safe_load(f)\n\n    return config","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:17:29.623545Z","iopub.execute_input":"2023-09-23T06:17:29.623857Z","iopub.status.idle":"2023-09-23T06:17:29.630691Z","shell.execute_reply.started":"2023-09-23T06:17:29.623826Z","shell.execute_reply":"2023-09-23T06:17:29.629695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        inputs = collate(inputs)\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:17:29.634161Z","iopub.execute_input":"2023-09-23T06:17:29.634458Z","iopub.status.idle":"2023-09-23T06:17:29.641762Z","shell.execute_reply.started":"2023-09-23T06:17:29.634433Z","shell.execute_reply":"2023-09-23T06:17:29.640802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nconfig = get_config(\"/kaggle/input/exp4-wording-model/config.yaml\")\nseed_everything()\n\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/exp4-wording-model\")\nconfig[\"tokenizer\"] = tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:17:29.643265Z","iopub.execute_input":"2023-09-23T06:17:29.643956Z","iopub.status.idle":"2023-09-23T06:17:30.110251Z","shell.execute_reply.started":"2023-09-23T06:17:29.643885Z","shell.execute_reply":"2023-09-23T06:17:30.10936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"full_text\"] = test[\"full_text\"].apply(preprocess_text)\n\ntarget_columns = [\"wording\"]\n\npredictions1 = []\n\ntest_dataloader = get_test_dataloader(config, test)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:17:30.111561Z","iopub.execute_input":"2023-09-23T06:17:30.111831Z","iopub.status.idle":"2023-09-23T06:17:30.219948Z","shell.execute_reply.started":"2023-09-23T06:17:30.111798Z","shell.execute_reply":"2023-09-23T06:17:30.21891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(4):\n    model = get_model(config,\n                      check_point_path=f\"/kaggle/input/exp4-wording-model/fold{i}_best.pt\", \n                      backbone_cfg_path=\"/kaggle/input/exp4-wording-model/config.pt\",\n                      train=False)\n    prediction = inference_fn(test_dataloader, model, device)\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    predictions1.append(prediction)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:17:30.221939Z","iopub.execute_input":"2023-09-23T06:17:30.222519Z","iopub.status.idle":"2023-09-23T06:19:42.07587Z","shell.execute_reply.started":"2023-09-23T06:17:30.222481Z","shell.execute_reply":"2023-09-23T06:19:42.074839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = np.mean(predictions1, axis=0)\npreds","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:19:42.078584Z","iopub.execute_input":"2023-09-23T06:19:42.079108Z","iopub.status.idle":"2023-09-23T06:19:42.09543Z","shell.execute_reply.started":"2023-09-23T06:19:42.079069Z","shell.execute_reply":"2023-09-23T06:19:42.094422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## w6","metadata":{}},{"cell_type":"code","source":"class CFGw6:\n    competition = \"CommonLit\"\n    num_workers = 2\n    tokenizer = \"/kaggle/input/debertav3base\"\n    model = \"microsoft/deberta-v3-base\"\n    ckpt_name = \"microsoft/deberta-v3-base\"\n    betas = (0.9, 0.999)\n    batch_size = 12\n    infer_batch_size = 8\n    max_len = 640\n    max_grad_norm = 1000\n    gradient_checkpointing = True\n    mlm_ratio = False\n    layer_reinitialize_n = 0\n    freeze_n_layers = 0\n    target_cols = [\n        \"wording\",\n    ]\n    seed = 42\n    n_fold = 4\n    trn_fold = [0, 1, 2, 3]\n    multi_sample_dropouts = None","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:19:42.097439Z","iopub.execute_input":"2023-09-23T06:19:42.098029Z","iopub.status.idle":"2023-09-23T06:19:42.114949Z","shell.execute_reply.started":"2023-09-23T06:19:42.097988Z","shell.execute_reply":"2023-09-23T06:19:42.113918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts_test = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv')\nsummary_test = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv')\n\ntest = prompts_test.merge(summary_test, on=\"prompt_id\")\ntest[\"full_text\"] = [replace_quotes_with_passage(plagiarism_checker(text, prompt_text, n =3)) for text, prompt_text in zip(test[\"text\"].values,test[\"prompt_text\"].values)]\ntest","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:19:42.116657Z","iopub.execute_input":"2023-09-23T06:19:42.117209Z","iopub.status.idle":"2023-09-23T06:19:42.177116Z","shell.execute_reply.started":"2023-09-23T06:19:42.117168Z","shell.execute_reply":"2023-09-23T06:19:42.176187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport numpy as np\n\ndef remove_short_quotes(text):\n    \"\"\"Removes quotes shorter than 30 chars\"\"\"\n    pattern = r'\\[QUOTE\\](.*?)\\[ENDQUOTE\\]'\n    \n    def replace_quotes(match):\n        content = match.group(1)\n        \n        if len(content) <= 20:  \n            return content\n        else:\n            return match.group()\n        \n    processed_text = re.sub(pattern, replace_quotes, text)\n    return processed_text\n\ndef remove_chars_by_indices(input_string, indices_to_remove):\n    new_string = \"\"\n    for i in range(len(input_string)):\n        if i not in indices_to_remove:\n            new_string += input_string[i]\n    return new_string\n\ndef combine_nearby_quotes(text):\n    result = []\n    quote_state = False\n    indices_to_delete = []\n\n    for i in range(len(text)):\n        if text[i:i+7] == \"[QUOTE]\":\n            if quote_state:\n                indices_to_delete.extend([j for j in range(i,i+7)])\n            quote_state = True\n        elif text[i:i+10] == \"[ENDQUOTE]\":\n            if quote_state:\n                quote_state = False\n                for z in range(20):\n                    if text[i+z:i+z+7] == \"[QUOTE]\":\n                        quote_state = True\n                        indices_to_delete.extend([j for j in range(i,i+10)])\n                        break\n\n\n    return remove_chars_by_indices(text,indices_to_delete)\n\n\n\ndef replace_quotes_with_passage(text):\n    \"\"\"Replaces content inside [QUOTE]...[ENDQUOTE] with either [PASSAGE] or [PLAGIARISM] depending on proximity of quotations\"\"\"\n    \n    pattern = r'(?s)\\[QUOTE\\](.*?)\\[ENDQUOTE\\]'\n    def determine_and_count_tokens(match):\n        content = match.group(1)  # Get content inside [QUOTE]...[ENDQUOTE]\n        quote_index = match.start(0)  # Get start index of [QUOTE]\n        endquote_index = match.end(0)  # Get end index of [ENDQUOTE]\n        content_length = len(content)\n        passage_count = content_length // 20 \n        quotation_symbols = ['\"', '', '', \"'\", '', '', '', '']\n        if content_length < 40:\n            return f' [REFERENCE] {content} [ENDREFERENCE]'\n        elif any(symbol in content for symbol in quotation_symbols) or \\\n            '\"' in text[max(0, quote_index - 20):min(len(text), endquote_index + 20)]:\n            return ' [PASSAGE] ' * passage_count\n        else:\n            return ' [PLAGIARISM] ' * passage_count\n        \n    # Replace matches based on context and content length\n    cleaned_text = re.sub(pattern, determine_and_count_tokens, text)\n    #cleaned_text= cleaned_text.replace(\"[ENDQUOTE]\", \"\")\n    #cleaned_text= cleaned_text.replace(\"[QUOTE]\", \"\")\n    return cleaned_text\n\ndef remove_substrings(strings_list):\n    return [s for i, s in enumerate(strings_list) if not any(s in t for j, t in enumerate(strings_list) if i != j)]\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\ndef plagiarism_checker(input_text, source_text, n=3, similarity_threshold=0.85):\n    try:\n        # Generate n-grams \n        def generate_ngrams(text, n):\n            words = text.split()\n            ngrams = []\n            for i in range(len(words) - n + 1):\n                ngram = ' '.join(words[i:i + n])\n                ngrams.append(ngram)\n            return ngrams\n\n        source_ngrams = generate_ngrams(source_text, n)\n        input_ngrams = generate_ngrams(input_text, n)\n\n        source_ngram_strings = [' '.join(ngram.split()) for ngram in source_ngrams]\n        input_ngram_strings = [' '.join(ngram.split()) for ngram in input_ngrams]\n\n        # Calculate TF-IDF vectors\n        vectorizer = TfidfVectorizer()\n        tfidf_matrix = vectorizer.fit_transform(source_ngram_strings + input_ngram_strings)\n        source_tfidf = tfidf_matrix[:len(source_ngram_strings)]\n        input_tfidf = tfidf_matrix[len(source_ngram_strings):]\n\n        # Calculate similarity\n        similarity_matrix = cosine_similarity(input_tfidf, source_tfidf)\n\n        # Find segments in the input text\n        plagiarized_indices = []\n        for i, input_ngram in enumerate(input_ngrams):\n            most_similar_index = np.argmax(similarity_matrix[i])\n            cosine_sim = similarity_matrix[i, most_similar_index]\n\n            if cosine_sim >= similarity_threshold:\n                plagiarized_indices.append(i)\n\n        marked_input_text = input_text\n        if len(plagiarized_indices) > 0:\n            input_ngrams = remove_substrings(merge_plagiarized_ngrams(input_ngrams,plagiarized_indices,marked_input_text))\n            # Mark plagiarized segments with [QUOTE] and [ENDQUOTE]\n            for input_ngram in input_ngrams:\n                # Replace only whole words\n\n                if len(input_ngram) > 5:\n                    marked_input_text = marked_input_text.replace(f\"{input_ngram}\", f\"[QUOTE]{input_ngram}[ENDQUOTE]\")\n                    #print(marked_input_text)\n        marked_input_text = marked_input_text.replace(\"[ENDQUOTE][QUOTE]\", \" \")\n        marked_input_text = marked_input_text.replace(\"[ENDQUOTE] [QUOTE]\", \" \")\n        #print(marked_input_text)\n        #print(combine_nearby_quotes(marked_input_text))\n        #print(remove_short_quotes(combine_nearby_quotes(marked_input_text)))\n        tokenized_text = remove_short_quotes(combine_nearby_quotes(marked_input_text))\n\n        return  tokenized_text\n    except Exception:\n        return input_text\n\ndef merge_plagiarized_ngrams(ngrams, plagiarized_indices, text):\n    ngrams = [ngrams[indice] for indice in plagiarized_indices]\n    merged_ngrams = []\n    current_ngram = ngrams[0]\n    for ngram in ngrams[1:]:\n        last_word = current_ngram.split()[-1]\n        second_word = ngram.split()[1]\n        first_word = ngram.split()[0]\n        if last_word == second_word and (current_ngram + \" \" + \" \".join(ngram.split()[2:])) in text:  \n            current_ngram += \" \" + \" \".join(ngram.split()[2:])\n        elif last_word == first_word and (current_ngram + \" \" + \" \".join(ngram.split()[1:])) in text:\n            current_ngram += \" \" + \" \".join(ngram.split()[1:])\n        else:\n            merged_ngrams.append(current_ngram)\n            current_ngram = ngram\n\n    merged_ngrams.append(current_ngram)\n    #print(merged_ngrams)\n    return merged_ngrams","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:19:42.181775Z","iopub.execute_input":"2023-09-23T06:19:42.184274Z","iopub.status.idle":"2023-09-23T06:19:42.22108Z","shell.execute_reply.started":"2023-09-23T06:19:42.184219Z","shell.execute_reply":"2023-09-23T06:19:42.220091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"full_text\"] = [replace_quotes_with_passage(plagiarism_checker(text, prompt_text, n =3)) for text, prompt_text in zip(test[\"text\"].values,test[\"prompt_text\"].values)]\ntest","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:19:42.224791Z","iopub.execute_input":"2023-09-23T06:19:42.224998Z","iopub.status.idle":"2023-09-23T06:19:42.255162Z","shell.execute_reply.started":"2023-09-23T06:19:42.224974Z","shell.execute_reply":"2023-09-23T06:19:42.254306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(text: str, cfg) -> dict[str, torch.Tensor]:\n    inputs = cfg.tokenizer.encode_plus(\n        text,\n        return_tensors=None,\n        add_special_tokens=True,\n        max_length=cfg.max_len,\n        pad_to_max_length=True,\n        truncation=True,\n    )\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, cfg):\n        self.cfg = cfg\n        self.texts = df[\"full_text\"].values\n\n    def __len__(self) -> int:\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = self.texts[item]\n        inputs = prepare_input(text,self.cfg)\n        return inputs\n\n\ndef collate(inputs: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n    \"\"\"dynamic padding\"\"\"\n\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n    for k, v in inputs.items():\n        inputs[k] = v[:, :mask_len]\n    return inputs\n\n# ====================================================\n# Model\n# ====================================================\ndef freeze(module):\n    \"\"\"\n    Freezes module's parameters.\n    \"\"\"\n    for parameter in module.parameters():\n        parameter.requires_grad = False\n\n\ndef get_freezed_parameters(module):\n    \"\"\"\n    Returns names of freezed parameters of the given module.\n    \"\"\"\n    freezed_parameters = []\n    for name, parameter in module.named_parameters():\n        if not parameter.requires_grad:\n            freezed_parameters.append(name)\n    return freezed_parameters\n\n\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n\n    def forward(\n        self, last_hidden_state: torch.Tensor, attention_mask: torch.Tensor\n    ) -> torch.Tensor:\n        input_mask_expanded = (\n            attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        )\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\n\nclass CustomModel(nn.Module):\n    def __init__(\n        self, cfg, config_path: Path | None = None, pretrained: bool = False\n    ):\n        super().__init__()\n        self.cfg = cfg\n        self.n_target = len(cfg.target_cols)\n\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(\n                cfg.model, output_hidden_states=True\n            )\n            self.config.hidden_dropout = 0.0\n            self.config.hidden_dropout_prob = 0.0\n            self.config.attention_dropout = 0.0\n            self.config.attention_probs_dropout_prob = 0.0\n            LOGGER.info(self.config)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.model.resize_token_embeddings(len(cfg.tokenizer))\n        \n        if self.cfg.gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n\n        if cfg.multi_sample_dropouts is not None:\n            self.dropouts = nn.ModuleList(\n                [nn.Dropout(p) for p in cfg.multi_sample_dropouts]\n            )\n        else:\n            self.dropouts = None\n\n        self.fc = nn.Linear(self.config.hidden_size, self.n_target) \n\n\n        self._re_init_layers(self.cfg.layer_reinitialize_n)\n        if 1 <= self.cfg.freeze_n_layers:\n            freeze(self.model.embeddings)\n            freeze(\n                self.model.encoder.layer[: self.cfg.freeze_n_layers]\n            ) \n\n    def _re_init_layers(self, n_layers: int):\n        if n_layers >= 1:\n            for layer in self.model.encoder.layer[-n_layers:]:\n                # deverta-v3\n                if hasattr(layer, \"modules\"):\n                    for module in layer.modules():\n                        for name, child in module.named_children():\n                            init_type_name = self._init_weights(child)\n                            if init_type_name is not None:\n                                print(\n                                    f\"{name} is re-initialized, type: {init_type_name}, {module.__class__}\"\n                                )\n\n    def _init_weights(self, module: nn.Module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n            return \"nn.Linear\"\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n            return \"nn.Embedding\"\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n            return \"nn.LayerNorm\"\n        return None\n\n\n    def forward(self, inputs: dict[str, torch.Tensor]) -> torch.Tensor:\n        outputs = self.model(**inputs)\n        cls_hidden_state = outputs.last_hidden_state[:, 0, :] \n        output = self.fc(cls_hidden_state)\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:19:42.257791Z","iopub.execute_input":"2023-09-23T06:19:42.258345Z","iopub.status.idle":"2023-09-23T06:19:42.285168Z","shell.execute_reply.started":"2023-09-23T06:19:42.258311Z","shell.execute_reply":"2023-09-23T06:19:42.284253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.inference_mode()\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    for inputs in tqdm(test_loader, total=len(test_loader)):\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        y_preds = model(inputs)\n        preds.append(y_preds.to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:19:42.288533Z","iopub.execute_input":"2023-09-23T06:19:42.289484Z","iopub.status.idle":"2023-09-23T06:19:42.300287Z","shell.execute_reply.started":"2023-09-23T06:19:42.289449Z","shell.execute_reply":"2023-09-23T06:19:42.29931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFGw6.tokenizer)\ntokenizer.add_tokens(['[PASSAGE]','[PLAGIARISM]','[REFERENCE]','[ENDREFERENCE]'])\nCFGw6.tokenizer = tokenizer\n\ntest_dataset = TestDataset(test, CFGw6)\ntest_loader = DataLoader(test_dataset,\n  batch_size=CFGw6.batch_size,\n  shuffle=False,\n  collate_fn=DataCollatorWithPadding(tokenizer=CFGw6.tokenizer, padding='longest'),\n  num_workers=CFGw6.num_workers, pin_memory=True, drop_last=False)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\npredictions6 = []\nfor fold in CFGw6.trn_fold:\n    model = CustomModel(CFGw6, config_path='/kaggle/input/commonlit-wording-exp5/config.pth', pretrained=False)\n    state = torch.load(f'/kaggle/input/commonlit-wording-exp5/microsoft_deberta-v3-base_fold{fold}_best.pth',\n                      map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions6.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npredictions6 = np.mean(predictions6, axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:19:42.301796Z","iopub.execute_input":"2023-09-23T06:19:42.302075Z","iopub.status.idle":"2023-09-23T06:20:53.907496Z","shell.execute_reply.started":"2023-09-23T06:19:42.302035Z","shell.execute_reply":"2023-09-23T06:20:53.906464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions6","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:20:53.909711Z","iopub.execute_input":"2023-09-23T06:20:53.910359Z","iopub.status.idle":"2023-09-23T06:20:53.919074Z","shell.execute_reply.started":"2023-09-23T06:20:53.910323Z","shell.execute_reply":"2023-09-23T06:20:53.917945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Shigeria1","metadata":{}},{"cell_type":"code","source":"from __future__ import annotations\n\nparam = {\n    'awp_eps': 1e-2,\n    'awp_lr': 1e-4,\n    'batch_size': 8, # 2\n    'betas': (0.9, 0.999),\n    'ckpt_name': 'deberta_v3_large',\n    'debug': False, # False\n    'decoder_lr': 5e-5,\n    'encoder_lr': 2e-5,\n    'layerwise_lr_decay': 1.0,\n    'eps': 1e-6,\n    'max_len': 1024,\n    'min_lr': 1e-7,\n    'model_name': 'microsoft/deberta-v3-large',\n    'n_cycles': 0.5,\n    'n_epochs': 4, # 12\n    'n_eval_steps': 100000,\n    'n_folds': 4, # 4\n    'n_gradient_accumulation_steps': 1,\n    'n_warmup_steps': 0,\n    'n_workers': 2,\n    'nth_awp_start_epoch': 5, # 4\n    'output_dir': './output/',\n    'path': '/kaggle/input/cess-deberta-v3-large-exp26/',\n    'print_freq': 100,\n    'scheduler_name': 'cosine',\n    'max_grad_norm': 100000000.0,\n    'seed': 42,\n    'weight_decay': 0.01,\n    'config_path': '/kaggle/input/cess-deberta-v3-large-exp26/config.pth',\n    'reinit_layers': 0,\n}","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:20:53.921236Z","iopub.execute_input":"2023-09-23T06:20:53.921803Z","iopub.status.idle":"2023-09-23T06:20:53.931593Z","shell.execute_reply.started":"2023-09-23T06:20:53.921766Z","shell.execute_reply":"2023-09-23T06:20:53.930596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    def __init__(self, d: dict) -> None:\n        for k,v in d.items():\n            setattr(self, k, v)\n\ncfg = Config(d=param)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:20:53.933184Z","iopub.execute_input":"2023-09-23T06:20:53.933761Z","iopub.status.idle":"2023-09-23T06:20:53.945293Z","shell.execute_reply.started":"2023-09-23T06:20:53.933717Z","shell.execute_reply":"2023-09-23T06:20:53.944242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nif not os.path.exists(cfg.output_dir):\n    os.makedirs(cfg.output_dir)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:20:53.94702Z","iopub.execute_input":"2023-09-23T06:20:53.94759Z","iopub.status.idle":"2023-09-23T06:20:53.955351Z","shell.execute_reply.started":"2023-09-23T06:20:53.947554Z","shell.execute_reply":"2023-09-23T06:20:53.954334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\n\ndef seed_everything(seed:int) -> None:\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nseed_everything(seed=cfg.seed)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:20:53.958581Z","iopub.execute_input":"2023-09-23T06:20:53.959227Z","iopub.status.idle":"2023-09-23T06:20:53.971503Z","shell.execute_reply.started":"2023-09-23T06:20:53.959188Z","shell.execute_reply":"2023-09-23T06:20:53.970427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\ntest_df = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv')\nprompt_df = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv')\ntest_df = prompt_df.merge(test_df, on='prompt_id')\nsubmission = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/sample_submission.csv')\ntest_df","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:20:53.973208Z","iopub.execute_input":"2023-09-23T06:20:53.974428Z","iopub.status.idle":"2023-09-23T06:20:54.020447Z","shell.execute_reply.started":"2023-09-23T06:20:53.974394Z","shell.execute_reply":"2023-09-23T06:20:54.019417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(cfg.path+'tokenizer/')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:20:54.025526Z","iopub.execute_input":"2023-09-23T06:20:54.026422Z","iopub.status.idle":"2023-09-23T06:20:54.41288Z","shell.execute_reply.started":"2023-09-23T06:20:54.02636Z","shell.execute_reply":"2023-09-23T06:20:54.411933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nimport numpy as np\ndef MCRMSE(y_trues, y_preds):\n    scores = []\n    idxes = y_trues.shape[1]\n    for i in range(idxes):\n        y_true = y_trues[:,i]\n        y_pred = y_preds[:,i]\n        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n        scores.append(score)\n    mcrmse_score = np.mean(scores)\n    return mcrmse_score, scores\n\n\ndef get_score(y_trues, y_preds):\n    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n    return mcrmse_score, scores","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:20:54.414436Z","iopub.execute_input":"2023-09-23T06:20:54.414689Z","iopub.status.idle":"2023-09-23T06:20:54.422259Z","shell.execute_reply.started":"2023-09-23T06:20:54.414653Z","shell.execute_reply":"2023-09-23T06:20:54.421263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nfrom pandas import DataFrame\nfrom torch import Tensor\nfrom torch.utils.data import Dataset\nfrom transformers.tokenization_utils import PreTrainedTokenizer\n\nclass TestDataset(Dataset):\n    def __init__(self, df: DataFrame, tokenizer: PreTrainedTokenizer, max_len: int):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        if len(self.tokenizer.encode(\"\\n\\n\"))==2:\n            df[\"text\"] = df['text'].transform(lambda x:x.str.replace(\"\\n\\n\",\"|\"))\n            df[\"prompt_question\"] = df['prompt_question'].transform(lambda x:x.str.replace(\"\\n\\n\",\"|\"))\n            df[\"prompt_title\"] = df['prompt_title'].transform(lambda x:x.str.replace(\"\\n\\n\",\"|\"))\n            df[\"prompt_text\"] = df['prompt_text'].transform(lambda x:x.str.replace(\"\\n\\n\",\"|\"))\n        if len(self.tokenizer.encode(\"\\r\\n\"))==2:\n            df[\"text\"] = df['text'].transform(lambda x:x.str.replace(\"\\r\\n\",\"|\"))\n            df[\"prompt_question\"] = df['prompt_question'].transform(lambda x:x.str.replace(\"\\r\\n\",\"|\"))\n            df[\"prompt_title\"] = df['prompt_title'].transform(lambda x:x.str.replace(\"\\r\\n\",\"|\"))\n            df[\"prompt_text\"] = df['prompt_text'].transform(lambda x:x.str.replace(\"\\r\\n\",\"|\"))\n        self.prompt_ids = df['prompt_id'].to_numpy()\n        self.prompt_questions = df['prompt_question'].to_numpy()\n        self.prompt_titles = df['prompt_title'].to_numpy()\n        self.prompt_texts = df['prompt_text'].to_numpy()\n        self.ids = df['student_id'].to_numpy()\n        self.texts = df['text'].to_numpy()\n\n    def __len__(self) -> int:\n        return len(self.ids)\n\n    def __getitem__(self, item: int) -> \"tuple[dict, Tensor, Tensor]\":\n        \n        text1 = 'Content Wording'\n        text1 += self.tokenizer.sep_token\n        text1 += \"Instruction : \"\n        text1 += self.prompt_questions[item]\n        text1 += self.tokenizer.sep_token\n        text1 += \"Title : \"\n        text1 += self.prompt_titles[item]\n        text1 += self.tokenizer.sep_token\n        text1 += \"Summary : \"\n        text1 += self.texts[item]\n        text1 += self.tokenizer.sep_token\n        text1 += \"Full Text : \"\n        text1 += self.prompt_texts[item]\n\n        encoded1 = self.tokenizer(\n            text1,\n            max_length = self.max_len,\n            padding='max_length',\n            add_special_tokens=True,\n            truncation=True\n        )\n        \n        sep_ids = np.nonzero(np.array(encoded1[\"input_ids\"]) == self.tokenizer.sep_token_id)[0].tolist()\n        \n        sep_ids += [self.max_len for _ in range(5 - len(sep_ids))]\n\n        for k,v in encoded1.items():\n            encoded1[k] = torch.tensor(v, dtype=torch.long)\n\n        return encoded1, torch.tensor(sep_ids, dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:20:54.423765Z","iopub.execute_input":"2023-09-23T06:20:54.424413Z","iopub.status.idle":"2023-09-23T06:20:54.442583Z","shell.execute_reply.started":"2023-09-23T06:20:54.424361Z","shell.execute_reply":"2023-09-23T06:20:54.441536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nfrom torch.nn.parameter import Parameter\nimport torch.nn.functional as F\nimport copy\n\n\nclass GeMText(nn.Module):\n    def __init__(self, dim=1, p=3, eps=1e-6):\n        super(GeMText, self).__init__()\n        self.dim = dim\n        self.p = Parameter(torch.ones(1) * p)\n        self.eps = eps\n\n    def forward(self, x, attention_mask):\n        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(x.shape)\n        x = ((x.clamp(min=self.eps) * attention_mask_expanded).pow(self.p)).sum(self.dim)\n        ret = (x/(attention_mask_expanded.sum(self.dim))).clip(min=self.eps)\n        ret = ret.pow(1/self.p)\n        return ret\n    \nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:20:54.443771Z","iopub.execute_input":"2023-09-23T06:20:54.444585Z","iopub.status.idle":"2023-09-23T06:20:54.457385Z","shell.execute_reply.started":"2023-09-23T06:20:54.444544Z","shell.execute_reply":"2023-09-23T06:20:54.456535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MaskFilledAttentionHead(nn.Module):\n    def __init__(self, input_dim, head_hidden_dim):\n        super(MaskFilledAttentionHead, self).__init__()\n        head_hidden_dim = input_dim if head_hidden_dim is None else head_hidden_dim\n        self.W = nn.Linear(input_dim, head_hidden_dim)\n        self.ln = nn.LayerNorm(head_hidden_dim)\n        self.V = nn.Linear(head_hidden_dim, 1)\n        \n    def forward(self, x, attention_mask):\n        attention_scores = self.V(torch.tanh(self.ln(self.W(x))))\n        attention_scores[attention_mask==0] = -10.\n        attention_scores = torch.softmax(attention_scores, dim=1)\n        attentive_x = attention_scores * x\n        attentive_x = attentive_x.sum(axis=1)\n        return attentive_x","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:20:54.458915Z","iopub.execute_input":"2023-09-23T06:20:54.459285Z","iopub.status.idle":"2023-09-23T06:20:54.470491Z","shell.execute_reply.started":"2023-09-23T06:20:54.459187Z","shell.execute_reply":"2023-09-23T06:20:54.469407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import Tensor\nfrom torch.nn import Module\nfrom transformers import AutoModel, AutoConfig\n\nclass CustomModel(Module):\n    def __init__(self, cfg, model_name, config_path, n_vocabs) -> None:\n        super().__init__()\n        self.cfg = cfg\n        self.model_config = torch.load(config_path)\n        self.model = AutoModel.from_config(self.model_config)\n        self.pool = MeanPooling()\n        self.dec0 = nn.Sequential(\n            nn.Linear(self.model_config.hidden_size, self.model_config.hidden_size),\n            nn.LayerNorm(self.model_config.hidden_size, 1e-7)\n        )\n        self.dec1 = nn.Sequential(\n            nn.Linear(self.model_config.hidden_size, self.model_config.hidden_size),\n            nn.LayerNorm(self.model_config.hidden_size, 1e-7)\n        )\n        self.dec2 = nn.Sequential(\n            nn.Linear(self.model_config.hidden_size, self.model_config.hidden_size),\n            nn.LayerNorm(self.model_config.hidden_size, 1e-7)\n        )\n        self.dec3 = nn.Sequential(\n            nn.Linear(self.model_config.hidden_size, self.model_config.hidden_size),\n            nn.LayerNorm(self.model_config.hidden_size, 1e-7)\n        )\n        self.dec4 = nn.Sequential(\n            nn.Linear(self.model_config.hidden_size, self.model_config.hidden_size),\n            nn.LayerNorm(self.model_config.hidden_size, 1e-7)\n        )\n        self.dec5 = nn.Sequential(\n            nn.Linear(self.model_config.hidden_size, self.model_config.hidden_size),\n            nn.LayerNorm(self.model_config.hidden_size, 1e-7)\n        )\n\n        self.fc = nn.Sequential(\n            nn.Linear(self.model_config.hidden_size * 6, self.model_config.hidden_size * 3),\n            nn.LayerNorm(self.model_config.hidden_size * 3, 1e-7),\n            nn.ReLU(),\n            nn.Linear(self.model_config.hidden_size * 3, self.model_config.hidden_size),\n            nn.LayerNorm(self.model_config.hidden_size, 1e-7),\n            nn.ReLU(),\n            nn.Linear(self.model_config.hidden_size, 2)\n        )\n\n    def _init_weights(self, module: Module) -> None:\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(\n                mean=0.0, std=self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(\n                mean=0.0, std=self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def forward(self, input_ids, attention_mask, idxes) -> Tensor:\n        encoder_layer = self.model(input_ids=input_ids, attention_mask=attention_mask)[0]\n        \n        l0 = self.pool(encoder_layer, input_ids)\n        \n        bs = encoder_layer.shape[0]\n        \n        l1 = []\n\n        for i in range(bs):\n            if idxes[i][0] > 1:\n                l1.append(torch.mean(encoder_layer[i, 0:idxes[i][0]], dim=0))\n            else:\n                l1.append(torch.zeros((encoder_layer.size(2)), dtype=torch.float16, device=encoder_layer.device))\n\n        l1 = torch.stack(l1)  # (bs, h)\n        \n        l2 = []\n\n        for i in range(bs):\n            if idxes[i][1] - idxes[i][0] > 1:\n                l2.append(torch.mean(encoder_layer[i, idxes[i][0]:idxes[i][1]], dim=0))\n            else:\n                l2.append(torch.zeros((encoder_layer.size(2)), dtype=torch.float16, device=encoder_layer.device))\n\n        l2 = torch.stack(l2)  # (bs, h)\n        \n        l3 = []\n\n        for i in range(bs):\n            if idxes[i][2] - idxes[i][1] > 1:\n                l3.append(torch.mean(encoder_layer[i, idxes[i][1]:idxes[i][2]], dim=0))\n            else:\n                l3.append(torch.zeros((encoder_layer.size(2)), dtype=torch.float16, device=encoder_layer.device))\n\n        l3 = torch.stack(l3)  # (bs, h)\n        \n        l4 = []\n\n        for i in range(bs):\n            if idxes[i][3] - idxes[i][2] > 1:\n                l4.append(torch.mean(encoder_layer[i, idxes[i][2]:idxes[i][3]], dim=0))\n            else:\n                l4.append(torch.zeros((encoder_layer.size(2)), dtype=torch.float16, device=encoder_layer.device))\n\n        l4 = torch.stack(l4)  # (bs, h)\n        \n        l5 = []\n\n        for i in range(bs):\n            if idxes[i][4] - idxes[i][3] > 1:\n                l5.append(torch.mean(encoder_layer[i, idxes[i][3]:idxes[i][4]], dim=0))\n            else:\n                l5.append(torch.zeros((encoder_layer.size(2)), dtype=torch.float16, device=encoder_layer.device))\n\n        l5 = torch.stack(l5)  # (bs, h)\n\n        output = torch.cat([self.dec0(l0), self.dec1(l1), self.dec2(l2), self.dec3(l3), self.dec4(l4), self.dec5(l5)], 1)\n        output = self.fc(output)\n        return output\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:20:54.472204Z","iopub.execute_input":"2023-09-23T06:20:54.472522Z","iopub.status.idle":"2023-09-23T06:20:54.504743Z","shell.execute_reply.started":"2023-09-23T06:20:54.472491Z","shell.execute_reply":"2023-09-23T06:20:54.503517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate(inputs):\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n    for k, v in inputs.items():\n        inputs[k] = inputs[k][:,:mask_len]\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:20:54.508173Z","iopub.execute_input":"2023-09-23T06:20:54.508387Z","iopub.status.idle":"2023-09-23T06:20:54.518002Z","shell.execute_reply.started":"2023-09-23T06:20:54.508346Z","shell.execute_reply":"2023-09-23T06:20:54.517025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport random\nimport warnings\nfrom functools import reduce\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nfrom numpy import ndarray\nimport scipy as sp\nimport torch\nfrom torch import inference_mode\nfrom torch import nn\nfrom transformers import AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom IPython.display import display\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\nfrom sklearn import metrics\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    for step, (inputs, idxes) in enumerate(test_loader):\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        idxes = idxes.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs['input_ids'], inputs['attention_mask'], idxes)\n        preds.append(y_preds.to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions\n\ntest_dataset = TestDataset(df=test_df, tokenizer=tokenizer, max_len=cfg.max_len)\n\ntest_loader = DataLoader(test_dataset,\n                                batch_size=cfg.batch_size,\n                                shuffle=False,\n                                num_workers=cfg.n_workers, \n                                pin_memory=True, \n                                drop_last=False)\n\npredictions = []\nfor fold in range(cfg.n_folds):\n    model = CustomModel(cfg, cfg.model_name, config_path=cfg.config_path, n_vocabs=len(tokenizer))\n    state = torch.load(cfg.path+f\"{cfg.ckpt_name}_fold{fold}_best.pth\",\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state; gc.collect()\n    torch.cuda.empty_cache()\npredictions_shigeria = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:20:54.520575Z","iopub.execute_input":"2023-09-23T06:20:54.521281Z","iopub.status.idle":"2023-09-23T06:22:52.263842Z","shell.execute_reply.started":"2023-09-23T06:20:54.521246Z","shell.execute_reply":"2023-09-23T06:22:52.262698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[[\"content\", \"wording\"]] = predictions_shigeria\ntest_df","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:22:52.266954Z","iopub.execute_input":"2023-09-23T06:22:52.267195Z","iopub.status.idle":"2023-09-23T06:22:52.286936Z","shell.execute_reply.started":"2023-09-23T06:22:52.267167Z","shell.execute_reply":"2023-09-23T06:22:52.285867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_shigeria_content = predictions_shigeria[:, 0:1]\npredictions_shigeria_wording = predictions_shigeria[:, 1:2]","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:22:52.288689Z","iopub.execute_input":"2023-09-23T06:22:52.289102Z","iopub.status.idle":"2023-09-23T06:22:52.294692Z","shell.execute_reply.started":"2023-09-23T06:22:52.289064Z","shell.execute_reply":"2023-09-23T06:22:52.293629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Wording Ensemble","metadata":{}},{"cell_type":"code","source":"weights = [0.20208678, 0.16789604, 0.23403965, 0.42868295]","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:22:52.296464Z","iopub.execute_input":"2023-09-23T06:22:52.297159Z","iopub.status.idle":"2023-09-23T06:22:52.305423Z","shell.execute_reply.started":"2023-09-23T06:22:52.297118Z","shell.execute_reply":"2023-09-23T06:22:52.304229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = weights[0]*wording_w2+weights[1]*preds+weights[2]*predictions6 + weights[3]*predictions_shigeria_wording\npredictions","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:22:52.307202Z","iopub.execute_input":"2023-09-23T06:22:52.307494Z","iopub.status.idle":"2023-09-23T06:22:52.319488Z","shell.execute_reply.started":"2023-09-23T06:22:52.307453Z","shell.execute_reply":"2023-09-23T06:22:52.318427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['wording'] = predictions\nsubmission_wording = test[['student_id','wording']]\nsubmission_wording","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:22:52.321156Z","iopub.execute_input":"2023-09-23T06:22:52.321465Z","iopub.status.idle":"2023-09-23T06:22:52.337033Z","shell.execute_reply.started":"2023-09-23T06:22:52.321433Z","shell.execute_reply":"2023-09-23T06:22:52.335966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Content Ensemble","metadata":{}},{"cell_type":"markdown","source":"### c1 lgbm","metadata":{}},{"cell_type":"code","source":"!pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\"","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:25:42.592563Z","iopub.execute_input":"2023-09-23T06:25:42.593524Z","iopub.status.idle":"2023-09-23T06:26:15.756037Z","shell.execute_reply.started":"2023-09-23T06:25:42.593482Z","shell.execute_reply":"2023-09-23T06:26:15.754951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\n\nprompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\nsummaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\ny = prompts_test.merge(summaries_test, on=\"prompt_id\")","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:30:43.727847Z","iopub.execute_input":"2023-09-23T06:30:43.728126Z","iopub.status.idle":"2023-09-23T06:30:43.744916Z","shell.execute_reply.started":"2023-09-23T06:30:43.728096Z","shell.execute_reply":"2023-09-23T06:30:43.743564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y['content_c1'] = content_c1\ny['wording_pred'] = submission_wording['wording'].values","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:30:44.374821Z","iopub.execute_input":"2023-09-23T06:30:44.375092Z","iopub.status.idle":"2023-09-23T06:30:44.381408Z","shell.execute_reply.started":"2023-09-23T06:30:44.375063Z","shell.execute_reply":"2023-09-23T06:30:44.380255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:30:45.07743Z","iopub.execute_input":"2023-09-23T06:30:45.077865Z","iopub.status.idle":"2023-09-23T06:30:45.09304Z","shell.execute_reply.started":"2023-09-23T06:30:45.077834Z","shell.execute_reply":"2023-09-23T06:30:45.091898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom spellchecker import SpellChecker\nfrom nltk.corpus import stopwords\nimport spacy\nfrom collections import Counter\nimport nltk\nfrom tqdm import tqdm\ntqdm.pandas()\n\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\n\nfrom transformers import AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:30:46.836771Z","iopub.execute_input":"2023-09-23T06:30:46.837041Z","iopub.status.idle":"2023-09-23T06:30:46.843479Z","shell.execute_reply.started":"2023-09-23T06:30:46.837011Z","shell.execute_reply":"2023-09-23T06:30:46.842334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Preprocessor:\n    def __init__(self, model_name=\"/kaggle/input/microsoft-deberta-v3-large\") -> None:\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.STOP_WORDS = set(stopwords.words('english'))\n        \n        self.spacy_ner_model = spacy.load('en_core_web_sm')\n        \n        self.spellchecker = SpellChecker() \n\n    def word_overlap_count(self, row):\n        \"\"\" intersection(prompt_text, text) \"\"\"        \n        def check_is_stop_word(word):\n            return word in self.STOP_WORDS\n        \n        prompt_words = row['prompt_tokens']\n        summary_words = row['summary_tokens']\n        if self.STOP_WORDS:\n            prompt_words = list(filter(check_is_stop_word, prompt_words))\n            summary_words = list(filter(check_is_stop_word, summary_words))\n        return len(set(prompt_words).intersection(set(summary_words)))\n            \n    def ngrams(self, token, n):\n        # Use the zip function to help us generate n-grams\n        # Concatentate the tokens into ngrams and return\n        ngrams = zip(*[token[i:] for i in range(n)])\n        return [\" \".join(ngram) for ngram in ngrams]\n\n    def ngram_co_occurrence(self, row, n: int):\n        # Tokenize the original text and summary into words\n        original_tokens = row['prompt_tokens']\n        summary_tokens = row['summary_tokens']\n\n        # Generate n-grams for the original text and summary\n        original_ngrams = set(self.ngrams(original_tokens, n))\n        summary_ngrams = set(self.ngrams(summary_tokens, n))\n\n        # Calculate the number of common n-grams\n        common_ngrams = original_ngrams.intersection(summary_ngrams)\n\n        # # Optionally, you can get the frequency of common n-grams for a more nuanced analysis\n        # original_ngram_freq = Counter(ngrams(original_words, n))\n        # summary_ngram_freq = Counter(ngrams(summary_words, n))\n        # common_ngram_freq = {ngram: min(original_ngram_freq[ngram], summary_ngram_freq[ngram]) for ngram in common_ngrams}\n\n        return len(common_ngrams)\n    \n    def ner_overlap_count(self, row, mode:str):\n        model = self.spacy_ner_model\n        def clean_ners(ner_list):\n            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n        prompt = model(row['prompt_text'])\n        summary = model(row['text'])\n\n        if \"spacy\" in str(model):\n            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n        elif \"stanza\" in str(model):\n            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n            summary_ner = set([(token.text, token.type) for token in summary.ents])\n        else:\n            raise Exception(\"Model not supported\")\n\n        prompt_ner = clean_ners(prompt_ner)\n        summary_ner = clean_ners(summary_ner)\n\n        intersecting_ners = prompt_ner.intersection(summary_ner)\n        \n        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n        \n        if mode == \"train\":\n            return ner_dict\n        elif mode == \"test\":\n            return {key: ner_dict.get(key) for key in self.ner_keys}\n\n    \n    def quotes_count(self, row):\n        summary = row['text']\n        text = row['prompt_text']\n        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n        if len(quotes_from_summary)>0:\n            return [quote in text for quote in quotes_from_summary].count(True)\n        else:\n            return 0\n\n    def spelling(self, text):\n        \n        wordlist=text.split()\n        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n\n        return amount_miss\n    \n    def add_spelling_dictionary(self, tokens):\n        \"\"\"dictionary update for pyspell checker and autocorrect\"\"\"\n        self.spellchecker.word_frequency.load_words(tokens)\n        self.speller.nlp_data.update({token:1000 for token in tokens})\n        \n    def filter_stopwords(self, text):\n        stop_words = set(stopwords.words('english'))\n        stop_lens = len([i for i in text if i in stop_words])\n        \n        return stop_lens\n    \n    def parse_pos(self, text, pos):\n        pos_dict = Counter([j for i,j in nltk.pos_tag(text)])\n        n = 0\n        for k in pos:\n            if k in pos_dict:\n                n += pos_dict[k]\n                \n        return n\n    \n    def run(self, \n            summaries:pd.DataFrame,\n            mode:str\n        ) -> pd.DataFrame:\n        \n        summaries[\"prompt_length\"] = summaries[\"prompt_text\"].apply(\n            lambda x: len(self.tokenizer.encode(x))\n        )\n        summaries[\"prompt_tokens\"] = summaries[\"prompt_text\"].apply(\n            lambda x: self.tokenizer.convert_ids_to_tokens(\n                self.tokenizer.encode(x), \n                skip_special_tokens=True\n            )\n        )\n        \n        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n            lambda x: len(self.tokenizer.encode(x))\n        )\n        summaries[\"summary_unique_length\"] = summaries[\"text\"].apply(\n            lambda x: len(set(self.tokenizer.encode(x)))\n        )\n        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n            lambda x: self.tokenizer.convert_ids_to_tokens(self.tokenizer.encode(x), skip_special_tokens=True)\n        )\n        \n        summaries['stopword_count'] = summaries['summary_tokens'].progress_apply(self.filter_stopwords)\n        summaries['stopword_ratio'] = summaries['stopword_count'] / summaries['summary_length']\n        \n#         summaries['pos_NN_count'] = summaries['summary_tokens'].progress_apply(self.parse_pos,args=(['NN'],))\n#         summaries['pos_NNP_count'] = summaries['summary_tokens'].progress_apply(self.parse_pos,args=(['NNP'],))\n#         summaries['pos_RB_count'] = summaries['summary_tokens'].progress_apply(self.parse_pos,args=(['RB'],))\n#         summaries['pos_RP_count'] = summaries['summary_tokens'].progress_apply(self.parse_pos,args=(['RP'],))\n#         summaries['pos_WPWRB_count'] = summaries['summary_tokens'].progress_apply(self.parse_pos,args=(['WP','WRB'],))\n        \n        #summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(lambda x: self.speller(x))\n    \n        \n        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n        # summaries[\"fixed_splling_err_num\"] = summaries[\"fixed_summary_text\"].progress_apply(self.spelling)\n        summaries[\"splling_err_ratio\"] = summaries[\"splling_err_num\"] / summaries[\"summary_length\"]\n\n        # merge prompts and summaries\n        input_df = summaries.copy()\n\n        # after merge preprocess\n        #input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n        #input_df['unique_ratio'] = input_df['summary_unique_length'] / input_df['prompt_length']\n        input_df['unique_length_ratio'] = input_df['summary_unique_length'] / input_df['summary_length']\n        \n        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n        \n        input_df['bigram_overlap_count'] = input_df.progress_apply(self.ngram_co_occurrence,args=(2,), axis=1)\n        input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n        \n        input_df['trigram_overlap_count'] = input_df.progress_apply(self.ngram_co_occurrence, args=(3,), axis=1)\n        input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n        \n        # Crate dataframe with count of each category NERs overlap for all the summaries\n        # Because it spends too much time for this feature, I don't use this time.\n#         ners_count_df  = input_df.progress_apply(\n#             lambda row: pd.Series(self.ner_overlap_count(row, mode=mode), dtype='float64'), axis=1\n#         ).fillna(0)\n#         self.ner_keys = ners_count_df.columns\n#         ners_count_df['sum'] = ners_count_df.sum(axis=1)\n#         ners_count_df.columns = ['NER_' + col for col in ners_count_df.columns]\n#         # join ner count dataframe with train dataframe\n#         input_df = pd.concat([input_df, ners_count_df], axis=1)\n        \n        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n        # input_df['fixed_splling_err_ratio'] = input_df['fixed_splling_err_num'] / input_df['summary_length']\n        input_df['quotes_ratio'] = input_df['quotes_count'] / input_df['summary_length']\n        \n        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:30:47.79464Z","iopub.execute_input":"2023-09-23T06:30:47.794926Z","iopub.status.idle":"2023-09-23T06:30:47.833033Z","shell.execute_reply.started":"2023-09-23T06:30:47.794896Z","shell.execute_reply":"2023-09-23T06:30:47.831904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessor = Preprocessor()\ny_test = preprocessor.run(y, mode='test')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:30:48.665788Z","iopub.execute_input":"2023-09-23T06:30:48.666059Z","iopub.status.idle":"2023-09-23T06:30:50.892243Z","shell.execute_reply.started":"2023-09-23T06:30:48.666028Z","shell.execute_reply":"2023-09-23T06:30:50.890708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:30:52.051753Z","iopub.execute_input":"2023-09-23T06:30:52.052021Z","iopub.status.idle":"2023-09-23T06:30:52.075162Z","shell.execute_reply.started":"2023-09-23T06:30:52.051992Z","shell.execute_reply":"2023-09-23T06:30:52.07368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = [\"content\"]\n\ndrop_columns = [\"student_id\", \"prompt_id\", \"text\", \n                \"prompt_question\", \"prompt_title\", \"prompt_length\", \"prompt_text\"\n               ]","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:30:55.293853Z","iopub.execute_input":"2023-09-23T06:30:55.29486Z","iopub.status.idle":"2023-09-23T06:30:55.300491Z","shell.execute_reply.started":"2023-09-23T06:30:55.294811Z","shell.execute_reply":"2023-09-23T06:30:55.299435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ordered_columns = ['prompt_length', 'summary_length', 'summary_unique_length', 'stopword_count', 'stopword_ratio',\n                   'splling_err_num', 'splling_err_ratio', 'wording_pred', 'content_c1', 'unique_length_ratio',\n                   'word_overlap_count', 'bigram_overlap_count', 'bigram_overlap_ratio', 'trigram_overlap_count', 'trigram_overlap_ratio', \n                   'quotes_count', 'quotes_ratio'\n]","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:30:58.347859Z","iopub.execute_input":"2023-09-23T06:30:58.348159Z","iopub.status.idle":"2023-09-23T06:30:58.356318Z","shell.execute_reply.started":"2023-09-23T06:30:58.348126Z","shell.execute_reply":"2023-09-23T06:30:58.355426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_paths = glob.glob(\"/kaggle/input/commomlit-lgb-c1-0527wording/*\")\n\npredictions = []\nfor fold, model_path in enumerate(model_paths):\n    model = lgb.Booster(model_file=model_path)\n    X_eval_cv = y_test.drop(columns=drop_columns)\n    X_eval_cv = X_eval_cv.reindex(columns=ordered_columns)\n    pred = model.predict(X_eval_cv)\n    predictions.append(pred)\n\npreds = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:30:59.938233Z","iopub.execute_input":"2023-09-23T06:30:59.939272Z","iopub.status.idle":"2023-09-23T06:31:00.009933Z","shell.execute_reply.started":"2023-09-23T06:30:59.939224Z","shell.execute_reply":"2023-09-23T06:31:00.008874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = [0.6295255599255956, 0.3993599947505763]","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:26:49.442925Z","iopub.execute_input":"2023-09-23T06:26:49.443149Z","iopub.status.idle":"2023-09-23T06:26:49.449462Z","shell.execute_reply.started":"2023-09-23T06:26:49.443123Z","shell.execute_reply":"2023-09-23T06:26:49.448407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = weights[0]*predictions_shigeria_content + weights[1]*preds.reshape(-1,1)\npredictions","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:31:27.331489Z","iopub.execute_input":"2023-09-23T06:31:27.331772Z","iopub.status.idle":"2023-09-23T06:31:27.339541Z","shell.execute_reply.started":"2023-09-23T06:31:27.331744Z","shell.execute_reply":"2023-09-23T06:31:27.338634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['content'] = predictions\nsubmission_content = test[[\"student_id\",\"content\"]]\nsubmission_content","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:31:31.341036Z","iopub.execute_input":"2023-09-23T06:31:31.341321Z","iopub.status.idle":"2023-09-23T06:31:31.354445Z","shell.execute_reply.started":"2023-09-23T06:31:31.341291Z","shell.execute_reply":"2023-09-23T06:31:31.353109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Submission file","metadata":{}},{"cell_type":"code","source":"submission_content","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:31:35.51325Z","iopub.execute_input":"2023-09-23T06:31:35.513863Z","iopub.status.idle":"2023-09-23T06:31:35.524332Z","shell.execute_reply.started":"2023-09-23T06:31:35.513826Z","shell.execute_reply":"2023-09-23T06:31:35.52295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_wording","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:31:36.049039Z","iopub.execute_input":"2023-09-23T06:31:36.049307Z","iopub.status.idle":"2023-09-23T06:31:36.059324Z","shell.execute_reply.started":"2023-09-23T06:31:36.049277Z","shell.execute_reply":"2023-09-23T06:31:36.058209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = submission_content.merge(submission_wording,on=\"student_id\")\nsubmission","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:31:36.733599Z","iopub.execute_input":"2023-09-23T06:31:36.734433Z","iopub.status.idle":"2023-09-23T06:31:36.750458Z","shell.execute_reply.started":"2023-09-23T06:31:36.734384Z","shell.execute_reply":"2023-09-23T06:31:36.748331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[\"content\"] = submission[\"content\"].clip(-2, 4.5)\nsubmission[\"wording\"] = submission[\"wording\"].clip(-2, 4.5)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:31:37.622722Z","iopub.execute_input":"2023-09-23T06:31:37.623Z","iopub.status.idle":"2023-09-23T06:31:37.637703Z","shell.execute_reply.started":"2023-09-23T06:31:37.622971Z","shell.execute_reply":"2023-09-23T06:31:37.636678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T06:31:38.578892Z","iopub.execute_input":"2023-09-23T06:31:38.579164Z","iopub.status.idle":"2023-09-23T06:31:38.58818Z","shell.execute_reply.started":"2023-09-23T06:31:38.579134Z","shell.execute_reply":"2023-09-23T06:31:38.586749Z"},"trusted":true},"execution_count":null,"outputs":[]}]}