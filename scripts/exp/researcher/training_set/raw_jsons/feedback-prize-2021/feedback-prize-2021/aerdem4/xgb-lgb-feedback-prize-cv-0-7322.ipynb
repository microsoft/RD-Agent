{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Configuration","metadata":{"gradient":{"editing":false,"id":"e0167440-e64f-45dd-89fd-961bba1bea34","kernelId":""},"papermill":{"duration":0.025052,"end_time":"2022-03-09T13:26:45.390222","exception":false,"start_time":"2022-03-09T13:26:45.36517","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\n\nos.listdir(\"/kaggle/input\")","metadata":{"execution":{"iopub.execute_input":"2022-03-09T13:26:45.447445Z","iopub.status.busy":"2022-03-09T13:26:45.446648Z","iopub.status.idle":"2022-03-09T13:26:45.453463Z","shell.execute_reply":"2022-03-09T13:26:45.453963Z","shell.execute_reply.started":"2022-03-08T18:13:20.443882Z"},"papermill":{"duration":0.040077,"end_time":"2022-03-09T13:26:45.454191","exception":false,"start_time":"2022-03-09T13:26:45.414114","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from cuml import ForestInference\n\n\ndiscourses = ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement', 'Counterclaim', 'Rebuttal']\nxgb_models, lgb_models = dict(), dict()\n\nensemble_weights = {\"Rebuttal\": 0.65,\n                    \"Counterclaim\": 0.75,\n                    \"Concluding Statement\": 0.60,\n                    \"Claim\": 0.65,\n                    \"Evidence\": 0.60,\n                    \"Position\": 0.75,\n                    \"Lead\": 0.70\n                    }\n\n\nthresholds = {'Lead': 0.66,\n 'Position': 0.56,\n 'Evidence': 0.57,\n 'Claim': 0.54,\n 'Concluding Statement': 0.56,\n 'Counterclaim': 0.7,\n 'Rebuttal': 0.74}\n\nfeatures_dict = {'Lead': [i for i in range(34)],\n 'Position': [i for i in range(34)],\n 'Evidence': [i for i in range(20)],\n 'Claim': [i for i in range(20)],\n 'Concluding Statement': [i for i in range(34)],\n 'Counterclaim': [i for i in range(17)] + [i for i in range(27, 34)],\n 'Rebuttal': [i for i in range(17)]}\n\nN_XGB_FOLDS = 5\n\nfor d in discourses:\n    model_list = []\n    for f in range(N_XGB_FOLDS):\n        xgb_model = ForestInference.load(f\"../input/student-writing-7322/xgb_{d}_{f}.json\", output_class=True, model_type=\"xgboost_json\")\n        model_list.append(xgb_model)\n    xgb_models[d] = model_list\n    \n    model_list = []\n    for f in range(N_XGB_FOLDS):\n        lgb_model = ForestInference.load(f\"../input/student-writing-7322/lgb_{d}_{f}.txt\", output_class=True, model_type=\"lightgbm\")\n        model_list.append(lgb_model)\n    lgb_models[d] = model_list","metadata":{"execution":{"iopub.execute_input":"2022-03-09T13:26:45.506457Z","iopub.status.busy":"2022-03-09T13:26:45.505763Z","iopub.status.idle":"2022-03-09T13:26:54.687467Z","shell.execute_reply":"2022-03-09T13:26:54.688271Z","shell.execute_reply.started":"2022-03-07T05:26:10.716256Z"},"papermill":{"duration":9.213727,"end_time":"2022-03-09T13:26:54.688458","exception":false,"start_time":"2022-03-09T13:26:45.474731","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_tp_prob(testDs, disc_type):\n\n    if testDs.features.shape[0] == 0:\n        return np.array([])\n\n    pred = np.mean([clf.predict_proba(testDs.features[:, features_dict[disc_type]].astype(\"float32\"))[:,1] for clf in xgb_models[disc_type]], axis=0)/2\n    pred += np.mean([clf.predict_proba(testDs.features[:, features_dict[disc_type]].astype(\"float32\"))[:, 1] for clf in lgb_models[disc_type]], axis=0)/2\n\n    return pred","metadata":{"execution":{"iopub.execute_input":"2022-03-09T13:26:54.73578Z","iopub.status.busy":"2022-03-09T13:26:54.734995Z","iopub.status.idle":"2022-03-09T13:26:54.736977Z","shell.execute_reply":"2022-03-09T13:26:54.737351Z","shell.execute_reply.started":"2022-03-07T05:26:36.110299Z"},"papermill":{"duration":0.027204,"end_time":"2022-03-09T13:26:54.737482","exception":false,"start_time":"2022-03-09T13:26:54.710278","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, sys\n# DECLARE HOW MANY GPUS YOU WISH TO USE. \n# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu\n\n# IF VARIABLE IS NONE, THEN NOTEBOOK COMPUTES TOKENS\n# OTHERWISE NOTEBOOK LOADS TOKENS FROM PATH\nLOAD_TOKENS_FROM = '../input/py-bigbird-v26'\n\n# IF VARIABLE IS NONE, THEN NOTEBOOK TRAINS A NEW MODEL\n# OTHERWISE IT LOADS YOUR PREVIOUSLY TRAINED MODEL\nLOAD_MODEL_FROM = '../input/fp-test78'\n\n# IF FOLLOWING IS NONE, THEN NOTEBOOK \n# USES INTERNET AND DOWNLOADS HUGGINGFACE \n# CONFIG, TOKENIZER, AND MODEL\nDOWNLOADED_MODEL_PATH = '../input/deberta-xlarge' \n\n\n# A cache of the BigBird predictions for the validation/sequence training set and the corresponding sequence dataset\nKAGGLE_CACHE = '../input/feedbackcache2'\n\nN_FEATURES=34\n\nTEST_PERCENT = None\n\ncache = 'cache'\ncacheExists = os.path.exists(cache)\nif not cacheExists:\n  os.makedirs(cache)","metadata":{"execution":{"iopub.execute_input":"2022-03-09T13:26:54.7841Z","iopub.status.busy":"2022-03-09T13:26:54.782578Z","iopub.status.idle":"2022-03-09T13:26:54.784673Z","shell.execute_reply":"2022-03-09T13:26:54.785103Z","shell.execute_reply.started":"2022-03-01T23:20:19.886495Z"},"gradient":{"editing":false,"execution_count":2,"id":"8e783988-827d-4a32-9402-61603b89347b","kernelId":""},"papermill":{"duration":0.027739,"end_time":"2022-03-09T13:26:54.785232","exception":false,"start_time":"2022-03-09T13:26:54.757493","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import cuda\nconfig = {'model_name': '',   \n         'max_length': 2048,\n         'train_batch_size':4,\n         'valid_batch_size':4,\n         'epochs':5,\n         'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n         'max_grad_norm':10,\n         'device': 'cuda' if cuda.is_available() else 'cpu'}","metadata":{"execution":{"iopub.execute_input":"2022-03-09T13:26:54.829835Z","iopub.status.busy":"2022-03-09T13:26:54.828816Z","iopub.status.idle":"2022-03-09T13:26:56.095758Z","shell.execute_reply":"2022-03-09T13:26:56.096251Z","shell.execute_reply.started":"2022-03-01T23:20:19.893966Z"},"gradient":{"editing":false,"execution_count":4,"id":"42571122-fde4-4d21-959f-2bd6327e8741","kernelId":""},"papermill":{"duration":1.291163,"end_time":"2022-03-09T13:26:56.096423","exception":false,"start_time":"2022-03-09T13:26:54.80526","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np, os \nfrom scipy import stats\nimport pandas as pd, gc \nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, AdamW\nfrom transformers import *\n\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nfrom sklearn.metrics import accuracy_score\nfrom torch.cuda import amp\nimport warnings\n\nwarnings.filterwarnings('ignore', '.*__floordiv__ is deprecated.*',)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-03-09T13:26:56.142671Z","iopub.status.busy":"2022-03-09T13:26:56.141642Z","iopub.status.idle":"2022-03-09T13:27:04.445068Z","shell.execute_reply":"2022-03-09T13:27:04.444579Z","shell.execute_reply.started":"2022-03-01T23:20:21.357805Z"},"gradient":{"editing":false,"execution_count":6,"id":"685d622c-38b1-4e57-99ff-c19b4d0e4b63","kernelId":""},"papermill":{"duration":8.32819,"end_time":"2022-03-09T13:27:04.445206","exception":false,"start_time":"2022-03-09T13:26:56.117016","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\ntest_names, test_texts = [], []\nfor f in list(os.listdir('../input/feedback-prize-2021/test')):\n    test_names.append(f.replace('.txt', ''))\n    test_texts.append(open('../input/feedback-prize-2021/test/' + f, 'r').read())\ntest_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\n\nif TEST_PERCENT is not None:\n    print(f\"testing and submitting with only {TEST_PERCENT} of test data\")\n    np.random.seed(2022)\n    test_select=np.arange(len(test_texts))\n    np.random.shuffle(test_select)\n    test_texts=test_texts.iloc[test_select[:int(TEST_PERCENT*len(test_texts))]].reset_index()\n\n#sort by length of texts to minimize padding in each batch\ntest_texts['len']=test_texts['text'].apply(lambda x:len(x.split()))\ntest_texts=test_texts.sort_values(by=['len']).reset_index()\n\n\n    \n\ntest_texts\n\nSUBMISSION = True\nif len(test_names) > 5:\n      SUBMISSION = True\n\ntest_texts.head()","metadata":{"execution":{"iopub.execute_input":"2022-03-09T13:27:04.49366Z","iopub.status.busy":"2022-03-09T13:27:04.493046Z","iopub.status.idle":"2022-03-09T13:27:04.535275Z","shell.execute_reply":"2022-03-09T13:27:04.534531Z","shell.execute_reply.started":"2022-03-01T23:20:29.555884Z"},"gradient":{"execution_count":9,"id":"00c14a04-6f87-416f-a18f-2f50165da64a","kernelId":""},"papermill":{"duration":0.06998,"end_time":"2022-03-09T13:27:04.535398","exception":false,"start_time":"2022-03-09T13:27:04.465418","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convert Train Text to NER Labels\nWe will now convert all text words into NER labels and save in a dataframe.","metadata":{"papermill":{"duration":0.020076,"end_time":"2022-03-09T13:27:04.575663","exception":false,"start_time":"2022-03-09T13:27:04.555587","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# CREATE DICTIONARIES THAT WE CAN USE DURING TRAIN AND INFER\noutput_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n\nlabels_to_ids = {v:k for k,v in enumerate(output_labels)}\nids_to_labels = {k:v for k,v in enumerate(output_labels)}\ndisc_type_to_ids = {'Evidence':(11,12),'Claim':(5,6),'Lead':(1,2),'Position':(3,4),'Counterclaim':(7,8),'Rebuttal':(9,10),'Concluding Statement':(13,14)}","metadata":{"execution":{"iopub.execute_input":"2022-03-09T13:27:04.622581Z","iopub.status.busy":"2022-03-09T13:27:04.621771Z","iopub.status.idle":"2022-03-09T13:27:04.62379Z","shell.execute_reply":"2022-03-09T13:27:04.624205Z","shell.execute_reply.started":"2022-03-01T23:20:29.609872Z"},"gradient":{"execution_count":12,"id":"ee648d66-852f-47a5-a404-5890cde5c054","kernelId":""},"papermill":{"duration":0.02841,"end_time":"2022-03-09T13:27:04.624341","exception":false,"start_time":"2022-03-09T13:27:04.595931","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_to_ids","metadata":{"execution":{"iopub.execute_input":"2022-03-09T13:27:04.67224Z","iopub.status.busy":"2022-03-09T13:27:04.671528Z","iopub.status.idle":"2022-03-09T13:27:04.674171Z","shell.execute_reply":"2022-03-09T13:27:04.674598Z","shell.execute_reply.started":"2022-03-01T23:20:29.618074Z"},"gradient":{"execution_count":13,"id":"181002ad-1436-4890-8230-149cd4e7fcc1","kernelId":""},"papermill":{"duration":0.029534,"end_time":"2022-03-09T13:27:04.67472","exception":false,"start_time":"2022-03-09T13:27:04.645186","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define the dataset function\nBelow is our PyTorch dataset function. It always outputs tokens and attention. During training it also provides labels. And during inference it also provides word ids to help convert token predictions into word predictions.\n\nNote that we use `text.split()` and `is_split_into_words=True` when we convert train text to labeled train tokens. This is how the HugglingFace tutorial does it. However, this removes characters like `\\n` new paragraph. If you want your model to see new paragraphs, then we need to map words to tokens ourselves using `return_offsets_mapping=True`. See my TensorFlow notebook [here][1] for an example.\n\nSome of the following code comes from the example at HuggingFace [here][2]. However I think the code at that link is wrong. The HuggingFace original code is [here][3]. With the flag `LABEL_ALL` we can either label just the first subword token (when one word has more than one subword token). Or we can label all the subword tokens (with the word's label). In this notebook version, we label all the tokens. There is a Kaggle discussion [here][4]\n\n[1]: https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-617\n[2]: https://huggingface.co/docs/transformers/custom_datasets#tok_ner\n[3]: https://github.com/huggingface/transformers/blob/86b40073e9aee6959c8c85fcba89e47b432c4f4d/examples/pytorch/token-classification/run_ner.py#L371\n[4]: https://www.kaggle.com/c/feedback-prize-2021/discussion/296713","metadata":{"papermill":{"duration":0.020134,"end_time":"2022-03-09T13:27:04.714906","exception":false,"start_time":"2022-03-09T13:27:04.694772","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Return an array that maps character index to index of word in list of split() words\ndef split_mapping(unsplit):\n    splt = unsplit.split()\n    offset_to_wordidx = np.full(len(unsplit),-1)\n    txt_ptr = 0\n    for split_index, full_word in enumerate(splt):\n        while unsplit[txt_ptr:txt_ptr + len(full_word)] != full_word:\n            txt_ptr += 1\n        offset_to_wordidx[txt_ptr:txt_ptr + len(full_word)] = split_index\n        txt_ptr += len(full_word)\n    return offset_to_wordidx","metadata":{"execution":{"iopub.execute_input":"2022-03-09T13:27:04.758598Z","iopub.status.busy":"2022-03-09T13:27:04.757819Z","iopub.status.idle":"2022-03-09T13:27:04.763642Z","shell.execute_reply":"2022-03-09T13:27:04.763251Z","shell.execute_reply.started":"2022-03-01T23:20:29.634056Z"},"gradient":{"execution_count":14,"id":"24ebb3cc-c13f-4356-b3b4-6e9dba4727ff","kernelId":""},"papermill":{"duration":0.0286,"end_time":"2022-03-09T13:27:04.76375","exception":false,"start_time":"2022-03-09T13:27:04.73515","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class dataset(Dataset):\n  def __init__(self, dataframe, tokenizer, max_len, get_wids):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.get_wids = get_wids # for validation\n\n  def __getitem__(self, index):\n        # GET TEXT AND WORD LABELS \n        text = self.data.text[index]        \n        word_labels = self.data.entities[index] if not self.get_wids else None\n\n        # TOKENIZE TEXT\n        encoding = self.tokenizer(text,\n                             return_offsets_mapping=True, \n                             padding=False, \n                             truncation=True, \n                             max_length=self.max_len)\n        \n        word_ids = encoding.word_ids()  \n        split_word_ids = np.full(len(word_ids),-1)\n        offset_to_wordidx = split_mapping(text)\n        offsets = encoding['offset_mapping']\n        \n        # CREATE TARGETS AND MAPPING OF TOKENS TO SPLIT() WORDS\n        label_ids = []\n        # Iterate in reverse to label whitespace tokens until a Begin token is encountered\n        for token_idx, word_idx in reversed(list(enumerate(word_ids))):\n            \n            if word_idx is None:\n                if not self.get_wids: label_ids.append(-100)\n            else:\n                if offsets[token_idx][0] != offsets[token_idx][1]:\n                    #Choose the split word that shares the most characters with the token if any\n                    split_idxs = offset_to_wordidx[offsets[token_idx][0]:offsets[token_idx][1]]\n                    split_index = stats.mode(split_idxs[split_idxs != -1]).mode[0] if len(np.unique(split_idxs)) > 1 else split_idxs[0]\n                    \n                    if split_index != -1: \n                        if not self.get_wids: label_ids.append( labels_to_ids[word_labels[split_index]] )\n                        split_word_ids[token_idx] = split_index\n                    else:\n                        # Even if we don't find a word, continue labeling 'I' tokens until a 'B' token is found\n                        if label_ids and label_ids[-1] != -100 and ids_to_labels[label_ids[-1]][0] == 'I':\n                            split_word_ids[token_idx] = split_word_ids[token_idx + 1]\n                            if not self.get_wids: label_ids.append(label_ids[-1])\n                        else:\n                            if not self.get_wids: label_ids.append(-100)\n                else:\n                    if not self.get_wids: label_ids.append(-100)\n        \n        encoding['labels'] = list(reversed(label_ids))\n\n        # CONVERT TO TORCH TENSORS\n        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n        if self.get_wids: \n            item['wids'] = torch.as_tensor(split_word_ids)\n        \n        return item\n\n  def __len__(self):\n        return self.len\n    \nclass CustomCollate:\n    def __init__(self,tokenizer,sliding_window=None):\n        self.tokenizer=tokenizer\n        self.sliding_window=sliding_window\n\n    def __call__(self,data):\n        \"\"\"\n        need to collate: input_ids, attention_mask, labels\n        input_ids is padded with 1, attention_mask 0, labels -100\n\n        \"\"\"\n\n        bs=len(data)\n        lengths=[]\n        for i in range(bs):\n            lengths.append(len(data[i]['input_ids']))\n            # print(data[i]['input_ids'].shape)\n            # print(data[i]['attention_mask'].shape)\n            # print(data[i]['labels'].shape)\n        max_len=max(lengths)\n        if self.sliding_window is not None and max_len > self.sliding_window:\n            max_len= int((np.floor(max_len/self.sliding_window-1e-6)+1)*self.sliding_window)\n        #always pad the right side\n        input_ids, attention_mask, labels, BIO_labels, discourse_labels=[],[],[],[],[]\n        #if np.random.uniform()>0.5:\n        #print(data[0].keys())\n        #print(max_len)\n        if 'wids' in data[0]:\n            get_wids=True\n        else:\n            get_wids=False\n        #print(get_wids)\n        wids = []\n            #wids.append(torch.nn.functional.pad(data[i]['wids'],(0,max_len-lengths[i]),value=-1))\n        for i in range(bs):\n            input_ids.append(torch.nn.functional.pad(data[i]['input_ids'],(0,max_len-lengths[i]),value=self.tokenizer.pad_token_id))\n            attention_mask.append(torch.nn.functional.pad(data[i]['attention_mask'],(0,max_len-lengths[i]),value=0))\n            #labels.append(torch.nn.functional.pad(data[i]['labels'],(0,max_len-lengths[i]),value=-100))\n            #BIO_labels.append(torch.nn.functional.pad(data[i]['BIO_labels'],(0,max_len-lengths[i]),value=-100))\n            #discourse_labels.append(torch.nn.functional.pad(data[i]['discourse_labels'],(0,max_len-lengths[i]),value=-100))\n            if get_wids:\n                wids.append(torch.nn.functional.pad(data[i]['wids'],(0,max_len-lengths[i]),value=-1))\n        # else:\n        #     for i in range(bs):\n        #         input_ids.append(torch.nn.functional.pad(data[i]['input_ids'],(max_len-lengths[i],0),value=1))\n        #         attention_mask.append(torch.nn.functional.pad(data[i]['attention_mask'],(max_len-lengths[i],0),value=0))\n        #         labels.append(torch.nn.functional.pad(data[i]['labels'],(max_len-lengths[i],0),value=-100))\n\n        input_ids=torch.stack(input_ids)\n        attention_mask=torch.stack(attention_mask)\n        #labels=torch.stack(labels)\n        #BIO_labels=torch.stack(BIO_labels)\n        #discourse_labels=torch.stack(discourse_labels)\n        if get_wids:\n            wids=torch.stack(wids)\n        #exit()\n        if get_wids:\n            return {\"input_ids\":input_ids,\"attention_mask\":attention_mask,\n            \"labels\":labels,\"BIO_labels\":BIO_labels,\"discourse_labels\":discourse_labels,\n            \"wids\":wids}\n        else:\n            return {\"input_ids\":input_ids,\"attention_mask\":attention_mask,\n            \"labels\":labels,\"BIO_labels\":BIO_labels,\"discourse_labels\":discourse_labels}  ","metadata":{"execution":{"iopub.execute_input":"2022-03-09T13:27:04.828641Z","iopub.status.busy":"2022-03-09T13:27:04.827539Z","iopub.status.idle":"2022-03-09T13:27:04.829772Z","shell.execute_reply":"2022-03-09T13:27:04.830186Z","shell.execute_reply.started":"2022-03-01T23:20:29.642568Z"},"gradient":{"execution_count":15,"id":"7c2ec5a9-c120-4883-934d-41a83e3da1cc","kernelId":""},"papermill":{"duration":0.046389,"end_time":"2022-03-09T13:27:04.830332","exception":false,"start_time":"2022-03-09T13:27:04.783943","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_params = {'batch_size': config['valid_batch_size'],\n                'shuffle': False,\n                'num_workers': 2,\n                'pin_memory':True\n                }\n\ntokenizer = AutoTokenizer.from_pretrained(DOWNLOADED_MODEL_PATH) \n\n\n# TEST DATASET\ntest_texts_set = dataset(test_texts, tokenizer, config['max_length'], True)\ntest_texts_loader = DataLoader(test_texts_set, **test_params,collate_fn=CustomCollate(tokenizer,512))\n\ntokenizer_longformer = AutoTokenizer.from_pretrained(\"../input/pytorch-longformer-large\") \ntest_texts_set_longformer = dataset(test_texts, tokenizer_longformer, config['max_length'], True)\ntest_texts_loader_longformer = DataLoader(test_texts_set_longformer, **test_params,collate_fn=CustomCollate(tokenizer))","metadata":{"execution":{"iopub.execute_input":"2022-03-09T13:27:04.877043Z","iopub.status.busy":"2022-03-09T13:27:04.87643Z","iopub.status.idle":"2022-03-09T13:27:05.158672Z","shell.execute_reply":"2022-03-09T13:27:05.158103Z","shell.execute_reply.started":"2022-03-01T23:20:29.671898Z"},"gradient":{"execution_count":18,"id":"3b670879-0d5d-44c6-950d-780518c2b3bf","kernelId":""},"papermill":{"duration":0.307964,"end_time":"2022-03-09T13:27:05.158812","exception":false,"start_time":"2022-03-09T13:27:04.850848","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Network","metadata":{"papermill":{"duration":0.025696,"end_time":"2022-03-09T13:27:05.206099","exception":false,"start_time":"2022-03-09T13:27:05.180403","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from transformers import *\nimport torch.nn as nn\nimport torch.nn.functional as F\nrearrange_indices=[14, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\nclass ResidualLSTM(nn.Module):\n\n    def __init__(self, d_model,rnn):\n        super(ResidualLSTM, self).__init__()\n        self.downsample=nn.Linear(d_model,d_model//2)\n        if rnn=='GRU':\n            self.LSTM=nn.GRU(d_model//2, d_model//2, num_layers=2, bidirectional=False, dropout=0.2)\n        else:\n            self.LSTM=nn.LSTM(d_model//2, d_model//2, num_layers=2, bidirectional=False, dropout=0.2)\n        self.dropout1=nn.Dropout(0.2)\n        self.norm1= nn.LayerNorm(d_model//2)\n        self.linear1=nn.Linear(d_model//2, d_model*4)\n        self.linear2=nn.Linear(d_model*4, d_model)\n        self.dropout2=nn.Dropout(0.2)\n        self.norm2= nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        res=x\n        x=self.downsample(x)\n        x, _ = self.LSTM(x)\n        x=self.dropout1(x)\n        x=self.norm1(x)\n        x=F.relu(self.linear1(x))\n        x=self.linear2(x)\n        x=self.dropout2(x)\n        x=res+x\n        return self.norm2(x)\n\n\nclass ConvLSTMHead(nn.Module):\n    def __init__(self):\n        super(ConvLSTMHead, self).__init__()\n        self.downsample=nn.Sequential(nn.Linear(1024,256))\n        self.conv1=  nn.Sequential(nn.Conv1d(256,256,3,padding=1),\n                                  nn.ReLU())\n        self.norm1 = nn.LayerNorm(256)\n        self.conv2=  nn.Sequential(nn.Conv1d(256,256,3,padding=1),\n                                  nn.ReLU())\n        self.norm2 = nn.LayerNorm(256)\n        #self.lstm=nn.LSTM(256,256,2,bidirectional=True)\n        self.lstm=ResidualLSTM(256)\n        self.upsample=nn.Sequential(nn.Linear(256,1024),nn.ReLU())\n        self.classification_head=nn.Sequential(nn.Linear(1024,15))\n\n\n    def forward(self,x):\n\n        x=self.downsample(x)\n        res=x\n        x=self.conv1(x.permute(0,2,1))\n        x=self.norm1(x.permute(0,2,1)).permute(0,2,1)\n        x=self.conv2(x)\n        x=self.norm1(x.permute(0,2,1))\n        x=x+res\n        x=self.lstm(x.permute(1,0,2))\n        x=x.permute(1,0,2)\n        x=self.upsample(x)\n        x=self.classification_head(x)\n        #print(x.shape)\n        #exit()\n        return x\n\n\nclass TransformerModel(nn.Module):\n    def __init__(self,DOWNLOADED_MODEL_PATH, rnn='LSTM'):\n        super(TransformerModel, self).__init__()\n        config_model = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'/config.json')\n\n        self.backbone=AutoModel.from_pretrained(\n                           DOWNLOADED_MODEL_PATH+'/pytorch_model.bin',config=config_model)\n\n        self.lstm=ResidualLSTM(1024,rnn)\n        self.classification_head=nn.Linear(1024,15)\n        #self.head=nn.Sequential(nn.Linear(1024,15))\n\n        # self.downsample=nn.Sequential(nn.Linear(1024,256))\n        # self.conv1d=nn.Sequential(nn.Conv1d(256,256,3,padding=0),\n        #                           nn.ReLU(),\n        #                           nn.LayerNorm(256),\n        #                           nn.Conv1d(256,256,3,padding=1),\n        #                           nn.ReLU(),\n        #                           nn.LayerNorm(256))\n\n        #self.BIO_head=nn.Sequential(nn.Linear(1024,3))\n\n    def forward(self,x,attention_mask):\n        x=self.backbone(input_ids=x,attention_mask=attention_mask,return_dict=False)[0]\n\n        x=self.lstm(x.permute(1,0,2)).permute(1,0,2)\n        x=self.classification_head(x)\n        # x=x.permute(0,2,1)\n        # x=self.conv1d(x)\n        # print(x.shape)\n        # exit()\n        # classification_output=self.classification_head(x)\n        #BIO_output=self.BIO_head(x[0])\n        # print(x.shape)\n        # exit()\n        return [x[:,:,rearrange_indices]]#, BIO_output\n    \nclass SlidingWindowTransformerModel(nn.Module):\n    def __init__(self,DOWNLOADED_MODEL_PATH, rnn, window_size=512, edge_len=64):\n        super(SlidingWindowTransformerModel, self).__init__()\n        config_model = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'/config.json')\n\n        self.backbone=AutoModel.from_pretrained(\n                           DOWNLOADED_MODEL_PATH+'/pytorch_model.bin',config=config_model)\n\n        self.lstm=ResidualLSTM(1024,rnn)\n        self.classification_head=nn.Linear(1024,15)\n        self.window_size=window_size\n        self.edge_len=edge_len\n        self.inner_len=window_size-edge_len*2\n        #self.head=nn.Sequential(nn.Linear(1024,15))\n\n        # self.downsample=nn.Sequential(nn.Linear(1024,256))\n        # self.conv1d=nn.Sequential(nn.Conv1d(256,256,3,padding=0),\n        #                           nn.ReLU(),\n        #                           nn.LayerNorm(256),\n        #                           nn.Conv1d(256,256,3,padding=1),\n        #                           nn.ReLU(),\n        #                           nn.LayerNorm(256))\n\n        #self.BIO_head=nn.Sequential(nn.Linear(1024,3))\n\n    def forward(self,input_ids,attention_mask):\n\n        B,L=input_ids.shape\n\n        # print(L)\n        # exit()\n        #x=self.backbone(input_ids=input_ids,attention_mask=attention_mask,return_dict=False)[0]\n        if L<=self.window_size:\n            x=self.backbone(input_ids=input_ids,attention_mask=attention_mask,return_dict=False)[0]\n            #pass\n        else:\n            #print(\"####\")\n            #print(input_ids.shape)\n            segments=(L-self.window_size)//self.inner_len\n            if (L-self.window_size)%self.inner_len>self.edge_len:\n                segments+=1\n            elif segments==0:\n                segments+=1\n            x=self.backbone(input_ids=input_ids[:,:self.window_size],attention_mask=attention_mask[:,:self.window_size],return_dict=False)[0]\n            for i in range(1,segments+1):\n                start=self.window_size-self.edge_len+(i-1)*self.inner_len\n                end=self.window_size-self.edge_len+(i-1)*self.inner_len+self.window_size\n                end=min(end,L)\n                x_next=input_ids[:,start:end]\n                mask_next=attention_mask[:,start:end]\n                x_next=self.backbone(input_ids=x_next,attention_mask=mask_next,return_dict=False)[0]\n                #L_next=x_next.shape[1]-self.edge_len,\n                if i==segments:\n                    x_next=x_next[:,self.edge_len:]\n                else:\n                    x_next=x_next[:,self.edge_len:self.edge_len+self.inner_len]\n                #print(x_next.shape)\n                x=torch.cat([x,x_next],1)\n\n                #print(start,end)\n        #print(x.shape)\n        x=self.lstm(x.permute(1,0,2)).permute(1,0,2)\n        x=self.classification_head(x)\n\n        # x=x.permute(0,2,1)\n        # x=self.conv1d(x)\n        # print(x.shape)\n        # exit()\n        # classification_output=self.classification_head(x)\n        #BIO_output=self.BIO_head(x[0])\n        # print(x.shape)\n        # exit()\n        #return x\n        return [x[:,:,rearrange_indices]]#, BIO_output\n    ","metadata":{"execution":{"iopub.execute_input":"2022-03-09T13:27:05.281565Z","iopub.status.busy":"2022-03-09T13:27:05.277518Z","iopub.status.idle":"2022-03-09T13:27:05.284763Z","shell.execute_reply":"2022-03-09T13:27:05.284344Z","shell.execute_reply.started":"2022-03-01T23:20:29.937904Z"},"papermill":{"duration":0.057929,"end_time":"2022-03-09T13:27:05.28491","exception":false,"start_time":"2022-03-09T13:27:05.226981","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{"papermill":{"duration":0.020464,"end_time":"2022-03-09T13:27:05.325982","exception":false,"start_time":"2022-03-09T13:27:05.305518","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\n# Returns per-word, mean class prediction probability over all tokens corresponding to each word\ndef inference(data_loader, model_ids, model, path):\n    \n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    ensemble_preds = np.zeros((len(data_loader.dataset), config['max_length'], len(labels_to_ids)), dtype=np.float32)\n    wids = np.full((len(data_loader.dataset), config['max_length']), -100)\n    for model_i, model_id in enumerate(model_ids):\n        \n        model.load_state_dict(torch.load(f'{path}/fold{model_id}.pt', map_location=config['device']))\n        \n        # put model in training mode\n        model.eval()\n        for batch_i, batch in tqdm(enumerate(data_loader)):\n            \n            if model_i == 0: wids[batch_i*config['valid_batch_size']:(batch_i+1)*config['valid_batch_size'],:batch['wids'].shape[1]] = batch['wids'].numpy()\n\n            # MOVE BATCH TO GPU AND INFER\n            ids = batch[\"input_ids\"].to(config['device'])\n            mask = batch[\"attention_mask\"].to(config['device'])\n            with torch.no_grad():\n                #with amp.autocast():\n                outputs = model(ids, attention_mask=mask)\n            all_preds = torch.nn.functional.softmax(outputs[0], dim=2).cpu().detach().numpy() \n            ensemble_preds[batch_i*config['valid_batch_size']:(batch_i+1)*config['valid_batch_size'],:all_preds.shape[1]] += all_preds\n            \n            del ids\n            del mask\n            del outputs\n            del all_preds\n            \n        gc.collect()\n        torch.cuda.empty_cache()\n            \n    ensemble_preds /= len(model_ids)\n    predictions = []\n    # INTERATE THROUGH EACH TEXT AND GET PRED\n    for text_i in range(ensemble_preds.shape[0]):\n        token_preds = ensemble_preds[text_i]\n        \n        prediction = []\n        previous_word_idx = -1\n        prob_buffer = []\n        word_ids = wids[text_i][wids[text_i] != -100]\n        for idx,word_idx in enumerate(word_ids):                            \n            if word_idx == -1:\n                pass\n            elif word_idx != previous_word_idx:              \n                if prob_buffer:\n                    prediction.append(np.mean(prob_buffer, dtype=np.float32, axis=0))\n                    prob_buffer = []\n                prob_buffer.append(token_preds[idx])\n                previous_word_idx = word_idx\n            else: \n                prob_buffer.append(token_preds[idx])\n        prediction.append(np.mean(prob_buffer, dtype=np.float32, axis=0))\n        predictions.append(prediction)\n            \n    gc.collect()\n    torch.cuda.empty_cache()\n    return predictions","metadata":{"execution":{"iopub.execute_input":"2022-03-09T13:27:05.382036Z","iopub.status.busy":"2022-03-09T13:27:05.381192Z","iopub.status.idle":"2022-03-09T13:27:05.382968Z","shell.execute_reply":"2022-03-09T13:27:05.383456Z","shell.execute_reply.started":"2022-03-01T23:20:29.970761Z"},"gradient":{"execution_count":22,"id":"93df9ccc-f7ff-4cb2-8e20-8030026fff42","kernelId":""},"papermill":{"duration":0.037145,"end_time":"2022-03-09T13:27:05.383579","exception":false,"start_time":"2022-03-09T13:27:05.346434","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SlidingWindowTransformerModel(DOWNLOADED_MODEL_PATH,'GRU').to(config['device'])\ntest_word_preds = inference(test_texts_loader, [0, 1, 3, 4, 6, 7], model, LOAD_MODEL_FROM)","metadata":{"execution":{"iopub.execute_input":"2022-03-09T13:27:05.428896Z","iopub.status.busy":"2022-03-09T13:27:05.428354Z","iopub.status.idle":"2022-03-09T13:29:52.599112Z","shell.execute_reply":"2022-03-09T13:29:52.598599Z","shell.execute_reply.started":"2022-03-01T23:20:29.990181Z"},"gradient":{"execution_count":24,"id":"7597203a-32ac-477b-951e-befcb89c709a","kernelId":""},"papermill":{"duration":167.19546,"end_time":"2022-03-09T13:29:52.599271","exception":false,"start_time":"2022-03-09T13:27:05.403811","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TransformerModel(\"../input/pytorch-longformer-large\",'GRU').to(config['device'])\ntest_word_preds2 = inference(test_texts_loader_longformer, [0, 2, 3, 4, 5, 6], model, \"../input/fp-test63\")","metadata":{"execution":{"iopub.execute_input":"2022-03-09T13:29:52.656083Z","iopub.status.busy":"2022-03-09T13:29:52.655442Z","iopub.status.idle":"2022-03-09T13:31:23.310145Z","shell.execute_reply":"2022-03-09T13:31:23.310894Z","shell.execute_reply.started":"2022-03-01T23:23:38.281Z"},"papermill":{"duration":90.685687,"end_time":"2022-03-09T13:31:23.311135","exception":false,"start_time":"2022-03-09T13:29:52.625448","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sequence Datasets\nWe will create datasets that, instead of describing individual words or tokens, describes sequences of words. Within some heuristic constraints, every possible sub-sequence of words in a text will converted to a dataset sample with the following attributes:\n\n* features- sequence length, position, and various kinds of class probability predictions/statistics\n* labels- whether the sequence matches exactly a discourse instance\n* truePos- whether the sequence matches a discourse instance by competition criteria for true positive\n* groups- the integer index of the text where the sequence is found\n* wordRanges- the start and end word index of the sequence in the text\n\nSequence datasets are generated for each discourse type and for validation and submission datasets. ","metadata":{"papermill":{"duration":0.040327,"end_time":"2022-03-09T13:31:23.406117","exception":false,"start_time":"2022-03-09T13:31:23.36579","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"","metadata":{"papermill":{"duration":0.032412,"end_time":"2022-03-09T13:31:23.473355","exception":false,"start_time":"2022-03-09T13:31:23.440943","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from collections import Counter\nfrom bisect import bisect_left\n\n# Percentile code taken from https://www.kaggle.com/vuxxxx/tensorflow-longformer-ner-postprocessing\n# Thank Vu!\n#\n# Use 99.5% of the distribution of lengths for a disourse type as maximum. \n# Increasing this constraint makes this step slower but generally increases performance.\ntrain_df=pd.read_csv(\"../input/feedback-prize-2021/train.csv\")\nMAX_SEQ_LEN = {}\ntrain_df['len'] = train_df['predictionstring'].apply(lambda x:len(x.split()))\nmax_lens = train_df.groupby('discourse_type')['len'].quantile(.995)\nfor disc_type in disc_type_to_ids:\n    MAX_SEQ_LEN[disc_type] = int(max_lens[disc_type])\n\n#The minimum probability prediction for a 'B'egin class for which we will evaluate a word sequence\nMIN_BEGIN_PROB = {\n    'Claim': .35*0.8,\n    'Concluding Statement': .15*1.0,\n    'Counterclaim': .04*1.25,\n    'Evidence': .1*0.8,\n    'Lead': .32*1.0,\n    'Position': .25*0.8,\n    'Rebuttal': .01*1.25,\n}\n        \nclass SeqDataset(object):\n    \n    def __init__(self, features, labels, groups, wordRanges, truePos):\n        \n        self.features = np.array(features, dtype=np.float32)\n        self.labels = np.array(labels)\n        self.groups = np.array(groups, dtype=np.int16)\n        self.wordRanges = np.array(wordRanges, dtype=np.int16)\n        self.truePos = np.array(truePos)\n\n# Adapted from https://stackoverflow.com/questions/60467081/linear-interpolation-in-numpy-quantile\n# This is used to prevent re-sorting to compute quantile for every sequence.\ndef sorted_quantile(array, q):\n    array = np.array(array)\n    n = len(array)\n    index = (n - 1) * q\n    left = np.floor(index).astype(int)\n    fraction = index - left\n    right = left\n    right = right + (fraction > 0).astype(int)\n    i, j = array[left], array[right]\n    return i + (j - i) * fraction\n        \ndef seq_dataset(disc_type, pred_indices=None, submit=False):\n    begin_class_ids = [0, 1, 3, 5, 7, 9, 11, 13]\n    word_preds = valid_word_preds if not submit else test_word_preds   \n    w = ensemble_weights[disc_type]\n    \n    \n    window = pred_indices if pred_indices else range(len(word_preds))\n    X = np.empty((int(1e6),N_FEATURES), dtype=np.float32)\n    X_ind = 0\n    y = []\n    truePos = []\n    wordRanges = []\n    groups = []\n    for text_i in window:\n        text_preds, text_preds2 = np.array(test_word_preds[text_i]), np.array(test_word_preds2[text_i])\n        \n        if len(text_preds) <= len(text_preds2):\n            text_preds = w*text_preds + (1-w)*text_preds2[:len(text_preds)] \n        else:\n            text_preds[:len(text_preds2)] = w*text_preds[:len(text_preds2)] + (1-w)*text_preds2\n        \n        num_words = len(text_preds)\n        \n        global_features, global_locs = [], []\n        \n        for dt in disc_type_to_ids:\n            disc_begin, disc_inside = disc_type_to_ids[dt]\n            \n            gmean = (text_preds[:, disc_begin] + text_preds[:, disc_inside]).mean()\n            global_features.append(gmean)\n            global_locs.append(np.argmax(text_preds[:, disc_begin])/float(num_words))\n        \n        disc_begin, disc_inside = disc_type_to_ids[disc_type]\n        \n        # The probability that a word corresponds to either a 'B'-egin or 'I'-nside token for a class\n        prob_or = lambda word_preds: word_preds[:,disc_begin] + word_preds[:,disc_inside]\n        \n        if not submit:\n            gt_idx = set()\n            gt_arr = np.zeros(num_words, dtype=int)\n            text_gt = valid.loc[valid.id == test_dataset.id.values[text_i]]\n            disc_gt = text_gt.loc[text_gt.discourse_type == disc_type]\n            \n            # Represent the discourse instance locations in a hash set and an integer array for speed\n            for row_i, row in enumerate(disc_gt.iterrows()):\n                splt = row[1]['predictionstring'].split()\n                start, end = int(splt[0]), int(splt[-1]) + 1\n                gt_idx.add((start, end))\n                gt_arr[start:end] = row_i + 1\n            gt_lens = np.bincount(gt_arr)\n        \n        # Iterate over every sub-sequence in the text\n        quants = np.linspace(0,1,7)\n        prob_begins = np.copy(text_preds[:,disc_begin])\n        min_begin = MIN_BEGIN_PROB[disc_type]\n        for pred_start in range(num_words):\n            prob_begin = prob_begins[pred_start]\n            if prob_begin > min_begin:\n                begin_or_inside = []\n                for pred_end in range(pred_start+1,min(num_words+1, pred_start+MAX_SEQ_LEN[disc_type]+1)):\n                    \n                    new_prob = prob_or(text_preds[pred_end-1:pred_end])\n                    insert_i = bisect_left(begin_or_inside, new_prob)\n                    begin_or_inside.insert(insert_i, new_prob[0])\n\n                    # Generate features for a word sub-sequence\n\n                    # The length and position of start/end of the sequence\n                    features = [pred_end - pred_start, pred_start / float(num_words), pred_end / float(num_words)]\n                    \n                    # 7 evenly spaced quantiles of the distribution of relevant class probabilities for this sequence\n                    features.extend(list(sorted_quantile(begin_or_inside, quants)))\n\n                    # The probability that words on either edge of the current sub-sequence belong to the class of interest\n                    features.append(prob_or(text_preds[pred_start-1:pred_start])[0] if pred_start > 0 else 0)\n                    features.append(prob_or(text_preds[pred_end:pred_end+1])[0] if pred_end < num_words else 0)\n                    features.append(prob_or(text_preds[pred_start-2:pred_start-1])[0] if pred_start > 1 else 0)\n                    features.append(prob_or(text_preds[pred_end+1:pred_end+2])[0] if pred_end < (num_words-1) else 0)\n                    \n                    # The probability that the first word corresponds to a 'B'-egin token\n                    features.append(text_preds[pred_start,disc_begin])\n                    features.append(text_preds[pred_start-1,disc_begin])\n                    \n                    if pred_end < num_words:\n                        features.append(text_preds[pred_end, begin_class_ids].sum())\n                    else:\n                        features.append(1.0)\n                    \n                    s = prob_or(text_preds[pred_start:pred_end])\n                    features.append(np.argmax(s)/features[0]) # maximum point location on sequence\n                    features.append(np.argmin(s)/features[0]) # minimum point location on sequence\n                    instability = 0\n                    if len(s) > 1:\n                        instability = (np.diff(s)**2).mean()\n                    features.append(instability)\n                    \n                    features.extend(list(global_features))\n                    features.extend(list([loc - features[1] for loc in global_locs]))\n                    \n                    exact_match = (pred_start, pred_end) in gt_idx if not submit else None\n\n                    if not submit:\n                        true_pos = False\n                        for match_cand, count in Counter(gt_arr[pred_start:pred_end]).most_common(2):\n                            if match_cand != 0 and count / float(pred_end - pred_start) >= .5 and float(count) / gt_lens[match_cand] >= .5: true_pos = True\n                    else: true_pos = None\n\n                    # For efficiency, use a numpy array instead of a list that doubles in size when full to conserve constant \"append\" time complexity\n                    if X_ind >= X.shape[0]:\n                        new_X = np.empty((X.shape[0]*2,N_FEATURES), dtype=np.float32)\n                        new_X[:X.shape[0]] = X\n                        X = new_X\n                    X[X_ind] = features\n                    X_ind += 1\n                    \n                    y.append(exact_match)\n                    truePos.append(true_pos)\n                    wordRanges.append((np.int16(pred_start), np.int16(pred_end)))\n                    groups.append(np.int16(text_i))\n\n    return SeqDataset(X[:X_ind], y, groups, wordRanges, truePos)\n","metadata":{"execution":{"iopub.execute_input":"2022-03-09T13:31:23.574692Z","iopub.status.busy":"2022-03-09T13:31:23.573382Z","iopub.status.idle":"2022-03-09T13:31:25.390756Z","shell.execute_reply":"2022-03-09T13:31:25.390266Z","shell.execute_reply.started":"2022-03-01T23:26:51.633145Z"},"gradient":{"execution_count":25,"id":"268caa30-cdb4-4dbe-873e-60d40637e003","kernelId":""},"papermill":{"duration":1.885128,"end_time":"2022-03-09T13:31:25.390912","exception":false,"start_time":"2022-03-09T13:31:23.505784","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict strings and submit","metadata":{"papermill":{"duration":0.031169,"end_time":"2022-03-09T13:31:25.453725","exception":false,"start_time":"2022-03-09T13:31:25.422556","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from joblib import Parallel, delayed\nfrom multiprocessing import Manager\nfrom sklearn.model_selection import cross_val_score, GroupKFold\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom skopt.space import Real\nfrom skopt import gp_minimize\nimport sys\nimport xgboost\n\nNUM_FOLDS = 8\n\nwarnings.filterwarnings('ignore', '.*ragged nested sequences*',)\n\nprob_cache = {} # Cache each fold's probability predictions for speed\nclfs = []  # Each fold will add its classifier here\n# Predict sub-sequences for a discourse type and set of train/test texts\ndef predict_strings(disc_type, probThresh, test_groups, train_ind=None, submit=False):\n    string_preds = []\n    #validSeqDs = validSeqSets[disc_type]\n    #submitSeqDs = submitSeqSets[disc_type]\n    \n    # Average the probability predictions of a set of classifiers\n    \n\n    \n    predict_df = test_texts\n    text_df = test_texts\n    \n    for text_idx in tqdm(test_groups):\n        # The probability of true positive and (start,end) of each sub-sequence in the curent text\n        \n        testDs=seq_dataset(disc_type, pred_indices=[text_idx],submit=True)\n        \n        prob_tp_curr = get_tp_prob(testDs, disc_type)\n        word_ranges_curr = testDs.wordRanges[testDs.groups == text_idx]\n        \n        split_text = text_df.loc[text_df.id == predict_df.id.values[text_idx]].iloc[0].text.split()\n        full_preds = np.zeros(len(split_text))\n        # Include the sub-sequence predictions in order of predicted probability\n        for prob, wordRange in reversed(sorted(zip(prob_tp_curr, [tuple(wr) for wr in word_ranges_curr]))):\n            \n            # Until the predicted probability is lower than the tuned threshold\n            if prob < probThresh: break\n                \n            intersect = np.sum(full_preds[wordRange[0]:wordRange[1]])        \n            total = wordRange[1] - wordRange[0]\n            condition = intersect/total <= 0.15\n\n            if condition:\n                full_preds[wordRange[0]:wordRange[1]] = 1\n                string_preds.append((predict_df.id.values[text_idx], disc_type, ' '.join(map(str, list(range(wordRange[0], wordRange[1]))))))\n    return string_preds\n\ndef sub_df(string_preds):\n    return pd.DataFrame(string_preds, columns=['id','class','predictionstring'])\n    \n","metadata":{"execution":{"iopub.execute_input":"2022-03-09T13:31:25.529225Z","iopub.status.busy":"2022-03-09T13:31:25.528625Z","iopub.status.idle":"2022-03-09T13:31:25.724744Z","shell.execute_reply":"2022-03-09T13:31:25.725567Z","shell.execute_reply.started":"2022-03-01T23:32:52.197383Z"},"gradient":{"execution_count":27,"id":"cc2e7c81-2347-4b3d-90c6-2b52d2ffa036","kernelId":""},"papermill":{"duration":0.240536,"end_time":"2022-03-09T13:31:25.725757","exception":false,"start_time":"2022-03-09T13:31:25.485221","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the tuned probability thresholds from tuning result files, and make sub-sequence predictions!","metadata":{"papermill":{"duration":0.031353,"end_time":"2022-03-09T13:31:25.789997","exception":false,"start_time":"2022-03-09T13:31:25.758644","status":"completed"},"tags":[]}},{"cell_type":"code","source":"uniqueSubmitGroups = range(len(test_word_preds))\n\nsub = pd.concat([sub_df(predict_strings(disc_type, thresholds[disc_type], \n                                        uniqueSubmitGroups, submit=True)) for disc_type in disc_type_to_ids ]).reset_index(drop=True)","metadata":{"execution":{"iopub.execute_input":"2022-03-09T13:31:25.871788Z","iopub.status.busy":"2022-03-09T13:31:25.871025Z","iopub.status.idle":"2022-03-09T13:31:27.821078Z","shell.execute_reply":"2022-03-09T13:31:27.820575Z","shell.execute_reply.started":"2022-03-01T23:32:54.2975Z"},"gradient":{"execution_count":28,"id":"087e600e-f15f-4154-8c26-6a8dabcfb73b","kernelId":""},"papermill":{"duration":1.997904,"end_time":"2022-03-09T13:31:27.821203","exception":false,"start_time":"2022-03-09T13:31:25.823299","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)\nsub.head()","metadata":{"execution":{"iopub.execute_input":"2022-03-09T13:31:27.903499Z","iopub.status.busy":"2022-03-09T13:31:27.902948Z","iopub.status.idle":"2022-03-09T13:31:27.91718Z","shell.execute_reply":"2022-03-09T13:31:27.916704Z","shell.execute_reply.started":"2022-03-01T23:26:54.447223Z"},"gradient":{"execution_count":29,"id":"658e2ab3-b02d-449c-ad21-340e6206586f","kernelId":""},"papermill":{"duration":0.057249,"end_time":"2022-03-09T13:31:27.917299","exception":false,"start_time":"2022-03-09T13:31:27.86005","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}