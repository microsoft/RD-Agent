{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport gc\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission0 = pd.read_csv('/kaggle/input/m5-final-models/submission_LSTM.csv') \nsubmission1 = pd.read_csv('/kaggle/input/m5-final-models/submission_XGBoost.csv') \nsubmission2 = pd.read_csv('/kaggle/input/m5-final-models/submission_LGBM.csv')\nsubmission3 = pd.read_csv('/kaggle/input/m5-final-models/submission_prophet.csv')\nsubmission4 = pd.read_csv('/kaggle/input/m5-final-models/submission_SARIMAX.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission0 = pd.read_csv('/kaggle/input/basemodels/submissionRod.csv') # 0.64\n#submission1 = pd.read_csv('/kaggle/input/basemodels/LightGBM.csv') # 0.47\n#submission2 = pd.read_csv('/kaggle/input/basemodels/submissionDeepNeuralNet(DNN).csv')  \n#submission3 = pd.read_csv('/kaggle/input/basemodels/submissionWitchTime.csv') # Eval = 0----------------------------------\n#submission4 = pd.read_csv('/kaggle/input/basemodels/submissionm5-baseline.csv')  # Eval = 0 \n#submission5 = pd.read_csv('/kaggle/input/basemodels/STOREandCAT.csv') #Eval=0\n#submission6 = pd.read_csv('/kaggle/input/basemodels/M5ForecasteR.csv')\n#submission7 = pd.read_csv('/kaggle/input/basemodels/NolanLightGBM.csv') \n#submission8 = pd.read_csv('/kaggle/input/basemodels/DNNwithCategoricalEmbeddings day-to-day.csv') #Eval = 0\n#submission9 = pd.read_csv('/kaggle/input/basemodels/SARIMAX_submission.csv')\n#submission10 = pd.read_csv('/kaggle/input/basemodels/submission_XGB.csv') # Evaluation values are all zero!!!!!\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\ncalendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\nprices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\nvalidation = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv') \nsample_sub = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def perfect_sub(): # returns a perfect submition just to be sure that we are validating on the right window of values\n    submission = OperateBaseModels(submission0,submission1, a=0, b=1)\n    diference = validation.merge(submission, how='right')\n    shift= 56\n    perfect_submission = diference[diference.columns[-shift:-shift+28]]\n\n\n    #\n    col = { 'd_'+str(1914+i):'F'+str(i+1) for i in range(28)}\n    perfect_submission = perfect_submission.rename(columns=col)\n    perfect_submission.insert(loc=0, column='id', value= submission0['id']) \n    perfect_submission = perfect_submission.fillna(0)\n    return perfect_submission\n\n\ndef OperateBaseModels(ModelA,ModelB, a,b):\n    submissionMerge = pd.merge(ModelA, ModelB, on='id', how='left', suffixes=('_x', '_y'))#.mean(level=0)\n    submissionMerge\n    submission = pd.DataFrame()\n    submission.insert(loc=0, column='id', value= ModelB['id']) \n\n    for j in range(28):\n        i =j+1\n        \n        submission.insert(loc=i, column='F'+str(i), value= ((submissionMerge[submissionMerge.columns[i]]*a)+(submissionMerge[submissionMerge.columns[i+28]])*b)/(a+b)) \n     \n\n    return submission\n\n\n                                       ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the WRMSSEEvaluator class is from  https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import Union\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm_notebook as tqdm\n\n\nclass WRMSSEEvaluator(object):\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 0  # for lv1 aggregation\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n\n        self.group_ids = (\n            'all_id',\n            'state_id',\n            'store_id',\n            'cat_id',\n            'dept_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            'item_id',\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n        )\n\n        for i, group_id in enumerate(tqdm(self.group_ids)):\n            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n            scale = []\n            for _, row in train_y.iterrows():\n                series = row.values[np.argmax(row.values != 0):]\n                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n            setattr(self, f'lv{i + 1}_train_df', train_y)\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n\n        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n        return weight_df\n\n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        valid_y = getattr(self, f'lv{lv}_valid_df')\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n        scale = getattr(self, f'lv{lv}_scale')\n        return (score / scale).map(np.sqrt)\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n            all_scores.append(lv_scores.sum())\n\n        return np.mean(all_scores)\ndf_train_full =  pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_evaluation.csv\")\ndf_calendar = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\")\ndf_prices = pd.read_csv(\"../input/m5-forecasting-accuracy/sell_prices.csv\")\n\ndf_train = df_train_full.iloc[:, :-28]\ndf_valid = df_train_full.iloc[:, -28:]\nevaluator = WRMSSEEvaluator(df_train, df_valid, df_calendar, df_prices)\n\n\ndef WRMSSEE(sub): # Add a function to take the standard submission format \n    val_preds = sample_sub[['id']].merge(sub, on = 'id')  #Order values like the submission example\n    \n    val_preds.columns = ['id'] + list(df_valid.columns)  # Rename columns\n    valid_preds = val_preds.iloc[:30490, -28:] #Take just validation data from the submition dataframe\n    return evaluator.score(valid_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef function(solution):\n    solution = np.abs(solution )\n    submission = OperateBaseModels(submission0,submission1, a=solution[0], b=solution[1])\n    submission = OperateBaseModels(submission,submission2, a=1, b=solution[2]) \n    submission = OperateBaseModels(submission,submission3, a=1, b=solution[3]) \n    submission = OperateBaseModels(submission,submission4, a=1, b=solution[4])\n                                   \n                                   \n    #submission = OperateBaseModels(submission,submission2, a=1, b=solution[2]) #I just added this for fun to see how much it affects to have more models into consideration \n    #submission = OperateBaseModels(submission,submission3, a=1, b=solution[3]) #00\n    #submission = OperateBaseModels(submission,submission4, a=1, b=solution[4])#00\n    #submission = OperateBaseModels(submission,submission5, a=1, b=solution[5])#00\n    #submission = OperateBaseModels(submission,submission6, a=1, b=solution[3])\n    #submission = OperateBaseModels(submission,submission7, a=1, b=solution[4])\n    #submission = OperateBaseModels(submission,submission8, a=1, b=solution[8])#00\n    #submission = OperateBaseModels(submission,submission9, a=1, b=solution[5])\n    #submission = OperateBaseModels(submission,submission10, a=1, b=solution[10])#00\n\n    #Mean_error = np.mean(np.mean(np.abs(perfect_sub._get_numeric_data()-submission._get_numeric_data())))\n    error = WRMSSEE(submission) #+Mean_error/2\n    return error \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perfect_sub = perfect_sub()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.optimize import OptimizeResult\nfrom scipy.optimize import minimize\nhist = []\ndef custmin(fun, x0, args=(), maxfev=None, stepsize=0.1,  # Then we optimize the coeficients to minimize error\n        maxiter=500, callback=None, **options):\n    bestx = x0\n    besty = fun(x0)\n    funcalls = 1\n    niter = 0\n    improved = True\n    stop = False\n\n    while improved and not stop and niter < maxiter:\n        improved = False\n        niter += 1\n        print('Iteration number',niter,'WRMSSEE',besty, 'using ', str(np.abs(np.array(bestx)) ))\n        hist.append(besty)\n        for dim in range(np.size(x0)):\n            for s in [bestx[dim] - stepsize, bestx[dim] + stepsize]:\n                testx = np.copy(bestx)\n                testx[dim] = s\n                testy = fun(testx, *args)\n                funcalls += 1\n                if testy < besty:\n                    besty = testy\n                    bestx = testx\n                    improved = True\n            if callback is not None:\n                callback(bestx)\n            if maxfev is not None and funcalls >= maxfev:\n                stop = True\n                break\n\n    return OptimizeResult(fun=besty, x=bestx, nit=niter,\n                          nfev=funcalls, success=(niter > 1))\nx0 = np.random.rand(5)\nres = minimize(function, x0, method=custmin, options=dict(stepsize=0.05))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Plot_solution =np.abs(res.x)\n# [9.35946038e-04, 1.00239378e+01, 1.67410230e-02, 4.01652749e+00,   1.16401096e-02, 9.17120136e-01]\nsns.set()\nplt.plot(hist)\nplt.ylabel('WRMSSEE')\nplt.xlabel('Iteration')\n\nPlot_solution","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\n\n\nax = sns.barplot( x=pd.DataFrame(Plot_solution).index,y=0, data=pd.DataFrame(Plot_solution));\nax.set(xlabel='Model', ylabel='Coeficients form weighted average ');\nax.set_xticklabels(['DNN Embedings','XGBoost','LGBM','SARIMAX','Prophet'], rotation=30);\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"solution = Plot_solution\n\nsolution = np.abs(solution )\nsubmission = OperateBaseModels(submission0,submission1, a=solution[0], b=solution[1])\nsubmission = OperateBaseModels(submission,submission2, a=1, b=solution[2]) \nsubmission = OperateBaseModels(submission,submission3, a=1, b=solution[3]) \nsubmission = OperateBaseModels(submission,submission4, a=1, b=solution[4])\n\n\n#submission = OperateBaseModels(submission,submission2, a=1, b=solution[2]) #I just added this for fun to see how much it affects to have more models into consideration \n#submission = OperateBaseModels(submission,submission3, a=1, b=solution[3]) #00\n#submission = OperateBaseModels(submission,submission4, a=1, b=solution[4])#00\n#submission = OperateBaseModels(submission,submission5, a=1, b=solution[5])#00\n#submission = OperateBaseModels(submission,submission6, a=1, b=solution[3])\n#submission = OperateBaseModels(submission,submission7, a=1, b=solution[4])\n#submission = OperateBaseModels(submission,submission8, a=1, b=solution[8])#00\n#submission = OperateBaseModels(submission,submission9, a=1, b=solution[5])\n#submission = OperateBaseModels(submission,submission10, a=1, b=solution[10])#00\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"WRMSSEE(submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#error\n#perfect_sub\ndiference = validation.merge(submission,how='left')                                      \nerror = np.mean(np.mean(np.abs(perfect_sub._get_numeric_data()-submission._get_numeric_data())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import seaborn as sns\n# sns.set(style=\"whitegrid\")\n# tips = sns.load_dataset(\"tips\")\n# solution = pd.DataFrame(solution)\n# ax = sns.barplot( x=solution.index,y=0, data=solution,palette=\"Blues_d\")\n# ax.set(xlabel='Model', ylabel='Optimized weight in the average ')\n# ax.set_xticklabels(['DNN Embedings','NolanFirstSubmission','DeepNeuralNet(DNN)'], rotation=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}