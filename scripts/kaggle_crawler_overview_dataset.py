import asyncio
import csv
import json
import os
import re
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import pandas as pd
from crawl4ai import (
    AsyncWebCrawler,
    BrowserConfig,
    CacheMode,
    CrawlerRunConfig,
    LLMConfig,
)
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field


class KaggleCompetitionInfo(BaseModel):
    overview: str = Field(..., description="Overview/description of the competition.")
    evaluation: str = Field(..., description="Evaluation method and metrics for the competition.")
    submission_file: str = Field(..., description="Submission file and format for the competition.")
    other_important_info: str = Field(..., description="Other important information for the competition.")
    tags: list[str] = Field(
        ..., description="Tags of the competitionï¼Œshould give the full tag list, not just the first few tags."
    )


Provider = "gpt-4.1"
api_token = "sk-1234"
base_url = "http://10.150.240.117:38888"


class DataPageInfo(BaseModel):
    dataset_description: str = Field(
        ...,
        description="Extract all detailed descriptions about the dataset from the Kaggle competition data page, including dataset structure, file lists, field definitions, data types, data sources, and any usage notes. The content should be presented in complete and structured Markdown format, including all original formatting such as headers, tables, lists, etc. You need to cover as much content as possible from the page and not miss any information. Except for ## Dataset Description, the maximum heading level for other sections should be ###. In English.",
    )


MAX_RETRIES = 3
RETRY_DELAY = 5  # ç§’
RETRY_BACKOFF = 2  # æŒ‡æ•°é€€é¿å€æ•°


def create_overview_config():
    return CrawlerRunConfig(
        word_count_threshold=1,
        delay_before_return_html=3,
        page_timeout=30000,  # å¢åŠ é¡µé¢è¶…æ—¶æ—¶é—´
        wait_until="networkidle",  # ç­‰å¾…ç½‘ç»œç©ºé—²
        extraction_strategy=LLMExtractionStrategy(
            llm_config=LLMConfig(provider=Provider, api_token=api_token, base_url=base_url),
            schema=KaggleCompetitionInfo.model_json_schema(),
            extraction_type="block",
            instruction="""ä»çˆ¬å–çš„å†…å®¹ä¸­æå–Kaggleç«èµ›çš„ä»¥ä¸‹ä¿¡æ¯ï¼š
            1. Overview (æ¦‚è§ˆ): ç«èµ›çš„ç®€ä»‹å’Œç›®æ ‡
            2. Evaluation (è¯„ä¼°æ–¹æ³•): ç«èµ›æäº¤çš„è¯„ä¼°æ ‡å‡†å’ŒæŒ‡æ ‡
            3. Submission File (æäº¤æ–‡ä»¶): ç«èµ›æäº¤çš„æ–‡ä»¶æ ¼å¼å’Œè¦æ±‚
            4. Other Important Info (å…¶ä»–é‡è¦ä¿¡æ¯): ç«èµ›çš„å…¶ä»–é‡è¦ä¿¡æ¯
            5. Tags (æ ‡ç­¾): ç«èµ›çš„æ ‡ç­¾,ä»¥åˆ—è¡¨å½¢å¼è¿”å›,åº”è¯¥è¿”å›æ‰€æœ‰çš„tags
            
            æ³¨æ„ï¼Œä»¥ä¸‹å†…å®¹ä¸éœ€è¦æå–ï¼š
            1. Timeline (æ—¶é—´çº¿): ç«èµ›çš„é‡è¦æ—¥æœŸï¼ŒåŒ…æ‹¬å¼€å§‹æ—¶é—´ã€ç»“æŸæ—¶é—´ç­‰
            2. Citation (å¼•ç”¨): å¦‚ä½•å¼•ç”¨è¯¥ç«èµ›æˆ–æ•°æ®é›†çš„ä¿¡æ¯
            3. Prizes (å¥–å“): ç«èµ›å¥–å“ä¿¡æ¯ 
            4. Competition Host
            5. Participation

            
            è¯·ä»”ç»†æŸ¥æ‰¾é¡µé¢ä¸­æ‰€æœ‰ç›¸å…³éƒ¨åˆ†ï¼Œä¸è¦é—æ¼ä»»ä½•ä¿¡æ¯ã€‚æå–çš„å†…å®¹åº”è¯¥å°½é‡ä¿æŒä¸æ›´æ”¹åŸå§‹é¡µé¢çš„ä»»ä½•å†…å®¹ï¼Œä»¥markdownæ ¼å¼è¿”å›,è¿”å›çš„æœ€å¤§æ ¼å¼ä¸º##ï¼Œæ³¨æ„åˆ†éš”ç¬¦ï¼ˆæ¯”å¦‚æ¢è¡Œç¬¦ç­‰ï¼‰ã€‚
            
            æ ¼å¼è¦æ±‚ï¼š
            - è¿”å›çš„æœ€å¤§æ ‡é¢˜æ ¼å¼ä¸º##ï¼ˆäºŒçº§æ ‡é¢˜ï¼‰
            - ä¿ç•™åŸæ–‡ä¸­æ‰€æœ‰HTMLæ ‡é¢˜ç¬¦å·ï¼Œè½¬æ¢æˆå¯¹åº”çš„markdownæ ‡é¢˜æ ¼å¼ï¼ˆh1->##, h2->##, h3->###, h4->####ç­‰ï¼Œä½†æœ€é«˜ä¸è¶…è¿‡##ï¼‰
            - ä¿æŒåŸå§‹å†…å®¹çš„åˆ†éš”ç¬¦ï¼ˆæ¢è¡Œç¬¦ã€ç©ºè¡Œç­‰ï¼‰
            - ä¿ç•™æ‰€æœ‰åˆ—è¡¨ã€ä»£ç å—ã€è¡¨æ ¼ç­‰æ ¼å¼
            - ä¸è¦ä¿®æ”¹æˆ–æ”¹å†™åŸå§‹å†…å®¹ï¼Œåªè¿›è¡Œæ ¼å¼è½¬æ¢
            - ä¸è¦ä¿ç•™ä»»ä½•HTMLæ ‡ç­¾ï¼Œä»¥åŠå¤–éƒ¨é“¾æ¥
            """,
        ),
        cache_mode=CacheMode.BYPASS,
        excluded_tags=["Timeline", "Citation", "Prizes"],
    )


def create_data_config():
    return CrawlerRunConfig(
        word_count_threshold=1,
        delay_before_return_html=3.0,
        extraction_strategy=LLMExtractionStrategy(
            llm_config=LLMConfig(provider=Provider, api_token=api_token, base_url=base_url),
            schema=DataPageInfo.model_json_schema(),
            extraction_type="schema",
            instruction="""ä»dataé¡µé¢ä¸­æå–æ‰€æœ‰ä¸æ•°æ®é›†ç›¸å…³çš„æ‰€æœ‰ä¿¡æ¯ï¼Œä½ éœ€è¦å°½å¯èƒ½å¤šçš„è¦†ç›–é¡µé¢ä¸­çš„æ‰€æœ‰å†…å®¹ï¼Œä¸è¦é—æ¼ä»»ä½•ä¿¡æ¯ã€‚
            
            **æå–åŸåˆ™**ï¼š
            1. æå–æ‰€æœ‰ä¸æ•°æ®é›†ç›¸å…³çš„æ ¸å¿ƒä¿¡æ¯
            2. åŒ…æ‹¬ä½†ä¸é™äºï¼šæ•°æ®é›†æè¿°ã€æ–‡ä»¶è¯´æ˜ã€æ ¼å¼ä¿¡æ¯ã€å­—æ®µå®šä¹‰ç­‰
            3. è‡ªåŠ¨è¯†åˆ«é¡µé¢ä¸­å­˜åœ¨çš„æ•°æ®ç›¸å…³ç« èŠ‚,æ¯”å¦‚files,columnsç­‰ï¼Œå¦‚æœé¡µé¢ä¸­æ²¡æœ‰æŸä¸ªç« èŠ‚ï¼Œå°±ä¸è¦åŒ…å«
            
            **æ’é™¤ä»¥ä¸‹å†…å®¹**ï¼š
            - License/è®¸å¯è¯ä¿¡æ¯
            - Prizes/å¥–å“ä¿¡æ¯
            - Citation/å¼•ç”¨ä¿¡æ¯
            - License/è®¸å¯è¯ä¿¡æ¯
            
            **è¾“å‡ºè¦æ±‚**ï¼š
            - ä¸¥æ ¼æŒ‰ç…§markdownæ ¼å¼è¾“å‡º
            - ä½¿ç”¨##ã€###ç­‰æ ‡é¢˜å±‚çº§
            - ä¿ç•™æ‰€æœ‰æ ¼å¼ï¼šåˆ—è¡¨ã€è¡¨æ ¼ã€ä»£ç ç­‰
            - æ¯ä¸ªç« èŠ‚éƒ½åº”è¯¥æ˜¯å®Œæ•´çš„markdownæ–‡æœ¬
            - ä¸è¦ä¿ç•™ä»»ä½•HTMLæ ‡ç­¾ï¼Œä»¥åŠå¤–éƒ¨é“¾æ¥
            
           
            ```""",
        ),
        cache_mode=CacheMode.BYPASS,
    )


async def crawl_page_with_retry(
    crawler: AsyncWebCrawler, url: str, config: CrawlerRunConfig, page_type: str, competition_name: str
) -> Optional[Dict[str, Any]]:
    """å¸¦é‡è¯•æœºåˆ¶çš„é¡µé¢çˆ¬å–å‡½æ•°"""
    retry_count = 0
    last_error = None

    while retry_count < MAX_RETRIES:
        try:
            print(f"æ­£åœ¨çˆ¬å– {competition_name} çš„ {page_type} é¡µé¢... (å°è¯• {retry_count + 1}/{MAX_RETRIES})")
            result = await crawler.arun(url=url, config=config)

            if result.extracted_content and result.extracted_content != "[]":
                extracted_data = (
                    json.loads(result.extracted_content)
                    if isinstance(result.extracted_content, str)
                    else result.extracted_content
                )
                if extracted_data and len(extracted_data) > 0:
                    print(f"âœ… æˆåŠŸçˆ¬å– {competition_name} çš„ {page_type} é¡µé¢")
                    return extracted_data[0]

            # å¦‚æœæ²¡æœ‰æå–åˆ°å†…å®¹ï¼Œä¹Ÿç®—ä½œå¤±è´¥
            raise ValueError(f"æœªèƒ½ä» {page_type} é¡µé¢æå–åˆ°æœ‰æ•ˆå†…å®¹")

        except Exception as e:
            last_error = e
            retry_count += 1
            print(f"âŒ çˆ¬å– {competition_name} çš„ {page_type} é¡µé¢å¤±è´¥ (å°è¯• {retry_count}/{MAX_RETRIES}): {str(e)}")

            if retry_count < MAX_RETRIES:
                delay = RETRY_DELAY * (RETRY_BACKOFF ** (retry_count - 1))
                print(f"â³ ç­‰å¾… {delay} ç§’åé‡è¯•...")
                await asyncio.sleep(delay)
            else:
                print(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒçˆ¬å– {competition_name} çš„ {page_type} é¡µé¢")

    return None


def create_competition_folder(competition_name: str, save_path: Path) -> Path:
    """ä¸ºæ¯”èµ›åˆ›å»ºæ–‡ä»¶å¤¹"""
    folder_path = save_path / competition_name
    folder_path.mkdir(parents=True, exist_ok=True)
    return folder_path


def save_competition_files(competition_name: str, final_result: Dict[str, Any], save_path: Path) -> None:
    """ä¿å­˜æ¯”èµ›çš„JSONå’ŒMarkdownæ–‡ä»¶åˆ°æŒ‡å®šæ–‡ä»¶å¤¹"""
    folder_path = create_competition_folder(competition_name, save_path)

    # Create final JSON output
    final_json = {
        competition_name: {
            "overview": final_result.get("overview", ""),
            "evaluation": final_result.get("evaluation", ""),
            "tags": final_result.get("tags", ""),
            "submission_file": final_result.get("submission_file", ""),
            "other_important_info": final_result.get("other_important_info", ""),
            "dataset_description": final_result.get("dataset_description", ""),
        }
    }

    # Save JSON file
    json_filename = folder_path / f"{competition_name}.json"
    with open(json_filename, "w", encoding="utf-8") as f:
        json.dump(final_json, f, indent=2, ensure_ascii=False)

    # Create markdown content
    markdown_content = f"# {competition_name.replace('-', ' ').title()}\n\n"

    for key in ["overview", "tags", "evaluation", "submission_file", "other_important_info", "dataset_description"]:
        result = final_result.get(key)
        if key == "tags":
            markdown_content += f"\n## {key.capitalize()}\n{result}\n\n"
        elif result and key != "dataset_description":
            markdown_content += f"\n{result}\n\n"
        elif result and key == "dataset_description":
            if result.startswith("###"):
                result = result.replace("###", "##", 1)
            markdown_content += f"\n{result}\n\n"

    # Save markdown file
    markdown_filename = folder_path / f"{competition_name}.md"
    with open(markdown_filename, "w", encoding="utf-8") as f:
        f.write(markdown_content)

    print(f"âœ… å·²ä¿å­˜ {competition_name} çš„æ–‡ä»¶åˆ°æ–‡ä»¶å¤¹: {folder_path}")


def write_failed_competition_to_csv(competition_name: str, error_message: str, save_path: Path) -> None:
    """å°†å¤±è´¥çš„æ¯”èµ›ä¿¡æ¯å†™å…¥CSVæ–‡ä»¶"""
    csv_file = save_path / "failed_competitions.csv"

    # æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºå¹¶å†™å…¥è¡¨å¤´
    file_exists = csv_file.exists()

    with open(csv_file, "a", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)

        # å¦‚æœæ˜¯æ–°æ–‡ä»¶ï¼Œå†™å…¥è¡¨å¤´
        if not file_exists:
            writer.writerow(["Competition Name", "Failed Time", "Error Message"])

        # å†™å…¥å¤±è´¥è®°å½•
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        writer.writerow([competition_name, current_time, error_message])

    print(f"âŒ å·²è®°å½•å¤±è´¥æ¯”èµ›åˆ°CSV: {competition_name}")


async def process_single_competition(crawler: AsyncWebCrawler, competition_name: str, save_path: Path) -> bool:
    """å¤„ç†å•ä¸ªæ¯”èµ›çš„çˆ¬å–"""
    print(f"\nğŸš€ å¼€å§‹å¤„ç†æ¯”èµ›: {competition_name}")

    base_url = f"https://www.kaggle.com/competitions/{competition_name}"

    pages_config = [
        ("overview", f"{base_url}/overview", create_overview_config()),
        ("data", f"{base_url}/data", create_data_config()),
    ]

    final_result = {}
    success_count = 0
    error_messages = []

    for page_type, url, page_config in pages_config:
        data = await crawl_page_with_retry(crawler, url, page_config, page_type, competition_name)

        if data:
            success_count += 1
            if page_type == "overview":
                final_result["overview"] = data.get("overview", "")
                final_result["evaluation"] = data.get("evaluation", "")
                final_result["submission_file"] = data.get("submission_file", "")
                final_result["other_important_info"] = data.get("other_important_info", "")
                final_result["tags"] = data.get("tags", "")
            elif page_type == "data":
                final_result["dataset_description"] = data.get("dataset_description", "")
        else:
            error_messages.append(f"Failed to crawl {page_type} page")

    # åªæœ‰å½“æ‰€æœ‰é¡µé¢éƒ½æˆåŠŸçˆ¬å–æ—¶æ‰ä¿å­˜æ–‡ä»¶ï¼Œå¦åˆ™è®°å½•åˆ°å¤±è´¥CSV
    if success_count == len(pages_config):
        save_competition_files(competition_name, final_result, save_path)
        print(f"âœ… æ¯”èµ› {competition_name} å¤„ç†å®Œæˆ (æˆåŠŸ: {success_count}/{len(pages_config)} é¡µé¢)")
        return True
    else:
        error_msg = f"éƒ¨åˆ†é¡µé¢çˆ¬å–å¤±è´¥ (æˆåŠŸ: {success_count}/{len(pages_config)}). é”™è¯¯: {'; '.join(error_messages)}"
        write_failed_competition_to_csv(competition_name, error_msg, save_path)
        print(f"âŒ æ¯”èµ› {competition_name} å¤„ç†å¤±è´¥ - {error_msg}")
        return False


async def process_competition_list(competition_list: List[str], save_path: Optional[str] = None) -> Dict[str, bool]:
    """æ‰¹é‡å¤„ç†æ¯”èµ›åˆ—è¡¨"""
    # è®¾ç½®ä¿å­˜è·¯å¾„
    if save_path is None:
        save_path_obj = Path.cwd()
    else:
        save_path_obj = Path(save_path)
        save_path_obj.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ“ æ–‡ä»¶ä¿å­˜è·¯å¾„: {save_path_obj.absolute()}")

    browser_config = BrowserConfig(
        verbose=False, headless=True, user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    )

    results = {}

    async with AsyncWebCrawler(config=browser_config) as crawler:
        print(f"ğŸ“Š å¼€å§‹æ‰¹é‡å¤„ç† {len(competition_list)} ä¸ªæ¯”èµ›")

        for i, competition_name in enumerate(competition_list, 1):
            print(f"\n{'='*50}")
            print(f"å¤„ç†è¿›åº¦: {i}/{len(competition_list)}")

            success = await process_single_competition(crawler, competition_name, save_path_obj)
            results[competition_name] = success

            # åœ¨æ¯”èµ›ä¹‹é—´æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            if i < len(competition_list):
                await asyncio.sleep(2)

    return results


async def main():
    """ä¸»å‡½æ•° - å¯ä»¥å¤„ç†å•ä¸ªæ¯”èµ›æˆ–æ¯”èµ›åˆ—è¡¨"""

    # ç¤ºä¾‹ï¼šå¤„ç†å•ä¸ªæ¯”èµ›
    single_competition = "us-patent-phrase-to-phrase-matching"

    # ç¤ºä¾‹ï¼šå¤„ç†æ¯”èµ›åˆ—è¡¨ï¼ˆä½ å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹è¿™ä¸ªåˆ—è¡¨ï¼‰
    df = pd.read_csv(
        "/data/userdata/v-zhangyifei/MSRA/github/knowledge/github_competitions/æ¯”èµ›å°èŠ‚/competitions_extracted_info.csv"
    )
    competition_list = df["link_slug"].tolist()
    print(f"å¤„ç†{len(competition_list)}ä¸ªæ¯”èµ›")

    # è®¾ç½®è‡ªå®šä¹‰ä¿å­˜è·¯å¾„ï¼ˆå¯ä»¥ä¿®æ”¹ä¸ºä½ æƒ³è¦çš„è·¯å¾„ï¼‰
    custom_save_path = (
        "/data/userdata/v-zhangyifei/MSRA/github/knowledge/kaggle_competitions"  # è®¾ç½®ä¸ºNoneåˆ™ä½¿ç”¨å½“å‰ç›®å½•
    )

    # é€‰æ‹©å¤„ç†æ¨¡å¼
    print("é€‰æ‹©å¤„ç†æ¨¡å¼:")
    print("1. å¤„ç†å•ä¸ªæ¯”èµ›")
    print("2. æ‰¹é‡å¤„ç†æ¯”èµ›åˆ—è¡¨")

    # è¿™é‡Œä½ å¯ä»¥ä¿®æ”¹ä¸ºç›´æ¥æŒ‡å®šæ¨¡å¼ï¼Œæˆ–è€…ä»å‘½ä»¤è¡Œå‚æ•°è·å–
    mode = int(input("è¯·è¾“å…¥å¤„ç†æ¨¡å¼: "))

    if mode == 1:
        # å¤„ç†å•ä¸ªæ¯”èµ›
        browser_config = BrowserConfig(
            verbose=False, headless=True, user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )

        # è®¾ç½®ä¿å­˜è·¯å¾„
        if custom_save_path is None:
            save_path_obj = Path.cwd()
        else:
            save_path_obj = Path(custom_save_path)
            save_path_obj.mkdir(parents=True, exist_ok=True)

        print(f"ğŸ“ æ–‡ä»¶ä¿å­˜è·¯å¾„: {save_path_obj.absolute()}")

        async with AsyncWebCrawler(config=browser_config) as crawler:
            success = await process_single_competition(crawler, single_competition, save_path_obj)
            if success:
                print(f"\nğŸ‰ æ¯”èµ› {single_competition} å¤„ç†æˆåŠŸ!")
            else:
                print(f"\nğŸ˜ æ¯”èµ› {single_competition} å¤„ç†å¤±è´¥!")

    else:
        # æ‰¹é‡å¤„ç†æ¯”èµ›åˆ—è¡¨
        results = await process_competition_list(competition_list, custom_save_path)

        # è¾“å‡ºå¤„ç†ç»“æœç»Ÿè®¡
        print(f"\n{'='*50}")
        print("ğŸ“‹ å¤„ç†ç»“æœç»Ÿè®¡:")
        successful = sum(results.values())
        total = len(results)

        print(f"âœ… æˆåŠŸ: {successful}/{total}")
        print(f"âŒ å¤±è´¥: {total - successful}/{total}")

        if successful < total:
            print("\nå¤±è´¥çš„æ¯”èµ›:")
            for comp, success in results.items():
                if not success:
                    print(f"  - {comp}")
            print(
                f"\nğŸ“„ å¤±è´¥è®°å½•å·²ä¿å­˜åˆ°: {Path(custom_save_path if custom_save_path else '.') / 'failed_competitions.csv'}"
            )


def process_competitions(competition_list: List[str], save_path: Optional[str] = None) -> None:
    """æä¾›ç»™å¤–éƒ¨è°ƒç”¨çš„å‡½æ•°"""
    asyncio.run(process_competition_list(competition_list, save_path))


if __name__ == "__main__":
    asyncio.run(main())
